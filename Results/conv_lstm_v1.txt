Iteration 0:
agent_timesteps_total: 4000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022237328277237116
  StateBufferConnector_ms: 0.0013373363977191091
  ViewRequirementAgentConnector_ms: 0.11557669475160796
counters:
  num_agent_steps_sampled: 4000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 4000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-46-29
done: false
episode_len_mean: 22.67816091954023
episode_media: {}
episode_reward_max: 71.0
episode_reward_mean: 22.67816091954023
episode_reward_min: 9.0
episodes_this_iter: 174
episodes_total: 174
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 2782.2045454545455
      num_env_steps_trained: 4000.0
      total_loss: 8.274921428073537
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.20000000298023224
      curr_lr: 5.0e-05
      entropy: 0.6785264827988364
      mean_kl_loss: 0.017390087338342186
      policy_loss: -0.8141018030318347
      total_loss: 8.274921428073537
      vf_explained_var: -0.0005499544468793002
      vf_loss: 9.085545128042048
      vf_loss_unclipped: 253.18085167624733
  num_agent_steps_sampled: 4000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 4000
  num_env_steps_trained: 0
iterations_since_restore: 1
node_ip: 127.0.0.1
num_agent_steps_sampled: 4000
num_agent_steps_trained: 0
num_env_steps_sampled: 4000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 390.9848632055331
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 33.486666666666665
  ram_util_percent: 55.80666666666668
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05562797032032467
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02300894114482332
  mean_inference_ms: 1.0111525133427188
  mean_raw_obs_processing_ms: 0.2353007220946374
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022237328277237116
    StateBufferConnector_ms: 0.0013373363977191091
    ViewRequirementAgentConnector_ms: 0.11557669475160796
  custom_metrics: {}
  episode_len_mean: 22.67816091954023
  episode_media: {}
  episode_reward_max: 71.0
  episode_reward_mean: 22.67816091954023
  episode_reward_min: 9.0
  episodes_this_iter: 174
  hist_stats:
    episode_lengths: [12, 31, 16, 22, 18, 22, 10, 24, 17, 20, 15, 21, 14, 71, 10,
      43, 40, 11, 51, 14, 20, 20, 20, 23, 14, 17, 21, 13, 22, 12, 21, 38, 46, 32,
      40, 45, 19, 16, 33, 16, 16, 16, 9, 14, 29, 11, 12, 14, 14, 17, 15, 16, 26, 10,
      20, 13, 19, 23, 15, 12, 14, 13, 29, 21, 41, 21, 26, 13, 13, 28, 17, 20, 15,
      27, 23, 21, 15, 12, 27, 14, 29, 19, 35, 15, 23, 54, 10, 13, 32, 18, 35, 46,
      15, 25, 11, 28, 46, 22, 23, 19, 14, 48, 27, 15, 14, 21, 18, 25, 24, 29, 12,
      16, 15, 28, 11, 26, 23, 18, 15, 12, 13, 22, 33, 13, 17, 13, 20, 32, 11, 26,
      35, 20, 37, 15, 16, 16, 21, 13, 17, 39, 11, 35, 51, 15, 13, 19, 35, 24, 18,
      30, 20, 15, 31, 49, 35, 15, 18, 19, 58, 57, 10, 16, 18, 16, 9, 18, 51, 41, 13,
      10, 20, 36, 41, 20]
    episode_reward: [12.0, 31.0, 16.0, 22.0, 18.0, 22.0, 10.0, 24.0, 17.0, 20.0, 15.0,
      21.0, 14.0, 71.0, 10.0, 43.0, 40.0, 11.0, 51.0, 14.0, 20.0, 20.0, 20.0, 23.0,
      14.0, 17.0, 21.0, 13.0, 22.0, 12.0, 21.0, 38.0, 46.0, 32.0, 40.0, 45.0, 19.0,
      16.0, 33.0, 16.0, 16.0, 16.0, 9.0, 14.0, 29.0, 11.0, 12.0, 14.0, 14.0, 17.0,
      15.0, 16.0, 26.0, 10.0, 20.0, 13.0, 19.0, 23.0, 15.0, 12.0, 14.0, 13.0, 29.0,
      21.0, 41.0, 21.0, 26.0, 13.0, 13.0, 28.0, 17.0, 20.0, 15.0, 27.0, 23.0, 21.0,
      15.0, 12.0, 27.0, 14.0, 29.0, 19.0, 35.0, 15.0, 23.0, 54.0, 10.0, 13.0, 32.0,
      18.0, 35.0, 46.0, 15.0, 25.0, 11.0, 28.0, 46.0, 22.0, 23.0, 19.0, 14.0, 48.0,
      27.0, 15.0, 14.0, 21.0, 18.0, 25.0, 24.0, 29.0, 12.0, 16.0, 15.0, 28.0, 11.0,
      26.0, 23.0, 18.0, 15.0, 12.0, 13.0, 22.0, 33.0, 13.0, 17.0, 13.0, 20.0, 32.0,
      11.0, 26.0, 35.0, 20.0, 37.0, 15.0, 16.0, 16.0, 21.0, 13.0, 17.0, 39.0, 11.0,
      35.0, 51.0, 15.0, 13.0, 19.0, 35.0, 24.0, 18.0, 30.0, 20.0, 15.0, 31.0, 49.0,
      35.0, 15.0, 18.0, 19.0, 58.0, 57.0, 10.0, 16.0, 18.0, 16.0, 9.0, 18.0, 51.0,
      41.0, 13.0, 10.0, 20.0, 36.0, 41.0, 20.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05562797032032467
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02300894114482332
    mean_inference_ms: 1.0111525133427188
    mean_raw_obs_processing_ms: 0.2353007220946374
time_since_restore: 10.233100652694702
time_this_iter_s: 10.233100652694702
time_total_s: 10.233100652694702
timers:
  sample_time_ms: 2701.08
  synch_weights_time_ms: 3.481
  training_iteration_time_ms: 10230.556
timestamp: 1703047589
timesteps_total: 4000
training_iteration: 1
trial_id: default
-----------------------
----------------------

Iteration 1:
agent_timesteps_total: 8000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002327241188238475
  StateBufferConnector_ms: 0.0013379026050409995
  ViewRequirementAgentConnector_ms: 0.11658294141785172
counters:
  num_agent_steps_sampled: 8000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 8000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-46-38
done: false
episode_len_mean: 32.93388429752066
episode_media: {}
episode_reward_max: 100.0
episode_reward_mean: 32.93388429752066
episode_reward_min: 9.0
episodes_this_iter: 121
episodes_total: 295
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 3655.818181818182
      num_env_steps_trained: 4000.0
      total_loss: 8.296300223379424
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.20000000298023224
      curr_lr: 5.0e-05
      entropy: 0.6408882827469797
      mean_kl_loss: 0.012816148682483772
      policy_loss: -0.5460733644890062
      total_loss: 8.296300223379424
      vf_explained_var: -0.010806387121027166
      vf_loss: 8.839810486995813
      vf_loss_unclipped: 435.95208277846825
  num_agent_steps_sampled: 8000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 8000
  num_env_steps_trained: 0
iterations_since_restore: 2
node_ip: 127.0.0.1
num_agent_steps_sampled: 8000
num_agent_steps_trained: 0
num_env_steps_sampled: 8000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 472.8965843703222
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 31.541666666666668
  ram_util_percent: 56.38333333333333
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05614833699448623
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.023276757501645214
  mean_inference_ms: 1.0182099955372026
  mean_raw_obs_processing_ms: 0.23034147399414948
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002327241188238475
    StateBufferConnector_ms: 0.0013379026050409995
    ViewRequirementAgentConnector_ms: 0.11658294141785172
  custom_metrics: {}
  episode_len_mean: 32.93388429752066
  episode_media: {}
  episode_reward_max: 100.0
  episode_reward_mean: 32.93388429752066
  episode_reward_min: 9.0
  episodes_this_iter: 121
  hist_stats:
    episode_lengths: [20, 18, 39, 36, 15, 49, 41, 48, 25, 50, 11, 36, 29, 13, 22,
      33, 13, 29, 29, 24, 30, 67, 35, 100, 32, 27, 25, 39, 24, 19, 32, 28, 40, 15,
      20, 14, 31, 15, 10, 100, 20, 32, 43, 12, 35, 22, 16, 31, 41, 37, 61, 21, 62,
      32, 26, 34, 46, 21, 24, 29, 24, 17, 40, 22, 25, 80, 33, 38, 35, 25, 92, 36,
      16, 36, 96, 23, 45, 33, 60, 26, 18, 56, 32, 11, 23, 26, 73, 36, 15, 32, 20,
      26, 54, 15, 26, 11, 24, 15, 48, 47, 13, 18, 27, 32, 44, 38, 37, 19, 30, 30,
      55, 39, 9, 29, 33, 11, 31, 25, 37, 48, 42]
    episode_reward: [20.0, 18.0, 39.0, 36.0, 15.0, 49.0, 41.0, 48.0, 25.0, 50.0, 11.0,
      36.0, 29.0, 13.0, 22.0, 33.0, 13.0, 29.0, 29.0, 24.0, 30.0, 67.0, 35.0, 100.0,
      32.0, 27.0, 25.0, 39.0, 24.0, 19.0, 32.0, 28.0, 40.0, 15.0, 20.0, 14.0, 31.0,
      15.0, 10.0, 100.0, 20.0, 32.0, 43.0, 12.0, 35.0, 22.0, 16.0, 31.0, 41.0, 37.0,
      61.0, 21.0, 62.0, 32.0, 26.0, 34.0, 46.0, 21.0, 24.0, 29.0, 24.0, 17.0, 40.0,
      22.0, 25.0, 80.0, 33.0, 38.0, 35.0, 25.0, 92.0, 36.0, 16.0, 36.0, 96.0, 23.0,
      45.0, 33.0, 60.0, 26.0, 18.0, 56.0, 32.0, 11.0, 23.0, 26.0, 73.0, 36.0, 15.0,
      32.0, 20.0, 26.0, 54.0, 15.0, 26.0, 11.0, 24.0, 15.0, 48.0, 47.0, 13.0, 18.0,
      27.0, 32.0, 44.0, 38.0, 37.0, 19.0, 30.0, 30.0, 55.0, 39.0, 9.0, 29.0, 33.0,
      11.0, 31.0, 25.0, 37.0, 48.0, 42.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05614833699448623
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.023276757501645214
    mean_inference_ms: 1.0182099955372026
    mean_raw_obs_processing_ms: 0.23034147399414948
time_since_restore: 18.694228649139404
time_this_iter_s: 8.461127996444702
time_total_s: 18.694228649139404
timers:
  sample_time_ms: 2750.148
  synch_weights_time_ms: 3.397
  training_iteration_time_ms: 9344.523
timestamp: 1703047598
timesteps_total: 8000
training_iteration: 2
trial_id: default
-----------------------
----------------------

Iteration 2:
agent_timesteps_total: 12000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0023946762084960938
  StateBufferConnector_ms: 0.0013887882232666016
  ViewRequirementAgentConnector_ms: 0.11681723594665527
counters:
  num_agent_steps_sampled: 12000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 12000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-46-45
done: false
episode_len_mean: 49.65
episode_media: {}
episode_reward_max: 218.0
episode_reward_mean: 49.65
episode_reward_min: 9.0
episodes_this_iter: 66
episodes_total: 361
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 4414.535714285715
      num_env_steps_trained: 4000.0
      total_loss: 8.893521002360753
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.20000000298023224
      curr_lr: 5.0e-05
      entropy: 0.6000680880887168
      mean_kl_loss: 0.010300899612372762
      policy_loss: -0.36859675709690365
      total_loss: 8.893521002360753
      vf_explained_var: -0.0751585406916482
      vf_loss: 9.260057653699603
      vf_loss_unclipped: 1082.1919839041573
  num_agent_steps_sampled: 12000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 12000
  num_env_steps_trained: 0
iterations_since_restore: 3
node_ip: 127.0.0.1
num_agent_steps_sampled: 12000
num_agent_steps_trained: 0
num_env_steps_sampled: 12000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 532.9929406427539
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 32.290909090909096
  ram_util_percent: 56.554545454545455
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05623316990818782
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.023383205318726265
  mean_inference_ms: 1.0185900276771116
  mean_raw_obs_processing_ms: 0.2265894033172552
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0023946762084960938
    StateBufferConnector_ms: 0.0013887882232666016
    ViewRequirementAgentConnector_ms: 0.11681723594665527
  custom_metrics: {}
  episode_len_mean: 49.65
  episode_media: {}
  episode_reward_max: 218.0
  episode_reward_mean: 49.65
  episode_reward_min: 9.0
  episodes_this_iter: 66
  hist_stats:
    episode_lengths: [36, 15, 32, 20, 26, 54, 15, 26, 11, 24, 15, 48, 47, 13, 18,
      27, 32, 44, 38, 37, 19, 30, 30, 55, 39, 9, 29, 33, 11, 31, 25, 37, 48, 42, 59,
      70, 55, 37, 46, 62, 108, 46, 62, 87, 69, 16, 39, 61, 78, 41, 91, 218, 32, 19,
      31, 52, 67, 110, 41, 138, 28, 46, 42, 116, 38, 28, 81, 34, 32, 30, 52, 58, 25,
      64, 180, 26, 44, 29, 54, 49, 81, 116, 41, 93, 46, 27, 53, 56, 28, 70, 55, 113,
      62, 30, 57, 16, 71, 47, 61, 35]
    episode_reward: [36.0, 15.0, 32.0, 20.0, 26.0, 54.0, 15.0, 26.0, 11.0, 24.0, 15.0,
      48.0, 47.0, 13.0, 18.0, 27.0, 32.0, 44.0, 38.0, 37.0, 19.0, 30.0, 30.0, 55.0,
      39.0, 9.0, 29.0, 33.0, 11.0, 31.0, 25.0, 37.0, 48.0, 42.0, 59.0, 70.0, 55.0,
      37.0, 46.0, 62.0, 108.0, 46.0, 62.0, 87.0, 69.0, 16.0, 39.0, 61.0, 78.0, 41.0,
      91.0, 218.0, 32.0, 19.0, 31.0, 52.0, 67.0, 110.0, 41.0, 138.0, 28.0, 46.0, 42.0,
      116.0, 38.0, 28.0, 81.0, 34.0, 32.0, 30.0, 52.0, 58.0, 25.0, 64.0, 180.0, 26.0,
      44.0, 29.0, 54.0, 49.0, 81.0, 116.0, 41.0, 93.0, 46.0, 27.0, 53.0, 56.0, 28.0,
      70.0, 55.0, 113.0, 62.0, 30.0, 57.0, 16.0, 71.0, 47.0, 61.0, 35.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05623316990818782
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.023383205318726265
    mean_inference_ms: 1.0185900276771116
    mean_raw_obs_processing_ms: 0.2265894033172552
time_since_restore: 26.200519561767578
time_this_iter_s: 7.506290912628174
time_total_s: 26.200519561767578
timers:
  sample_time_ms: 2727.016
  synch_weights_time_ms: 3.997
  training_iteration_time_ms: 8731.274
timestamp: 1703047605
timesteps_total: 12000
training_iteration: 3
trial_id: default
-----------------------
----------------------

Iteration 3:
agent_timesteps_total: 16000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002385377883911133
  StateBufferConnector_ms: 0.0013260841369628906
  ViewRequirementAgentConnector_ms: 0.11461853981018066
counters:
  num_agent_steps_sampled: 16000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 16000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-46-52
done: false
episode_len_mean: 77.14
episode_media: {}
episode_reward_max: 228.0
episode_reward_mean: 77.14
episode_reward_min: 13.0
episodes_this_iter: 40
episodes_total: 401
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 4831.68
      num_env_steps_trained: 4000.0
      total_loss: 9.23056251525879
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.20000000298023224
      curr_lr: 5.0e-05
      entropy: 0.587630786895752
      mean_kl_loss: 0.0068421664376364785
      policy_loss: -0.3195604246854782
      total_loss: 9.23056251525879
      vf_explained_var: -0.07734531402587891
      vf_loss: 9.548754501342774
      vf_loss_unclipped: 1874.596337890625
  num_agent_steps_sampled: 16000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 16000
  num_env_steps_trained: 0
iterations_since_restore: 4
node_ip: 127.0.0.1
num_agent_steps_sampled: 16000
num_agent_steps_trained: 0
num_env_steps_sampled: 16000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 577.8907036986836
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.839999999999996
  ram_util_percent: 56.55
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.056163298523460574
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02329370168344723
  mean_inference_ms: 1.019505800551528
  mean_raw_obs_processing_ms: 0.22399466120148653
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002385377883911133
    StateBufferConnector_ms: 0.0013260841369628906
    ViewRequirementAgentConnector_ms: 0.11461853981018066
  custom_metrics: {}
  episode_len_mean: 77.14
  episode_media: {}
  episode_reward_max: 228.0
  episode_reward_mean: 77.14
  episode_reward_min: 13.0
  episodes_this_iter: 40
  hist_stats:
    episode_lengths: [108, 46, 62, 87, 69, 16, 39, 61, 78, 41, 91, 218, 32, 19, 31,
      52, 67, 110, 41, 138, 28, 46, 42, 116, 38, 28, 81, 34, 32, 30, 52, 58, 25, 64,
      180, 26, 44, 29, 54, 49, 81, 116, 41, 93, 46, 27, 53, 56, 28, 70, 55, 113, 62,
      30, 57, 16, 71, 47, 61, 35, 35, 162, 189, 123, 103, 84, 110, 64, 117, 218, 119,
      228, 22, 110, 125, 16, 41, 55, 90, 126, 84, 111, 97, 57, 71, 89, 128, 69, 92,
      53, 145, 166, 13, 103, 87, 72, 162, 117, 105, 136]
    episode_reward: [108.0, 46.0, 62.0, 87.0, 69.0, 16.0, 39.0, 61.0, 78.0, 41.0,
      91.0, 218.0, 32.0, 19.0, 31.0, 52.0, 67.0, 110.0, 41.0, 138.0, 28.0, 46.0, 42.0,
      116.0, 38.0, 28.0, 81.0, 34.0, 32.0, 30.0, 52.0, 58.0, 25.0, 64.0, 180.0, 26.0,
      44.0, 29.0, 54.0, 49.0, 81.0, 116.0, 41.0, 93.0, 46.0, 27.0, 53.0, 56.0, 28.0,
      70.0, 55.0, 113.0, 62.0, 30.0, 57.0, 16.0, 71.0, 47.0, 61.0, 35.0, 35.0, 162.0,
      189.0, 123.0, 103.0, 84.0, 110.0, 64.0, 117.0, 218.0, 119.0, 228.0, 22.0, 110.0,
      125.0, 16.0, 41.0, 55.0, 90.0, 126.0, 84.0, 111.0, 97.0, 57.0, 71.0, 89.0, 128.0,
      69.0, 92.0, 53.0, 145.0, 166.0, 13.0, 103.0, 87.0, 72.0, 162.0, 117.0, 105.0,
      136.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.056163298523460574
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02329370168344723
    mean_inference_ms: 1.019505800551528
    mean_raw_obs_processing_ms: 0.22399466120148653
time_since_restore: 33.123759269714355
time_this_iter_s: 6.923239707946777
time_total_s: 33.123759269714355
timers:
  sample_time_ms: 2695.335
  synch_weights_time_ms: 3.749
  training_iteration_time_ms: 8278.883
timestamp: 1703047612
timesteps_total: 16000
training_iteration: 4
trial_id: default
-----------------------
----------------------

Iteration 4:
agent_timesteps_total: 20000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0023140907287597656
  StateBufferConnector_ms: 0.0013265609741210938
  ViewRequirementAgentConnector_ms: 0.11395454406738281
counters:
  num_agent_steps_sampled: 20000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 20000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-46-59
done: false
episode_len_mean: 100.4
episode_media: {}
episode_reward_max: 396.0
episode_reward_mean: 100.4
episode_reward_min: 13.0
episodes_this_iter: 24
episodes_total: 425
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5277.478260869565
      num_env_steps_trained: 4000.0
      total_loss: 9.490260829096256
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.20000000298023224
      curr_lr: 5.0e-05
      entropy: 0.5621227129645969
      mean_kl_loss: 0.005858941208583454
      policy_loss: -0.2363727164009343
      total_loss: 9.490260829096256
      vf_explained_var: -0.03267025429269542
      vf_loss: 9.72546212569527
      vf_loss_unclipped: 3265.378863790761
  num_agent_steps_sampled: 20000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 20000
  num_env_steps_trained: 0
iterations_since_restore: 5
node_ip: 127.0.0.1
num_agent_steps_sampled: 20000
num_agent_steps_trained: 0
num_env_steps_sampled: 20000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 607.266596738582
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.466666666666665
  ram_util_percent: 56.6
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05602335968086081
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.023218303665212645
  mean_inference_ms: 1.017487909514059
  mean_raw_obs_processing_ms: 0.2217246725854902
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0023140907287597656
    StateBufferConnector_ms: 0.0013265609741210938
    ViewRequirementAgentConnector_ms: 0.11395454406738281
  custom_metrics: {}
  episode_len_mean: 100.4
  episode_media: {}
  episode_reward_max: 396.0
  episode_reward_mean: 100.4
  episode_reward_min: 13.0
  episodes_this_iter: 24
  hist_stats:
    episode_lengths: [38, 28, 81, 34, 32, 30, 52, 58, 25, 64, 180, 26, 44, 29, 54,
      49, 81, 116, 41, 93, 46, 27, 53, 56, 28, 70, 55, 113, 62, 30, 57, 16, 71, 47,
      61, 35, 35, 162, 189, 123, 103, 84, 110, 64, 117, 218, 119, 228, 22, 110, 125,
      16, 41, 55, 90, 126, 84, 111, 97, 57, 71, 89, 128, 69, 92, 53, 145, 166, 13,
      103, 87, 72, 162, 117, 105, 136, 103, 90, 84, 144, 100, 53, 396, 50, 125, 271,
      201, 383, 176, 143, 97, 105, 325, 185, 224, 132, 65, 175, 178, 159]
    episode_reward: [38.0, 28.0, 81.0, 34.0, 32.0, 30.0, 52.0, 58.0, 25.0, 64.0, 180.0,
      26.0, 44.0, 29.0, 54.0, 49.0, 81.0, 116.0, 41.0, 93.0, 46.0, 27.0, 53.0, 56.0,
      28.0, 70.0, 55.0, 113.0, 62.0, 30.0, 57.0, 16.0, 71.0, 47.0, 61.0, 35.0, 35.0,
      162.0, 189.0, 123.0, 103.0, 84.0, 110.0, 64.0, 117.0, 218.0, 119.0, 228.0, 22.0,
      110.0, 125.0, 16.0, 41.0, 55.0, 90.0, 126.0, 84.0, 111.0, 97.0, 57.0, 71.0,
      89.0, 128.0, 69.0, 92.0, 53.0, 145.0, 166.0, 13.0, 103.0, 87.0, 72.0, 162.0,
      117.0, 105.0, 136.0, 103.0, 90.0, 84.0, 144.0, 100.0, 53.0, 396.0, 50.0, 125.0,
      271.0, 201.0, 383.0, 176.0, 143.0, 97.0, 105.0, 325.0, 185.0, 224.0, 132.0,
      65.0, 175.0, 178.0, 159.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05602335968086081
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.023218303665212645
    mean_inference_ms: 1.017487909514059
    mean_raw_obs_processing_ms: 0.2217246725854902
time_since_restore: 39.71184945106506
time_this_iter_s: 6.588090181350708
time_total_s: 39.71184945106506
timers:
  sample_time_ms: 2678.258
  synch_weights_time_ms: 3.594
  training_iteration_time_ms: 7940.482
timestamp: 1703047619
timesteps_total: 20000
training_iteration: 5
trial_id: default
-----------------------
----------------------

Iteration 5:
agent_timesteps_total: 24000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022745132446289062
  StateBufferConnector_ms: 0.00131988525390625
  ViewRequirementAgentConnector_ms: 0.1132192611694336
counters:
  num_agent_steps_sampled: 24000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 24000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-05
done: false
episode_len_mean: 128.67
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 128.67
episode_reward_min: 13.0
episodes_this_iter: 18
episodes_total: 443
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5507.181818181818
      num_env_steps_trained: 4000.0
      total_loss: 9.606631495735861
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.20000000298023224
      curr_lr: 5.0e-05
      entropy: 0.5472295636480505
      mean_kl_loss: 0.00841772261869264
      policy_loss: -0.19867875494740225
      total_loss: 9.606631495735861
      vf_explained_var: -0.01743915406140414
      vf_loss: 9.80362675406716
      vf_loss_unclipped: 4081.337091619318
  num_agent_steps_sampled: 24000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 24000
  num_env_steps_trained: 0
iterations_since_restore: 6
node_ip: 127.0.0.1
num_agent_steps_sampled: 24000
num_agent_steps_trained: 0
num_env_steps_sampled: 24000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 624.6445208952202
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 25.244444444444444
  ram_util_percent: 56.6
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05590859761891686
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.0231157850594425
  mean_inference_ms: 1.016806085024046
  mean_raw_obs_processing_ms: 0.21995664820404762
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022745132446289062
    StateBufferConnector_ms: 0.00131988525390625
    ViewRequirementAgentConnector_ms: 0.1132192611694336
  custom_metrics: {}
  episode_len_mean: 128.67
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 128.67
  episode_reward_min: 13.0
  episodes_this_iter: 18
  hist_stats:
    episode_lengths: [41, 93, 46, 27, 53, 56, 28, 70, 55, 113, 62, 30, 57, 16, 71,
      47, 61, 35, 35, 162, 189, 123, 103, 84, 110, 64, 117, 218, 119, 228, 22, 110,
      125, 16, 41, 55, 90, 126, 84, 111, 97, 57, 71, 89, 128, 69, 92, 53, 145, 166,
      13, 103, 87, 72, 162, 117, 105, 136, 103, 90, 84, 144, 100, 53, 396, 50, 125,
      271, 201, 383, 176, 143, 97, 105, 325, 185, 224, 132, 65, 175, 178, 159, 195,
      500, 32, 88, 144, 154, 181, 500, 140, 307, 258, 208, 215, 147, 90, 140, 304,
      245]
    episode_reward: [41.0, 93.0, 46.0, 27.0, 53.0, 56.0, 28.0, 70.0, 55.0, 113.0,
      62.0, 30.0, 57.0, 16.0, 71.0, 47.0, 61.0, 35.0, 35.0, 162.0, 189.0, 123.0, 103.0,
      84.0, 110.0, 64.0, 117.0, 218.0, 119.0, 228.0, 22.0, 110.0, 125.0, 16.0, 41.0,
      55.0, 90.0, 126.0, 84.0, 111.0, 97.0, 57.0, 71.0, 89.0, 128.0, 69.0, 92.0, 53.0,
      145.0, 166.0, 13.0, 103.0, 87.0, 72.0, 162.0, 117.0, 105.0, 136.0, 103.0, 90.0,
      84.0, 144.0, 100.0, 53.0, 396.0, 50.0, 125.0, 271.0, 201.0, 383.0, 176.0, 143.0,
      97.0, 105.0, 325.0, 185.0, 224.0, 132.0, 65.0, 175.0, 178.0, 159.0, 195.0, 500.0,
      32.0, 88.0, 144.0, 154.0, 181.0, 500.0, 140.0, 307.0, 258.0, 208.0, 215.0, 147.0,
      90.0, 140.0, 304.0, 245.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05590859761891686
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.0231157850594425
    mean_inference_ms: 1.016806085024046
    mean_raw_obs_processing_ms: 0.21995664820404762
time_since_restore: 46.1167893409729
time_this_iter_s: 6.404939889907837
time_total_s: 46.1167893409729
timers:
  sample_time_ms: 2667.873
  synch_weights_time_ms: 3.49
  training_iteration_time_ms: 7684.34
timestamp: 1703047625
timesteps_total: 24000
training_iteration: 6
trial_id: default
-----------------------
----------------------

Iteration 6:
agent_timesteps_total: 28000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022749900817871094
  StateBufferConnector_ms: 0.0012848377227783203
  ViewRequirementAgentConnector_ms: 0.1124720573425293
counters:
  num_agent_steps_sampled: 28000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 28000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-11
done: false
episode_len_mean: 157.21
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 157.21
episode_reward_min: 13.0
episodes_this_iter: 17
episodes_total: 460
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5565.454545454545
      num_env_steps_trained: 4000.0
      total_loss: 9.628596045754172
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.10000000149011612
      curr_lr: 5.0e-05
      entropy: 0.5377016446807168
      mean_kl_loss: 0.0039782673090371645
      policy_loss: -0.17750791324810547
      total_loss: 9.628596045754172
      vf_explained_var: -0.04170053655450994
      vf_loss: 9.805308125235818
      vf_loss_unclipped: 3889.5883567116475
  num_agent_steps_sampled: 28000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 28000
  num_env_steps_trained: 0
iterations_since_restore: 7
node_ip: 127.0.0.1
num_agent_steps_sampled: 28000
num_agent_steps_trained: 0
num_env_steps_sampled: 28000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 625.1722163882592
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.966666666666665
  ram_util_percent: 56.6888888888889
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05578563407948412
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02300226232463972
  mean_inference_ms: 1.016349544571616
  mean_raw_obs_processing_ms: 0.21803797012221895
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022749900817871094
    StateBufferConnector_ms: 0.0012848377227783203
    ViewRequirementAgentConnector_ms: 0.1124720573425293
  custom_metrics: {}
  episode_len_mean: 157.21
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 157.21
  episode_reward_min: 13.0
  episodes_this_iter: 17
  hist_stats:
    episode_lengths: [35, 35, 162, 189, 123, 103, 84, 110, 64, 117, 218, 119, 228,
      22, 110, 125, 16, 41, 55, 90, 126, 84, 111, 97, 57, 71, 89, 128, 69, 92, 53,
      145, 166, 13, 103, 87, 72, 162, 117, 105, 136, 103, 90, 84, 144, 100, 53, 396,
      50, 125, 271, 201, 383, 176, 143, 97, 105, 325, 185, 224, 132, 65, 175, 178,
      159, 195, 500, 32, 88, 144, 154, 181, 500, 140, 307, 258, 208, 215, 147, 90,
      140, 304, 245, 247, 140, 209, 207, 164, 191, 254, 270, 121, 321, 205, 285, 235,
      500, 158, 150, 123]
    episode_reward: [35.0, 35.0, 162.0, 189.0, 123.0, 103.0, 84.0, 110.0, 64.0, 117.0,
      218.0, 119.0, 228.0, 22.0, 110.0, 125.0, 16.0, 41.0, 55.0, 90.0, 126.0, 84.0,
      111.0, 97.0, 57.0, 71.0, 89.0, 128.0, 69.0, 92.0, 53.0, 145.0, 166.0, 13.0,
      103.0, 87.0, 72.0, 162.0, 117.0, 105.0, 136.0, 103.0, 90.0, 84.0, 144.0, 100.0,
      53.0, 396.0, 50.0, 125.0, 271.0, 201.0, 383.0, 176.0, 143.0, 97.0, 105.0, 325.0,
      185.0, 224.0, 132.0, 65.0, 175.0, 178.0, 159.0, 195.0, 500.0, 32.0, 88.0, 144.0,
      154.0, 181.0, 500.0, 140.0, 307.0, 258.0, 208.0, 215.0, 147.0, 90.0, 140.0,
      304.0, 245.0, 247.0, 140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0,
      321.0, 205.0, 285.0, 235.0, 500.0, 158.0, 150.0, 123.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05578563407948412
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02300226232463972
    mean_inference_ms: 1.016349544571616
    mean_raw_obs_processing_ms: 0.21803797012221895
time_since_restore: 52.51633143424988
time_this_iter_s: 6.3995420932769775
time_total_s: 52.51633143424988
timers:
  sample_time_ms: 2660.111
  synch_weights_time_ms: 3.712
  training_iteration_time_ms: 7500.609
timestamp: 1703047631
timesteps_total: 28000
training_iteration: 7
trial_id: default
-----------------------
----------------------

Iteration 7:
agent_timesteps_total: 32000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002263784408569336
  StateBufferConnector_ms: 0.0012962818145751953
  ViewRequirementAgentConnector_ms: 0.11251449584960938
counters:
  num_agent_steps_sampled: 32000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 32000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-18
done: false
episode_len_mean: 178.39
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 178.39
episode_reward_min: 13.0
episodes_this_iter: 15
episodes_total: 475
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5505.5
      num_env_steps_trained: 4000.0
      total_loss: 9.624938488006592
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.10000000149011612
      curr_lr: 5.0e-05
      entropy: 0.5188863358714364
      mean_kl_loss: 0.005733250161286885
      policy_loss: -0.20277570323510605
      total_loss: 9.624938488006592
      vf_explained_var: -0.0423760630867698
      vf_loss: 9.827140721407803
      vf_loss_unclipped: 4193.2059326171875
  num_agent_steps_sampled: 32000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 32000
  num_env_steps_trained: 0
iterations_since_restore: 8
node_ip: 127.0.0.1
num_agent_steps_sampled: 32000
num_agent_steps_trained: 0
num_env_steps_sampled: 32000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 621.8239648174551
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.533333333333335
  ram_util_percent: 56.76666666666667
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05570770359128683
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02295815542451094
  mean_inference_ms: 1.0154977780682293
  mean_raw_obs_processing_ms: 0.21649635255483649
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002263784408569336
    StateBufferConnector_ms: 0.0012962818145751953
    ViewRequirementAgentConnector_ms: 0.11251449584960938
  custom_metrics: {}
  episode_len_mean: 178.39
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 178.39
  episode_reward_min: 13.0
  episodes_this_iter: 15
  hist_stats:
    episode_lengths: [125, 16, 41, 55, 90, 126, 84, 111, 97, 57, 71, 89, 128, 69,
      92, 53, 145, 166, 13, 103, 87, 72, 162, 117, 105, 136, 103, 90, 84, 144, 100,
      53, 396, 50, 125, 271, 201, 383, 176, 143, 97, 105, 325, 185, 224, 132, 65,
      175, 178, 159, 195, 500, 32, 88, 144, 154, 181, 500, 140, 307, 258, 208, 215,
      147, 90, 140, 304, 245, 247, 140, 209, 207, 164, 191, 254, 270, 121, 321, 205,
      285, 235, 500, 158, 150, 123, 122, 249, 196, 315, 324, 328, 209, 436, 199, 132,
      500, 241, 201, 170, 215]
    episode_reward: [125.0, 16.0, 41.0, 55.0, 90.0, 126.0, 84.0, 111.0, 97.0, 57.0,
      71.0, 89.0, 128.0, 69.0, 92.0, 53.0, 145.0, 166.0, 13.0, 103.0, 87.0, 72.0,
      162.0, 117.0, 105.0, 136.0, 103.0, 90.0, 84.0, 144.0, 100.0, 53.0, 396.0, 50.0,
      125.0, 271.0, 201.0, 383.0, 176.0, 143.0, 97.0, 105.0, 325.0, 185.0, 224.0,
      132.0, 65.0, 175.0, 178.0, 159.0, 195.0, 500.0, 32.0, 88.0, 144.0, 154.0, 181.0,
      500.0, 140.0, 307.0, 258.0, 208.0, 215.0, 147.0, 90.0, 140.0, 304.0, 245.0,
      247.0, 140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0, 321.0, 205.0,
      285.0, 235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0, 196.0, 315.0, 324.0,
      328.0, 209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0, 170.0, 215.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05570770359128683
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02295815542451094
    mean_inference_ms: 1.0154977780682293
    mean_raw_obs_processing_ms: 0.21649635255483649
time_since_restore: 58.95024561882019
time_this_iter_s: 6.4339141845703125
time_total_s: 58.95024561882019
timers:
  sample_time_ms: 2651.134
  synch_weights_time_ms: 3.62
  training_iteration_time_ms: 7367.118
timestamp: 1703047638
timesteps_total: 32000
training_iteration: 8
trial_id: default
-----------------------
----------------------

Iteration 8:
agent_timesteps_total: 36000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022735595703125
  StateBufferConnector_ms: 0.0012829303741455078
  ViewRequirementAgentConnector_ms: 0.1119844913482666
counters:
  num_agent_steps_sampled: 36000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 36000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-24
done: false
episode_len_mean: 213.41
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 213.41
episode_reward_min: 13.0
episodes_this_iter: 10
episodes_total: 485
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5626.772727272727
      num_env_steps_trained: 4000.0
      total_loss: 9.722584247589111
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.05000000074505806
      curr_lr: 5.0e-05
      entropy: 0.5180246288126166
      mean_kl_loss: 0.0033464745257434977
      policy_loss: -0.1894088245250962
      total_loss: 9.722584247589111
      vf_explained_var: -0.015384641560641203
      vf_loss: 9.911658200350674
      vf_loss_unclipped: 5329.931529651989
  num_agent_steps_sampled: 36000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 36000
  num_env_steps_trained: 0
iterations_since_restore: 9
node_ip: 127.0.0.1
num_agent_steps_sampled: 36000
num_agent_steps_trained: 0
num_env_steps_sampled: 36000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 625.6466709924746
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.140000000000004
  ram_util_percent: 56.85999999999999
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05564752813261508
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02291540141363097
  mean_inference_ms: 1.01508008896351
  mean_raw_obs_processing_ms: 0.21542270480431092
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022735595703125
    StateBufferConnector_ms: 0.0012829303741455078
    ViewRequirementAgentConnector_ms: 0.1119844913482666
  custom_metrics: {}
  episode_len_mean: 213.41
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 213.41
  episode_reward_min: 13.0
  episodes_this_iter: 10
  hist_stats:
    episode_lengths: [71, 89, 128, 69, 92, 53, 145, 166, 13, 103, 87, 72, 162, 117,
      105, 136, 103, 90, 84, 144, 100, 53, 396, 50, 125, 271, 201, 383, 176, 143,
      97, 105, 325, 185, 224, 132, 65, 175, 178, 159, 195, 500, 32, 88, 144, 154,
      181, 500, 140, 307, 258, 208, 215, 147, 90, 140, 304, 245, 247, 140, 209, 207,
      164, 191, 254, 270, 121, 321, 205, 285, 235, 500, 158, 150, 123, 122, 249, 196,
      315, 324, 328, 209, 436, 199, 132, 500, 241, 201, 170, 215, 500, 403, 500, 193,
      500, 500, 500, 433, 275, 500]
    episode_reward: [71.0, 89.0, 128.0, 69.0, 92.0, 53.0, 145.0, 166.0, 13.0, 103.0,
      87.0, 72.0, 162.0, 117.0, 105.0, 136.0, 103.0, 90.0, 84.0, 144.0, 100.0, 53.0,
      396.0, 50.0, 125.0, 271.0, 201.0, 383.0, 176.0, 143.0, 97.0, 105.0, 325.0, 185.0,
      224.0, 132.0, 65.0, 175.0, 178.0, 159.0, 195.0, 500.0, 32.0, 88.0, 144.0, 154.0,
      181.0, 500.0, 140.0, 307.0, 258.0, 208.0, 215.0, 147.0, 90.0, 140.0, 304.0,
      245.0, 247.0, 140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0, 321.0,
      205.0, 285.0, 235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0, 196.0, 315.0,
      324.0, 328.0, 209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0, 170.0, 215.0,
      500.0, 403.0, 500.0, 193.0, 500.0, 500.0, 500.0, 433.0, 275.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05564752813261508
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02291540141363097
    mean_inference_ms: 1.01508008896351
    mean_raw_obs_processing_ms: 0.21542270480431092
time_since_restore: 65.34485363960266
time_this_iter_s: 6.394608020782471
time_total_s: 65.34485363960266
timers:
  sample_time_ms: 2643.916
  synch_weights_time_ms: 3.562
  training_iteration_time_ms: 7258.924
timestamp: 1703047644
timesteps_total: 36000
training_iteration: 9
trial_id: default
-----------------------
----------------------

Iteration 9:
agent_timesteps_total: 40000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022623538970947266
  StateBufferConnector_ms: 0.0012867450714111328
  ViewRequirementAgentConnector_ms: 0.11226534843444824
counters:
  num_agent_steps_sampled: 40000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 40000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-31
done: false
episode_len_mean: 238.77
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 238.77
episode_reward_min: 32.0
episodes_this_iter: 15
episodes_total: 500
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5565.863636363636
      num_env_steps_trained: 4000.0
      total_loss: 9.636374560269443
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.02500000037252903
      curr_lr: 5.0e-05
      entropy: 0.5122001279484142
      mean_kl_loss: 0.004258019153734371
      policy_loss: -0.18158735199408096
      total_loss: 9.636374560269443
      vf_explained_var: -0.04451508955522017
      vf_loss: 9.817749153483998
      vf_loss_unclipped: 4117.8890824751425
  num_agent_steps_sampled: 40000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 40000
  num_env_steps_trained: 0
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 40000
num_agent_steps_trained: 0
num_env_steps_sampled: 40000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 628.2459596388717
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.022222222222222
  ram_util_percent: 56.855555555555554
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05555367225014411
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022829536230741257
  mean_inference_ms: 1.0148545068142418
  mean_raw_obs_processing_ms: 0.21379263087606468
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022623538970947266
    StateBufferConnector_ms: 0.0012867450714111328
    ViewRequirementAgentConnector_ms: 0.11226534843444824
  custom_metrics: {}
  episode_len_mean: 238.77
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 238.77
  episode_reward_min: 32.0
  episodes_this_iter: 15
  hist_stats:
    episode_lengths: [136, 103, 90, 84, 144, 100, 53, 396, 50, 125, 271, 201, 383,
      176, 143, 97, 105, 325, 185, 224, 132, 65, 175, 178, 159, 195, 500, 32, 88,
      144, 154, 181, 500, 140, 307, 258, 208, 215, 147, 90, 140, 304, 245, 247, 140,
      209, 207, 164, 191, 254, 270, 121, 321, 205, 285, 235, 500, 158, 150, 123, 122,
      249, 196, 315, 324, 328, 209, 436, 199, 132, 500, 241, 201, 170, 215, 500, 403,
      500, 193, 500, 500, 500, 433, 275, 500, 315, 189, 388, 334, 322, 148, 132, 259,
      124, 304, 371, 301, 182, 441, 198]
    episode_reward: [136.0, 103.0, 90.0, 84.0, 144.0, 100.0, 53.0, 396.0, 50.0, 125.0,
      271.0, 201.0, 383.0, 176.0, 143.0, 97.0, 105.0, 325.0, 185.0, 224.0, 132.0,
      65.0, 175.0, 178.0, 159.0, 195.0, 500.0, 32.0, 88.0, 144.0, 154.0, 181.0, 500.0,
      140.0, 307.0, 258.0, 208.0, 215.0, 147.0, 90.0, 140.0, 304.0, 245.0, 247.0,
      140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0, 321.0, 205.0, 285.0,
      235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0, 196.0, 315.0, 324.0, 328.0,
      209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0, 170.0, 215.0, 500.0, 403.0,
      500.0, 193.0, 500.0, 500.0, 500.0, 433.0, 275.0, 500.0, 315.0, 189.0, 388.0,
      334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0, 371.0, 301.0, 182.0, 441.0,
      198.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05555367225014411
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022829536230741257
    mean_inference_ms: 1.0148545068142418
    mean_raw_obs_processing_ms: 0.21379263087606468
time_since_restore: 71.71300435066223
time_this_iter_s: 6.36815071105957
time_total_s: 71.71300435066223
timers:
  sample_time_ms: 2640.577
  synch_weights_time_ms: 3.506
  training_iteration_time_ms: 7169.724
timestamp: 1703047651
timesteps_total: 40000
training_iteration: 10
trial_id: default
-----------------------
----------------------

Iteration 10:
agent_timesteps_total: 44000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002239704132080078
  StateBufferConnector_ms: 0.0012843608856201172
  ViewRequirementAgentConnector_ms: 0.11235380172729492
counters:
  num_agent_steps_sampled: 44000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 44000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-37
done: false
episode_len_mean: 262.19
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 262.19
episode_reward_min: 32.0
episodes_this_iter: 11
episodes_total: 511
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5690.272727272727
      num_env_steps_trained: 4000.0
      total_loss: 9.722087079828436
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.012500000186264515
      curr_lr: 5.0e-05
      entropy: 0.5119462040337649
      mean_kl_loss: 0.002858533749340288
      policy_loss: -0.1657178611917929
      total_loss: 9.722087079828436
      vf_explained_var: -0.028280556201934814
      vf_loss: 9.887733459472656
      vf_loss_unclipped: 4979.463312322443
  num_agent_steps_sampled: 44000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 44000
  num_env_steps_trained: 0
iterations_since_restore: 11
node_ip: 127.0.0.1
num_agent_steps_sampled: 44000
num_agent_steps_trained: 0
num_env_steps_sampled: 44000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 621.7510525568363
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.2
  ram_util_percent: 56.72222222222222
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055504520258575364
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022802158444120348
  mean_inference_ms: 1.014262058575132
  mean_raw_obs_processing_ms: 0.21278183515147162
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002239704132080078
    StateBufferConnector_ms: 0.0012843608856201172
    ViewRequirementAgentConnector_ms: 0.11235380172729492
  custom_metrics: {}
  episode_len_mean: 262.19
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 262.19
  episode_reward_min: 32.0
  episodes_this_iter: 11
  hist_stats:
    episode_lengths: [201, 383, 176, 143, 97, 105, 325, 185, 224, 132, 65, 175, 178,
      159, 195, 500, 32, 88, 144, 154, 181, 500, 140, 307, 258, 208, 215, 147, 90,
      140, 304, 245, 247, 140, 209, 207, 164, 191, 254, 270, 121, 321, 205, 285, 235,
      500, 158, 150, 123, 122, 249, 196, 315, 324, 328, 209, 436, 199, 132, 500, 241,
      201, 170, 215, 500, 403, 500, 193, 500, 500, 500, 433, 275, 500, 315, 189, 388,
      334, 322, 148, 132, 259, 124, 304, 371, 301, 182, 441, 198, 500, 289, 500, 222,
      332, 191, 367, 305, 352, 500, 336]
    episode_reward: [201.0, 383.0, 176.0, 143.0, 97.0, 105.0, 325.0, 185.0, 224.0,
      132.0, 65.0, 175.0, 178.0, 159.0, 195.0, 500.0, 32.0, 88.0, 144.0, 154.0, 181.0,
      500.0, 140.0, 307.0, 258.0, 208.0, 215.0, 147.0, 90.0, 140.0, 304.0, 245.0,
      247.0, 140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0, 321.0, 205.0,
      285.0, 235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0, 196.0, 315.0, 324.0,
      328.0, 209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0, 170.0, 215.0, 500.0,
      403.0, 500.0, 193.0, 500.0, 500.0, 500.0, 433.0, 275.0, 500.0, 315.0, 189.0,
      388.0, 334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0, 371.0, 301.0, 182.0,
      441.0, 198.0, 500.0, 289.0, 500.0, 222.0, 332.0, 191.0, 367.0, 305.0, 352.0,
      500.0, 336.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055504520258575364
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022802158444120348
    mean_inference_ms: 1.014262058575132
    mean_raw_obs_processing_ms: 0.21278183515147162
time_since_restore: 78.14764761924744
time_this_iter_s: 6.434643268585205
time_total_s: 78.14764761924744
timers:
  sample_time_ms: 2630.234
  synch_weights_time_ms: 3.492
  training_iteration_time_ms: 6790.012
timestamp: 1703047657
timesteps_total: 44000
training_iteration: 11
trial_id: default
-----------------------
----------------------

Iteration 11:
agent_timesteps_total: 48000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022573471069335938
  StateBufferConnector_ms: 0.0012691020965576172
  ViewRequirementAgentConnector_ms: 0.11258482933044434
counters:
  num_agent_steps_sampled: 48000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 48000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-44
done: false
episode_len_mean: 278.92
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 278.92
episode_reward_min: 32.0
episodes_this_iter: 12
episodes_total: 523
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5626.090909090909
      num_env_steps_trained: 4000.0
      total_loss: 9.682711948047984
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.0062500000931322575
      curr_lr: 5.0e-05
      entropy: 0.5049434439702467
      mean_kl_loss: 0.004995108630277462
      policy_loss: -0.17453362928195434
      total_loss: 9.682711948047984
      vf_explained_var: -0.05139146067879417
      vf_loss: 9.85718367316506
      vf_loss_unclipped: 4476.690007990057
  num_agent_steps_sampled: 48000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 48000
  num_env_steps_trained: 0
iterations_since_restore: 12
node_ip: 127.0.0.1
num_agent_steps_sampled: 48000
num_agent_steps_trained: 0
num_env_steps_sampled: 48000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 623.641055609205
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.08888888888889
  ram_util_percent: 56.76666666666667
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05544669969277704
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022756716478483544
  mean_inference_ms: 1.0139282257914437
  mean_raw_obs_processing_ms: 0.21169835637529769
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022573471069335938
    StateBufferConnector_ms: 0.0012691020965576172
    ViewRequirementAgentConnector_ms: 0.11258482933044434
  custom_metrics: {}
  episode_len_mean: 278.92
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 278.92
  episode_reward_min: 32.0
  episodes_this_iter: 12
  hist_stats:
    episode_lengths: [178, 159, 195, 500, 32, 88, 144, 154, 181, 500, 140, 307, 258,
      208, 215, 147, 90, 140, 304, 245, 247, 140, 209, 207, 164, 191, 254, 270, 121,
      321, 205, 285, 235, 500, 158, 150, 123, 122, 249, 196, 315, 324, 328, 209, 436,
      199, 132, 500, 241, 201, 170, 215, 500, 403, 500, 193, 500, 500, 500, 433, 275,
      500, 315, 189, 388, 334, 322, 148, 132, 259, 124, 304, 371, 301, 182, 441, 198,
      500, 289, 500, 222, 332, 191, 367, 305, 352, 500, 336, 480, 292, 264, 153, 357,
      174, 500, 431, 293, 161, 391, 388]
    episode_reward: [178.0, 159.0, 195.0, 500.0, 32.0, 88.0, 144.0, 154.0, 181.0,
      500.0, 140.0, 307.0, 258.0, 208.0, 215.0, 147.0, 90.0, 140.0, 304.0, 245.0,
      247.0, 140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0, 321.0, 205.0,
      285.0, 235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0, 196.0, 315.0, 324.0,
      328.0, 209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0, 170.0, 215.0, 500.0,
      403.0, 500.0, 193.0, 500.0, 500.0, 500.0, 433.0, 275.0, 500.0, 315.0, 189.0,
      388.0, 334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0, 371.0, 301.0, 182.0,
      441.0, 198.0, 500.0, 289.0, 500.0, 222.0, 332.0, 191.0, 367.0, 305.0, 352.0,
      500.0, 336.0, 480.0, 292.0, 264.0, 153.0, 357.0, 174.0, 500.0, 431.0, 293.0,
      161.0, 391.0, 388.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05544669969277704
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022756716478483544
    mean_inference_ms: 1.0139282257914437
    mean_raw_obs_processing_ms: 0.21169835637529769
time_since_restore: 84.56345462799072
time_this_iter_s: 6.415807008743286
time_total_s: 84.56345462799072
timers:
  sample_time_ms: 2610.759
  synch_weights_time_ms: 3.639
  training_iteration_time_ms: 6585.556
timestamp: 1703047664
timesteps_total: 48000
training_iteration: 12
trial_id: default
-----------------------
----------------------

Iteration 12:
agent_timesteps_total: 52000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002249479293823242
  StateBufferConnector_ms: 0.0012671947479248047
  ViewRequirementAgentConnector_ms: 0.11243677139282227
counters:
  num_agent_steps_sampled: 52000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 52000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-50
done: false
episode_len_mean: 296.69
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 296.69
episode_reward_min: 90.0
episodes_this_iter: 11
episodes_total: 534
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5688.863636363636
      num_env_steps_trained: 4000.0
      total_loss: 9.709858244115656
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.0062500000931322575
      curr_lr: 5.0e-05
      entropy: 0.5214489508758892
      mean_kl_loss: 0.006151278115345801
      policy_loss: -0.1598238470879468
      total_loss: 9.709858244115656
      vf_explained_var: -0.045652600851925934
      vf_loss: 9.869643644853072
      vf_loss_unclipped: 4702.668212890625
  num_agent_steps_sampled: 52000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 52000
  num_env_steps_trained: 0
iterations_since_restore: 13
node_ip: 127.0.0.1
num_agent_steps_sampled: 52000
num_agent_steps_trained: 0
num_env_steps_sampled: 52000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 622.7928713796895
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.41111111111111
  ram_util_percent: 56.788888888888884
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05539670467349174
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022729857814783073
  mean_inference_ms: 1.0133224047334384
  mean_raw_obs_processing_ms: 0.21081272441136673
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002249479293823242
    StateBufferConnector_ms: 0.0012671947479248047
    ViewRequirementAgentConnector_ms: 0.11243677139282227
  custom_metrics: {}
  episode_len_mean: 296.69
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 296.69
  episode_reward_min: 90.0
  episodes_this_iter: 11
  hist_stats:
    episode_lengths: [307, 258, 208, 215, 147, 90, 140, 304, 245, 247, 140, 209, 207,
      164, 191, 254, 270, 121, 321, 205, 285, 235, 500, 158, 150, 123, 122, 249, 196,
      315, 324, 328, 209, 436, 199, 132, 500, 241, 201, 170, 215, 500, 403, 500, 193,
      500, 500, 500, 433, 275, 500, 315, 189, 388, 334, 322, 148, 132, 259, 124, 304,
      371, 301, 182, 441, 198, 500, 289, 500, 222, 332, 191, 367, 305, 352, 500, 336,
      480, 292, 264, 153, 357, 174, 500, 431, 293, 161, 391, 388, 500, 400, 296, 480,
      205, 466, 174, 431, 433, 339, 324]
    episode_reward: [307.0, 258.0, 208.0, 215.0, 147.0, 90.0, 140.0, 304.0, 245.0,
      247.0, 140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0, 321.0, 205.0,
      285.0, 235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0, 196.0, 315.0, 324.0,
      328.0, 209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0, 170.0, 215.0, 500.0,
      403.0, 500.0, 193.0, 500.0, 500.0, 500.0, 433.0, 275.0, 500.0, 315.0, 189.0,
      388.0, 334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0, 371.0, 301.0, 182.0,
      441.0, 198.0, 500.0, 289.0, 500.0, 222.0, 332.0, 191.0, 367.0, 305.0, 352.0,
      500.0, 336.0, 480.0, 292.0, 264.0, 153.0, 357.0, 174.0, 500.0, 431.0, 293.0,
      161.0, 391.0, 388.0, 500.0, 400.0, 296.0, 480.0, 205.0, 466.0, 174.0, 431.0,
      433.0, 339.0, 324.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05539670467349174
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022729857814783073
    mean_inference_ms: 1.0133224047334384
    mean_raw_obs_processing_ms: 0.21081272441136673
time_since_restore: 90.98732137680054
time_this_iter_s: 6.4238667488098145
time_total_s: 90.98732137680054
timers:
  sample_time_ms: 2601.413
  synch_weights_time_ms: 3.424
  training_iteration_time_ms: 6477.346
timestamp: 1703047670
timesteps_total: 52000
training_iteration: 13
trial_id: default
-----------------------
----------------------

Iteration 13:
agent_timesteps_total: 56000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002268075942993164
  StateBufferConnector_ms: 0.0012612342834472656
  ViewRequirementAgentConnector_ms: 0.11218953132629395
counters:
  num_agent_steps_sampled: 56000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 56000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-47-56
done: false
episode_len_mean: 312.2
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 312.2
episode_reward_min: 102.0
episodes_this_iter: 10
episodes_total: 544
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5626.954545454545
      num_env_steps_trained: 4000.0
      total_loss: 9.71804089979692
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.0062500000931322575
      curr_lr: 5.0e-05
      entropy: 0.5027322159572081
      mean_kl_loss: 0.0061400439284062195
      policy_loss: -0.1889824467626485
      total_loss: 9.71804089979692
      vf_explained_var: -0.04156963933597912
      vf_loss: 9.906985152851451
      vf_loss_unclipped: 5187.161288174716
  num_agent_steps_sampled: 56000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 56000
  num_env_steps_trained: 0
iterations_since_restore: 14
node_ip: 127.0.0.1
num_agent_steps_sampled: 56000
num_agent_steps_trained: 0
num_env_steps_sampled: 56000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 629.5468810513181
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.455555555555556
  ram_util_percent: 56.7
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055345404115335325
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022692154383300734
  mean_inference_ms: 1.0129934631698425
  mean_raw_obs_processing_ms: 0.21004657082571287
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002268075942993164
    StateBufferConnector_ms: 0.0012612342834472656
    ViewRequirementAgentConnector_ms: 0.11218953132629395
  custom_metrics: {}
  episode_len_mean: 312.2
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 312.2
  episode_reward_min: 102.0
  episodes_this_iter: 10
  hist_stats:
    episode_lengths: [140, 209, 207, 164, 191, 254, 270, 121, 321, 205, 285, 235,
      500, 158, 150, 123, 122, 249, 196, 315, 324, 328, 209, 436, 199, 132, 500, 241,
      201, 170, 215, 500, 403, 500, 193, 500, 500, 500, 433, 275, 500, 315, 189, 388,
      334, 322, 148, 132, 259, 124, 304, 371, 301, 182, 441, 198, 500, 289, 500, 222,
      332, 191, 367, 305, 352, 500, 336, 480, 292, 264, 153, 357, 174, 500, 431, 293,
      161, 391, 388, 500, 400, 296, 480, 205, 466, 174, 431, 433, 339, 324, 402, 446,
      500, 500, 204, 498, 184, 376, 102, 500]
    episode_reward: [140.0, 209.0, 207.0, 164.0, 191.0, 254.0, 270.0, 121.0, 321.0,
      205.0, 285.0, 235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0, 196.0, 315.0,
      324.0, 328.0, 209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0, 170.0, 215.0,
      500.0, 403.0, 500.0, 193.0, 500.0, 500.0, 500.0, 433.0, 275.0, 500.0, 315.0,
      189.0, 388.0, 334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0, 371.0, 301.0,
      182.0, 441.0, 198.0, 500.0, 289.0, 500.0, 222.0, 332.0, 191.0, 367.0, 305.0,
      352.0, 500.0, 336.0, 480.0, 292.0, 264.0, 153.0, 357.0, 174.0, 500.0, 431.0,
      293.0, 161.0, 391.0, 388.0, 500.0, 400.0, 296.0, 480.0, 205.0, 466.0, 174.0,
      431.0, 433.0, 339.0, 324.0, 402.0, 446.0, 500.0, 500.0, 204.0, 498.0, 184.0,
      376.0, 102.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055345404115335325
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022692154383300734
    mean_inference_ms: 1.0129934631698425
    mean_raw_obs_processing_ms: 0.21004657082571287
time_since_restore: 97.3422634601593
time_this_iter_s: 6.354942083358765
time_total_s: 97.3422634601593
timers:
  sample_time_ms: 2598.626
  synch_weights_time_ms: 3.428
  training_iteration_time_ms: 6420.552
timestamp: 1703047676
timesteps_total: 56000
training_iteration: 14
trial_id: default
-----------------------
----------------------

Iteration 14:
agent_timesteps_total: 60000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002233743667602539
  StateBufferConnector_ms: 0.0012664794921875
  ViewRequirementAgentConnector_ms: 0.11214089393615723
counters:
  num_agent_steps_sampled: 60000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 60000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-03
done: false
episode_len_mean: 334.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 334.0
episode_reward_min: 102.0
episodes_this_iter: 9
episodes_total: 553
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5688.227272727273
      num_env_steps_trained: 4000.0
      total_loss: 9.747128183191473
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.0031250000465661287
      curr_lr: 5.0e-05
      entropy: 0.5018401647155936
      mean_kl_loss: 0.0033786563446989767
      policy_loss: -0.1700654879889705
      total_loss: 9.747128183191473
      vf_explained_var: -0.05228215456008911
      vf_loss: 9.91717260534113
      vf_loss_unclipped: 5184.956787109375
  num_agent_steps_sampled: 60000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 60000
  num_env_steps_trained: 0
iterations_since_restore: 15
node_ip: 127.0.0.1
num_agent_steps_sampled: 60000
num_agent_steps_trained: 0
num_env_steps_sampled: 60000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 629.0256919774667
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.311111111111114
  ram_util_percent: 56.7
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.0553090692156972
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022676155126247096
  mean_inference_ms: 1.0123371628631104
  mean_raw_obs_processing_ms: 0.20942914330305004
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002233743667602539
    StateBufferConnector_ms: 0.0012664794921875
    ViewRequirementAgentConnector_ms: 0.11214089393615723
  custom_metrics: {}
  episode_len_mean: 334.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 334.0
  episode_reward_min: 102.0
  episodes_this_iter: 9
  hist_stats:
    episode_lengths: [205, 285, 235, 500, 158, 150, 123, 122, 249, 196, 315, 324,
      328, 209, 436, 199, 132, 500, 241, 201, 170, 215, 500, 403, 500, 193, 500, 500,
      500, 433, 275, 500, 315, 189, 388, 334, 322, 148, 132, 259, 124, 304, 371, 301,
      182, 441, 198, 500, 289, 500, 222, 332, 191, 367, 305, 352, 500, 336, 480, 292,
      264, 153, 357, 174, 500, 431, 293, 161, 391, 388, 500, 400, 296, 480, 205, 466,
      174, 431, 433, 339, 324, 402, 446, 500, 500, 204, 498, 184, 376, 102, 500, 500,
      500, 489, 500, 500, 410, 194, 500, 464]
    episode_reward: [205.0, 285.0, 235.0, 500.0, 158.0, 150.0, 123.0, 122.0, 249.0,
      196.0, 315.0, 324.0, 328.0, 209.0, 436.0, 199.0, 132.0, 500.0, 241.0, 201.0,
      170.0, 215.0, 500.0, 403.0, 500.0, 193.0, 500.0, 500.0, 500.0, 433.0, 275.0,
      500.0, 315.0, 189.0, 388.0, 334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0,
      371.0, 301.0, 182.0, 441.0, 198.0, 500.0, 289.0, 500.0, 222.0, 332.0, 191.0,
      367.0, 305.0, 352.0, 500.0, 336.0, 480.0, 292.0, 264.0, 153.0, 357.0, 174.0,
      500.0, 431.0, 293.0, 161.0, 391.0, 388.0, 500.0, 400.0, 296.0, 480.0, 205.0,
      466.0, 174.0, 431.0, 433.0, 339.0, 324.0, 402.0, 446.0, 500.0, 500.0, 204.0,
      498.0, 184.0, 376.0, 102.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 410.0,
      194.0, 500.0, 464.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0553090692156972
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022676155126247096
    mean_inference_ms: 1.0123371628631104
    mean_raw_obs_processing_ms: 0.20942914330305004
time_since_restore: 103.70252847671509
time_this_iter_s: 6.360265016555786
time_total_s: 103.70252847671509
timers:
  sample_time_ms: 2598.191
  synch_weights_time_ms: 3.431
  training_iteration_time_ms: 6397.767
timestamp: 1703047683
timesteps_total: 60000
training_iteration: 15
trial_id: default
-----------------------
----------------------

Iteration 15:
agent_timesteps_total: 64000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002263784408569336
  StateBufferConnector_ms: 0.0012488365173339844
  ViewRequirementAgentConnector_ms: 0.11222600936889648
counters:
  num_agent_steps_sampled: 64000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 64000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-09
done: false
episode_len_mean: 355.28
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 355.28
episode_reward_min: 102.0
episodes_this_iter: 9
episodes_total: 562
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5755.047619047619
      num_env_steps_trained: 4000.0
      total_loss: 9.774390629359655
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.0015625000232830644
      curr_lr: 5.0e-05
      entropy: 0.5088125694365728
      mean_kl_loss: 0.003633154053356217
      policy_loss: -0.15043767399731137
      total_loss: 9.774390629359655
      vf_explained_var: -0.047894046420142763
      vf_loss: 9.92481703985305
      vf_loss_unclipped: 5346.074869791667
  num_agent_steps_sampled: 64000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 64000
  num_env_steps_trained: 0
iterations_since_restore: 16
node_ip: 127.0.0.1
num_agent_steps_sampled: 64000
num_agent_steps_trained: 0
num_env_steps_sampled: 64000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.563793848181
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.22222222222222
  ram_util_percent: 56.77777777777778
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055272007019880574
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022648951846617388
  mean_inference_ms: 1.012087400926383
  mean_raw_obs_processing_ms: 0.2088685640768412
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002263784408569336
    StateBufferConnector_ms: 0.0012488365173339844
    ViewRequirementAgentConnector_ms: 0.11222600936889648
  custom_metrics: {}
  episode_len_mean: 355.28
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 355.28
  episode_reward_min: 102.0
  episodes_this_iter: 9
  hist_stats:
    episode_lengths: [196, 315, 324, 328, 209, 436, 199, 132, 500, 241, 201, 170,
      215, 500, 403, 500, 193, 500, 500, 500, 433, 275, 500, 315, 189, 388, 334, 322,
      148, 132, 259, 124, 304, 371, 301, 182, 441, 198, 500, 289, 500, 222, 332, 191,
      367, 305, 352, 500, 336, 480, 292, 264, 153, 357, 174, 500, 431, 293, 161, 391,
      388, 500, 400, 296, 480, 205, 466, 174, 431, 433, 339, 324, 402, 446, 500, 500,
      204, 498, 184, 376, 102, 500, 500, 500, 489, 500, 500, 410, 194, 500, 464, 500,
      283, 500, 500, 500, 389, 500, 500, 483]
    episode_reward: [196.0, 315.0, 324.0, 328.0, 209.0, 436.0, 199.0, 132.0, 500.0,
      241.0, 201.0, 170.0, 215.0, 500.0, 403.0, 500.0, 193.0, 500.0, 500.0, 500.0,
      433.0, 275.0, 500.0, 315.0, 189.0, 388.0, 334.0, 322.0, 148.0, 132.0, 259.0,
      124.0, 304.0, 371.0, 301.0, 182.0, 441.0, 198.0, 500.0, 289.0, 500.0, 222.0,
      332.0, 191.0, 367.0, 305.0, 352.0, 500.0, 336.0, 480.0, 292.0, 264.0, 153.0,
      357.0, 174.0, 500.0, 431.0, 293.0, 161.0, 391.0, 388.0, 500.0, 400.0, 296.0,
      480.0, 205.0, 466.0, 174.0, 431.0, 433.0, 339.0, 324.0, 402.0, 446.0, 500.0,
      500.0, 204.0, 498.0, 184.0, 376.0, 102.0, 500.0, 500.0, 500.0, 489.0, 500.0,
      500.0, 410.0, 194.0, 500.0, 464.0, 500.0, 283.0, 500.0, 500.0, 500.0, 389.0,
      500.0, 500.0, 483.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055272007019880574
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022648951846617388
    mean_inference_ms: 1.012087400926383
    mean_raw_obs_processing_ms: 0.2088685640768412
time_since_restore: 109.92882347106934
time_this_iter_s: 6.226294994354248
time_total_s: 109.92882347106934
timers:
  sample_time_ms: 2598.26
  synch_weights_time_ms: 3.426
  training_iteration_time_ms: 6379.91
timestamp: 1703047689
timesteps_total: 64000
training_iteration: 16
trial_id: default
-----------------------
----------------------

Iteration 16:
agent_timesteps_total: 68000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002245187759399414
  StateBufferConnector_ms: 0.0012464523315429688
  ViewRequirementAgentConnector_ms: 0.11239027976989746
counters:
  num_agent_steps_sampled: 68000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 68000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-15
done: false
episode_len_mean: 369.94
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 369.94
episode_reward_min: 102.0
episodes_this_iter: 9
episodes_total: 571
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5755.047619047619
      num_env_steps_trained: 4000.0
      total_loss: 9.770549138387045
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.0007812500116415322
      curr_lr: 5.0e-05
      entropy: 0.497353447335107
      mean_kl_loss: 0.003987587392121635
      policy_loss: -0.15314504539682752
      total_loss: 9.770549138387045
      vf_explained_var: -0.0493153901327224
      vf_loss: 9.923687980288552
      vf_loss_unclipped: 5257.012276785715
  num_agent_steps_sampled: 68000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 68000
  num_env_steps_trained: 0
iterations_since_restore: 17
node_ip: 127.0.0.1
num_agent_steps_sampled: 68000
num_agent_steps_trained: 0
num_env_steps_sampled: 68000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 643.120216430914
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.988888888888887
  ram_util_percent: 56.8
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055247412271003986
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02263277388459489
  mean_inference_ms: 1.011766145259254
  mean_raw_obs_processing_ms: 0.20838328768562028
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002245187759399414
    StateBufferConnector_ms: 0.0012464523315429688
    ViewRequirementAgentConnector_ms: 0.11239027976989746
  custom_metrics: {}
  episode_len_mean: 369.94
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 369.94
  episode_reward_min: 102.0
  episodes_this_iter: 9
  hist_stats:
    episode_lengths: [241, 201, 170, 215, 500, 403, 500, 193, 500, 500, 500, 433,
      275, 500, 315, 189, 388, 334, 322, 148, 132, 259, 124, 304, 371, 301, 182, 441,
      198, 500, 289, 500, 222, 332, 191, 367, 305, 352, 500, 336, 480, 292, 264, 153,
      357, 174, 500, 431, 293, 161, 391, 388, 500, 400, 296, 480, 205, 466, 174, 431,
      433, 339, 324, 402, 446, 500, 500, 204, 498, 184, 376, 102, 500, 500, 500, 489,
      500, 500, 410, 194, 500, 464, 500, 283, 500, 500, 500, 389, 500, 500, 483, 500,
      500, 500, 500, 500, 393, 468, 244, 500]
    episode_reward: [241.0, 201.0, 170.0, 215.0, 500.0, 403.0, 500.0, 193.0, 500.0,
      500.0, 500.0, 433.0, 275.0, 500.0, 315.0, 189.0, 388.0, 334.0, 322.0, 148.0,
      132.0, 259.0, 124.0, 304.0, 371.0, 301.0, 182.0, 441.0, 198.0, 500.0, 289.0,
      500.0, 222.0, 332.0, 191.0, 367.0, 305.0, 352.0, 500.0, 336.0, 480.0, 292.0,
      264.0, 153.0, 357.0, 174.0, 500.0, 431.0, 293.0, 161.0, 391.0, 388.0, 500.0,
      400.0, 296.0, 480.0, 205.0, 466.0, 174.0, 431.0, 433.0, 339.0, 324.0, 402.0,
      446.0, 500.0, 500.0, 204.0, 498.0, 184.0, 376.0, 102.0, 500.0, 500.0, 500.0,
      489.0, 500.0, 500.0, 410.0, 194.0, 500.0, 464.0, 500.0, 283.0, 500.0, 500.0,
      500.0, 389.0, 500.0, 500.0, 483.0, 500.0, 500.0, 500.0, 500.0, 500.0, 393.0,
      468.0, 244.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055247412271003986
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02263277388459489
    mean_inference_ms: 1.011766145259254
    mean_raw_obs_processing_ms: 0.20838328768562028
time_since_restore: 116.1497094631195
time_this_iter_s: 6.220885992050171
time_total_s: 116.1497094631195
timers:
  sample_time_ms: 2597.871
  synch_weights_time_ms: 3.311
  training_iteration_time_ms: 6362.054
timestamp: 1703047695
timesteps_total: 68000
training_iteration: 17
trial_id: default
-----------------------
----------------------

Iteration 17:
agent_timesteps_total: 72000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021843910217285156
  StateBufferConnector_ms: 0.0012478828430175781
  ViewRequirementAgentConnector_ms: 0.11212396621704102
counters:
  num_agent_steps_sampled: 72000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 72000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-21
done: false
episode_len_mean: 385.71
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 385.71
episode_reward_min: 102.0
episodes_this_iter: 8
episodes_total: 579
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5755.047619047619
      num_env_steps_trained: 4000.0
      total_loss: 9.789256550016857
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.0003906250058207661
      curr_lr: 5.0e-05
      entropy: 0.4875244555019197
      mean_kl_loss: 0.0039938214004266795
      policy_loss: -0.1593287651027952
      total_loss: 9.789256550016857
      vf_explained_var: -0.04905956699734643
      vf_loss: 9.94858201344808
      vf_loss_unclipped: 5391.596098400298
  num_agent_steps_sampled: 72000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 72000
  num_env_steps_trained: 0
iterations_since_restore: 18
node_ip: 127.0.0.1
num_agent_steps_sampled: 72000
num_agent_steps_trained: 0
num_env_steps_sampled: 72000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.5256992333827
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.922222222222224
  ram_util_percent: 56.8
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05522700423241333
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022618106223119862
  mean_inference_ms: 1.0115476482863
  mean_raw_obs_processing_ms: 0.20799181846779752
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021843910217285156
    StateBufferConnector_ms: 0.0012478828430175781
    ViewRequirementAgentConnector_ms: 0.11212396621704102
  custom_metrics: {}
  episode_len_mean: 385.71
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 385.71
  episode_reward_min: 102.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 433, 275, 500, 315, 189, 388, 334, 322, 148,
      132, 259, 124, 304, 371, 301, 182, 441, 198, 500, 289, 500, 222, 332, 191, 367,
      305, 352, 500, 336, 480, 292, 264, 153, 357, 174, 500, 431, 293, 161, 391, 388,
      500, 400, 296, 480, 205, 466, 174, 431, 433, 339, 324, 402, 446, 500, 500, 204,
      498, 184, 376, 102, 500, 500, 500, 489, 500, 500, 410, 194, 500, 464, 500, 283,
      500, 500, 500, 389, 500, 500, 483, 500, 500, 500, 500, 500, 393, 468, 244, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 433.0, 275.0, 500.0, 315.0, 189.0, 388.0,
      334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0, 371.0, 301.0, 182.0, 441.0,
      198.0, 500.0, 289.0, 500.0, 222.0, 332.0, 191.0, 367.0, 305.0, 352.0, 500.0,
      336.0, 480.0, 292.0, 264.0, 153.0, 357.0, 174.0, 500.0, 431.0, 293.0, 161.0,
      391.0, 388.0, 500.0, 400.0, 296.0, 480.0, 205.0, 466.0, 174.0, 431.0, 433.0,
      339.0, 324.0, 402.0, 446.0, 500.0, 500.0, 204.0, 498.0, 184.0, 376.0, 102.0,
      500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 410.0, 194.0, 500.0, 464.0, 500.0,
      283.0, 500.0, 500.0, 500.0, 389.0, 500.0, 500.0, 483.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 393.0, 468.0, 244.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05522700423241333
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022618106223119862
    mean_inference_ms: 1.0115476482863
    mean_raw_obs_processing_ms: 0.20799181846779752
time_since_restore: 122.35702252388
time_this_iter_s: 6.207313060760498
time_total_s: 122.35702252388
timers:
  sample_time_ms: 2598.35
  synch_weights_time_ms: 3.353
  training_iteration_time_ms: 6339.397
timestamp: 1703047701
timesteps_total: 72000
training_iteration: 18
trial_id: default
-----------------------
----------------------

Iteration 18:
agent_timesteps_total: 76000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002180814743041992
  StateBufferConnector_ms: 0.001234292984008789
  ViewRequirementAgentConnector_ms: 0.11226224899291992
counters:
  num_agent_steps_sampled: 76000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 76000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-28
done: false
episode_len_mean: 393.06
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 393.06
episode_reward_min: 102.0
episodes_this_iter: 9
episodes_total: 588
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5755.047619047619
      num_env_steps_trained: 4000.0
      total_loss: 9.781109401157924
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.00019531250291038305
      curr_lr: 5.0e-05
      entropy: 0.4811824829805465
      mean_kl_loss: 0.0034761211154119565
      policy_loss: -0.15536476884569442
      total_loss: 9.781109401157924
      vf_explained_var: -0.03410580044700986
      vf_loss: 9.936472801935105
      vf_loss_unclipped: 5290.061221168155
  num_agent_steps_sampled: 76000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 76000
  num_env_steps_trained: 0
iterations_since_restore: 19
node_ip: 127.0.0.1
num_agent_steps_sampled: 76000
num_agent_steps_trained: 0
num_env_steps_sampled: 76000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 645.7860452963597
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.85
  ram_util_percent: 56.8
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05520669867569733
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02260347797680126
  mean_inference_ms: 1.0112998451764617
  mean_raw_obs_processing_ms: 0.20760633024315253
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002180814743041992
    StateBufferConnector_ms: 0.001234292984008789
    ViewRequirementAgentConnector_ms: 0.11226224899291992
  custom_metrics: {}
  episode_len_mean: 393.06
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 393.06
  episode_reward_min: 102.0
  episodes_this_iter: 9
  hist_stats:
    episode_lengths: [334, 322, 148, 132, 259, 124, 304, 371, 301, 182, 441, 198,
      500, 289, 500, 222, 332, 191, 367, 305, 352, 500, 336, 480, 292, 264, 153, 357,
      174, 500, 431, 293, 161, 391, 388, 500, 400, 296, 480, 205, 466, 174, 431, 433,
      339, 324, 402, 446, 500, 500, 204, 498, 184, 376, 102, 500, 500, 500, 489, 500,
      500, 410, 194, 500, 464, 500, 283, 500, 500, 500, 389, 500, 500, 483, 500, 500,
      500, 500, 500, 393, 468, 244, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 335]
    episode_reward: [334.0, 322.0, 148.0, 132.0, 259.0, 124.0, 304.0, 371.0, 301.0,
      182.0, 441.0, 198.0, 500.0, 289.0, 500.0, 222.0, 332.0, 191.0, 367.0, 305.0,
      352.0, 500.0, 336.0, 480.0, 292.0, 264.0, 153.0, 357.0, 174.0, 500.0, 431.0,
      293.0, 161.0, 391.0, 388.0, 500.0, 400.0, 296.0, 480.0, 205.0, 466.0, 174.0,
      431.0, 433.0, 339.0, 324.0, 402.0, 446.0, 500.0, 500.0, 204.0, 498.0, 184.0,
      376.0, 102.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 410.0, 194.0, 500.0,
      464.0, 500.0, 283.0, 500.0, 500.0, 500.0, 389.0, 500.0, 500.0, 483.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 393.0, 468.0, 244.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 335.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05520669867569733
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02260347797680126
    mean_inference_ms: 1.0112998451764617
    mean_raw_obs_processing_ms: 0.20760633024315253
time_since_restore: 128.55219960212708
time_this_iter_s: 6.19517707824707
time_total_s: 128.55219960212708
timers:
  sample_time_ms: 2597.732
  synch_weights_time_ms: 3.37
  training_iteration_time_ms: 6319.459
timestamp: 1703047708
timesteps_total: 76000
training_iteration: 19
trial_id: default
-----------------------
----------------------

Iteration 19:
agent_timesteps_total: 80000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021560192108154297
  StateBufferConnector_ms: 0.001251220703125
  ViewRequirementAgentConnector_ms: 0.11229562759399414
counters:
  num_agent_steps_sampled: 80000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 80000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-34
done: false
episode_len_mean: 410.28
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 410.28
episode_reward_min: 102.0
episodes_this_iter: 8
episodes_total: 596
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.801806086585636
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.00019531250291038305
      curr_lr: 5.0e-05
      entropy: 0.4803565073580969
      mean_kl_loss: 0.005100900705019266
      policy_loss: -0.13351874904973166
      total_loss: 9.801806086585636
      vf_explained_var: -0.06125058446611677
      vf_loss: 9.935323987688337
      vf_loss_unclipped: 5143.6845703125
  num_agent_steps_sampled: 80000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 80000
  num_env_steps_trained: 0
iterations_since_restore: 20
node_ip: 127.0.0.1
num_agent_steps_sampled: 80000
num_agent_steps_trained: 0
num_env_steps_sampled: 80000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.0708222321771
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.18888888888889
  ram_util_percent: 56.82222222222222
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05519205461338699
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022595302998978606
  mean_inference_ms: 1.0110128730145163
  mean_raw_obs_processing_ms: 0.20728193759917715
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021560192108154297
    StateBufferConnector_ms: 0.001251220703125
    ViewRequirementAgentConnector_ms: 0.11229562759399414
  custom_metrics: {}
  episode_len_mean: 410.28
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 410.28
  episode_reward_min: 102.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [301, 182, 441, 198, 500, 289, 500, 222, 332, 191, 367, 305,
      352, 500, 336, 480, 292, 264, 153, 357, 174, 500, 431, 293, 161, 391, 388, 500,
      400, 296, 480, 205, 466, 174, 431, 433, 339, 324, 402, 446, 500, 500, 204, 498,
      184, 376, 102, 500, 500, 500, 489, 500, 500, 410, 194, 500, 464, 500, 283, 500,
      500, 500, 389, 500, 500, 483, 500, 500, 500, 500, 500, 393, 468, 244, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 335,
      500, 339, 500, 500, 377, 500, 500, 500]
    episode_reward: [301.0, 182.0, 441.0, 198.0, 500.0, 289.0, 500.0, 222.0, 332.0,
      191.0, 367.0, 305.0, 352.0, 500.0, 336.0, 480.0, 292.0, 264.0, 153.0, 357.0,
      174.0, 500.0, 431.0, 293.0, 161.0, 391.0, 388.0, 500.0, 400.0, 296.0, 480.0,
      205.0, 466.0, 174.0, 431.0, 433.0, 339.0, 324.0, 402.0, 446.0, 500.0, 500.0,
      204.0, 498.0, 184.0, 376.0, 102.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0,
      410.0, 194.0, 500.0, 464.0, 500.0, 283.0, 500.0, 500.0, 500.0, 389.0, 500.0,
      500.0, 483.0, 500.0, 500.0, 500.0, 500.0, 500.0, 393.0, 468.0, 244.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 335.0, 500.0, 339.0, 500.0, 500.0, 377.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05519205461338699
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022595302998978606
    mean_inference_ms: 1.0110128730145163
    mean_raw_obs_processing_ms: 0.20728193759917715
time_since_restore: 134.76386952400208
time_this_iter_s: 6.211669921875
time_total_s: 134.76386952400208
timers:
  sample_time_ms: 2596.909
  synch_weights_time_ms: 3.365
  training_iteration_time_ms: 6303.815
timestamp: 1703047714
timesteps_total: 80000
training_iteration: 20
trial_id: default
-----------------------
----------------------

Iteration 20:
agent_timesteps_total: 84000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021626949310302734
  StateBufferConnector_ms: 0.0012481212615966797
  ViewRequirementAgentConnector_ms: 0.11231708526611328
counters:
  num_agent_steps_sampled: 84000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 84000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-40
done: false
episode_len_mean: 418.21
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 418.21
episode_reward_min: 102.0
episodes_this_iter: 9
episodes_total: 605
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5753.761904761905
      num_env_steps_trained: 4000.0
      total_loss: 9.764115787687755
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 0.00019531250291038305
      curr_lr: 5.0e-05
      entropy: 0.4475831417810349
      mean_kl_loss: 0.005237908822282732
      policy_loss: -0.14653695764995756
      total_loss: 9.764115787687755
      vf_explained_var: -0.073023483866737
      vf_loss: 9.910651842753092
      vf_loss_unclipped: 4801.990908668155
  num_agent_steps_sampled: 84000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 84000
  num_env_steps_trained: 0
iterations_since_restore: 21
node_ip: 127.0.0.1
num_agent_steps_sampled: 84000
num_agent_steps_trained: 0
num_env_steps_sampled: 84000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 639.1239303516645
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.966666666666665
  ram_util_percent: 56.8
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05517777266338228
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022584619902361696
  mean_inference_ms: 1.0108654109710196
  mean_raw_obs_processing_ms: 0.20695078399228592
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021626949310302734
    StateBufferConnector_ms: 0.0012481212615966797
    ViewRequirementAgentConnector_ms: 0.11231708526611328
  custom_metrics: {}
  episode_len_mean: 418.21
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 418.21
  episode_reward_min: 102.0
  episodes_this_iter: 9
  hist_stats:
    episode_lengths: [191, 367, 305, 352, 500, 336, 480, 292, 264, 153, 357, 174,
      500, 431, 293, 161, 391, 388, 500, 400, 296, 480, 205, 466, 174, 431, 433, 339,
      324, 402, 446, 500, 500, 204, 498, 184, 376, 102, 500, 500, 500, 489, 500, 500,
      410, 194, 500, 464, 500, 283, 500, 500, 500, 389, 500, 500, 483, 500, 500, 500,
      500, 500, 393, 468, 244, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 335, 500, 339, 500, 500, 377, 500, 500, 500, 476,
      206, 500, 187, 500, 500, 500, 442, 447]
    episode_reward: [191.0, 367.0, 305.0, 352.0, 500.0, 336.0, 480.0, 292.0, 264.0,
      153.0, 357.0, 174.0, 500.0, 431.0, 293.0, 161.0, 391.0, 388.0, 500.0, 400.0,
      296.0, 480.0, 205.0, 466.0, 174.0, 431.0, 433.0, 339.0, 324.0, 402.0, 446.0,
      500.0, 500.0, 204.0, 498.0, 184.0, 376.0, 102.0, 500.0, 500.0, 500.0, 489.0,
      500.0, 500.0, 410.0, 194.0, 500.0, 464.0, 500.0, 283.0, 500.0, 500.0, 500.0,
      389.0, 500.0, 500.0, 483.0, 500.0, 500.0, 500.0, 500.0, 500.0, 393.0, 468.0,
      244.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 335.0, 500.0, 339.0, 500.0,
      500.0, 377.0, 500.0, 500.0, 500.0, 476.0, 206.0, 500.0, 187.0, 500.0, 500.0,
      500.0, 442.0, 447.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05517777266338228
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022584619902361696
    mean_inference_ms: 1.0108654109710196
    mean_raw_obs_processing_ms: 0.20695078399228592
time_since_restore: 141.02380347251892
time_this_iter_s: 6.259933948516846
time_total_s: 141.02380347251892
timers:
  sample_time_ms: 2600.293
  synch_weights_time_ms: 3.308
  training_iteration_time_ms: 6286.327
timestamp: 1703047720
timesteps_total: 84000
training_iteration: 21
trial_id: default
-----------------------
----------------------

Iteration 21:
agent_timesteps_total: 88000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002173185348510742
  StateBufferConnector_ms: 0.0012629032135009766
  ViewRequirementAgentConnector_ms: 0.11280202865600586
counters:
  num_agent_steps_sampled: 88000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 88000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-46
done: false
episode_len_mean: 428.41
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 428.41
episode_reward_min: 102.0
episodes_this_iter: 8
episodes_total: 613
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5753.523809523809
      num_env_steps_trained: 4000.0
      total_loss: 9.784555162702288
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 9.765625145519152e-05
      curr_lr: 5.0e-05
      entropy: 0.4508759251662663
      mean_kl_loss: 0.0042973406053192775
      policy_loss: -0.15713429096199216
      total_loss: 9.784555162702288
      vf_explained_var: -0.07508396534692674
      vf_loss: 9.941688810076032
      vf_loss_unclipped: 5099.991071428572
  num_agent_steps_sampled: 88000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 88000
  num_env_steps_trained: 0
iterations_since_restore: 22
node_ip: 127.0.0.1
num_agent_steps_sampled: 88000
num_agent_steps_trained: 0
num_env_steps_sampled: 88000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 643.9887930426614
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.27777777777778
  ram_util_percent: 56.8
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05516478443368218
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02257365755934403
  mean_inference_ms: 1.0108000161662034
  mean_raw_obs_processing_ms: 0.2066845814249659
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002173185348510742
    StateBufferConnector_ms: 0.0012629032135009766
    ViewRequirementAgentConnector_ms: 0.11280202865600586
  custom_metrics: {}
  episode_len_mean: 428.41
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 428.41
  episode_reward_min: 102.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [264, 153, 357, 174, 500, 431, 293, 161, 391, 388, 500, 400,
      296, 480, 205, 466, 174, 431, 433, 339, 324, 402, 446, 500, 500, 204, 498, 184,
      376, 102, 500, 500, 500, 489, 500, 500, 410, 194, 500, 464, 500, 283, 500, 500,
      500, 389, 500, 500, 483, 500, 500, 500, 500, 500, 393, 468, 244, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 335, 500,
      339, 500, 500, 377, 500, 500, 500, 476, 206, 500, 187, 500, 500, 500, 442, 447,
      500, 500, 500, 500, 500, 500, 343, 500]
    episode_reward: [264.0, 153.0, 357.0, 174.0, 500.0, 431.0, 293.0, 161.0, 391.0,
      388.0, 500.0, 400.0, 296.0, 480.0, 205.0, 466.0, 174.0, 431.0, 433.0, 339.0,
      324.0, 402.0, 446.0, 500.0, 500.0, 204.0, 498.0, 184.0, 376.0, 102.0, 500.0,
      500.0, 500.0, 489.0, 500.0, 500.0, 410.0, 194.0, 500.0, 464.0, 500.0, 283.0,
      500.0, 500.0, 500.0, 389.0, 500.0, 500.0, 483.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 393.0, 468.0, 244.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 335.0,
      500.0, 339.0, 500.0, 500.0, 377.0, 500.0, 500.0, 500.0, 476.0, 206.0, 500.0,
      187.0, 500.0, 500.0, 500.0, 442.0, 447.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 343.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05516478443368218
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02257365755934403
    mean_inference_ms: 1.0108000161662034
    mean_raw_obs_processing_ms: 0.2066845814249659
time_since_restore: 147.23630237579346
time_this_iter_s: 6.212498903274536
time_total_s: 147.23630237579346
timers:
  sample_time_ms: 2600.587
  synch_weights_time_ms: 3.293
  training_iteration_time_ms: 6266.061
timestamp: 1703047726
timesteps_total: 88000
training_iteration: 22
trial_id: default
-----------------------
----------------------

Iteration 22:
agent_timesteps_total: 92000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021851062774658203
  StateBufferConnector_ms: 0.0012612342834472656
  ViewRequirementAgentConnector_ms: 0.1127004623413086
counters:
  num_agent_steps_sampled: 92000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 92000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-52
done: false
episode_len_mean: 444.13
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 444.13
episode_reward_min: 102.0
episodes_this_iter: 9
episodes_total: 622
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.380952380952
      num_env_steps_trained: 4000.0
      total_loss: 9.800448553902763
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 9.765625145519152e-05
      curr_lr: 5.0e-05
      entropy: 0.45619092384974164
      mean_kl_loss: 0.005063657388837956
      policy_loss: -0.12941462723981767
      total_loss: 9.800448553902763
      vf_explained_var: -0.07582447074708484
      vf_loss: 9.929862431117467
      vf_loss_unclipped: 5012.736118861607
  num_agent_steps_sampled: 92000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 92000
  num_env_steps_trained: 0
iterations_since_restore: 23
node_ip: 127.0.0.1
num_agent_steps_sampled: 92000
num_agent_steps_trained: 0
num_env_steps_sampled: 92000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 643.9972224450698
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.522222222222226
  ram_util_percent: 56.81111111111111
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05515120602842377
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022563167267972077
  mean_inference_ms: 1.0106745651722648
  mean_raw_obs_processing_ms: 0.206410577380275
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021851062774658203
    StateBufferConnector_ms: 0.0012612342834472656
    ViewRequirementAgentConnector_ms: 0.1127004623413086
  custom_metrics: {}
  episode_len_mean: 444.13
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 444.13
  episode_reward_min: 102.0
  episodes_this_iter: 9
  hist_stats:
    episode_lengths: [388, 500, 400, 296, 480, 205, 466, 174, 431, 433, 339, 324,
      402, 446, 500, 500, 204, 498, 184, 376, 102, 500, 500, 500, 489, 500, 500, 410,
      194, 500, 464, 500, 283, 500, 500, 500, 389, 500, 500, 483, 500, 500, 500, 500,
      500, 393, 468, 244, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 335, 500, 339, 500, 500, 377, 500, 500, 500, 476, 206,
      500, 187, 500, 500, 500, 442, 447, 500, 500, 500, 500, 500, 500, 343, 500, 500,
      500, 500, 462, 500, 500, 334, 500, 500]
    episode_reward: [388.0, 500.0, 400.0, 296.0, 480.0, 205.0, 466.0, 174.0, 431.0,
      433.0, 339.0, 324.0, 402.0, 446.0, 500.0, 500.0, 204.0, 498.0, 184.0, 376.0,
      102.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 410.0, 194.0, 500.0, 464.0,
      500.0, 283.0, 500.0, 500.0, 500.0, 389.0, 500.0, 500.0, 483.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 393.0, 468.0, 244.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 335.0, 500.0, 339.0, 500.0, 500.0, 377.0, 500.0, 500.0, 500.0, 476.0,
      206.0, 500.0, 187.0, 500.0, 500.0, 500.0, 442.0, 447.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 343.0, 500.0, 500.0, 500.0, 500.0, 462.0, 500.0, 500.0,
      334.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05515120602842377
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022563167267972077
    mean_inference_ms: 1.0106745651722648
    mean_raw_obs_processing_ms: 0.206410577380275
time_since_restore: 153.44875645637512
time_this_iter_s: 6.212454080581665
time_total_s: 153.44875645637512
timers:
  sample_time_ms: 2601.867
  synch_weights_time_ms: 3.275
  training_iteration_time_ms: 6244.914
timestamp: 1703047732
timesteps_total: 92000
training_iteration: 23
trial_id: default
-----------------------
----------------------

Iteration 23:
agent_timesteps_total: 96000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021877288818359375
  StateBufferConnector_ms: 0.0012562274932861328
  ViewRequirementAgentConnector_ms: 0.11252689361572266
counters:
  num_agent_steps_sampled: 96000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 96000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-48-59
done: false
episode_len_mean: 455.04
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 455.04
episode_reward_min: 102.0
episodes_this_iter: 8
episodes_total: 630
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.81180731455485
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 4.882812572759576e-05
      curr_lr: 5.0e-05
      entropy: 0.44813204805056256
      mean_kl_loss: 0.0045323188053966335
      policy_loss: -0.1361032057376135
      total_loss: 9.81180731455485
      vf_explained_var: -0.0665942146664574
      vf_loss: 9.947910172598702
      vf_loss_unclipped: 5201.6386486235115
  num_agent_steps_sampled: 96000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 96000
  num_env_steps_trained: 0
iterations_since_restore: 24
node_ip: 127.0.0.1
num_agent_steps_sampled: 96000
num_agent_steps_trained: 0
num_env_steps_sampled: 96000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.4031824818617
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.211111111111112
  ram_util_percent: 56.833333333333336
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05514360506882678
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02255816444157975
  mean_inference_ms: 1.0105648633053035
  mean_raw_obs_processing_ms: 0.20620587072252472
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021877288818359375
    StateBufferConnector_ms: 0.0012562274932861328
    ViewRequirementAgentConnector_ms: 0.11252689361572266
  custom_metrics: {}
  episode_len_mean: 455.04
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 455.04
  episode_reward_min: 102.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [431, 433, 339, 324, 402, 446, 500, 500, 204, 498, 184, 376,
      102, 500, 500, 500, 489, 500, 500, 410, 194, 500, 464, 500, 283, 500, 500, 500,
      389, 500, 500, 483, 500, 500, 500, 500, 500, 393, 468, 244, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 335, 500, 339,
      500, 500, 377, 500, 500, 500, 476, 206, 500, 187, 500, 500, 500, 442, 447, 500,
      500, 500, 500, 500, 500, 343, 500, 500, 500, 500, 462, 500, 500, 334, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [431.0, 433.0, 339.0, 324.0, 402.0, 446.0, 500.0, 500.0, 204.0,
      498.0, 184.0, 376.0, 102.0, 500.0, 500.0, 500.0, 489.0, 500.0, 500.0, 410.0,
      194.0, 500.0, 464.0, 500.0, 283.0, 500.0, 500.0, 500.0, 389.0, 500.0, 500.0,
      483.0, 500.0, 500.0, 500.0, 500.0, 500.0, 393.0, 468.0, 244.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 335.0, 500.0, 339.0, 500.0, 500.0, 377.0, 500.0,
      500.0, 500.0, 476.0, 206.0, 500.0, 187.0, 500.0, 500.0, 500.0, 442.0, 447.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 343.0, 500.0, 500.0, 500.0, 500.0,
      462.0, 500.0, 500.0, 334.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05514360506882678
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02255816444157975
    mean_inference_ms: 1.0105648633053035
    mean_raw_obs_processing_ms: 0.20620587072252472
time_since_restore: 159.6572732925415
time_this_iter_s: 6.208516836166382
time_total_s: 159.6572732925415
timers:
  sample_time_ms: 2605.339
  synch_weights_time_ms: 3.263
  training_iteration_time_ms: 6230.265
timestamp: 1703047739
timesteps_total: 96000
training_iteration: 24
trial_id: default
-----------------------
----------------------

Iteration 24:
agent_timesteps_total: 100000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002177715301513672
  StateBufferConnector_ms: 0.0012505054473876953
  ViewRequirementAgentConnector_ms: 0.11289024353027344
counters:
  num_agent_steps_sampled: 100000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 100000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-05
done: false
episode_len_mean: 461.24
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 461.24
episode_reward_min: 102.0
episodes_this_iter: 9
episodes_total: 639
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5754.0
      num_env_steps_trained: 4000.0
      total_loss: 9.782904125395275
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.441406286379788e-05
      curr_lr: 5.0e-05
      entropy: 0.42688936846596853
      mean_kl_loss: 0.004825193478874705
      policy_loss: -0.1525902081103552
      total_loss: 9.782904125395275
      vf_explained_var: -0.06860536620730445
      vf_loss: 9.935494332086472
      vf_loss_unclipped: 4948.3984607514885
  num_agent_steps_sampled: 100000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 100000
  num_env_steps_trained: 0
iterations_since_restore: 25
node_ip: 127.0.0.1
num_agent_steps_sampled: 100000
num_agent_steps_trained: 0
num_env_steps_sampled: 100000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 641.4816448452793
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.233333333333334
  ram_util_percent: 56.86666666666666
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05513674868995624
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022553405103575282
  mean_inference_ms: 1.0105618869300104
  mean_raw_obs_processing_ms: 0.20601170500977048
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002177715301513672
    StateBufferConnector_ms: 0.0012505054473876953
    ViewRequirementAgentConnector_ms: 0.11289024353027344
  custom_metrics: {}
  episode_len_mean: 461.24
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 461.24
  episode_reward_min: 102.0
  episodes_this_iter: 9
  hist_stats:
    episode_lengths: [498, 184, 376, 102, 500, 500, 500, 489, 500, 500, 410, 194,
      500, 464, 500, 283, 500, 500, 500, 389, 500, 500, 483, 500, 500, 500, 500, 500,
      393, 468, 244, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 335, 500, 339, 500, 500, 377, 500, 500, 500, 476, 206, 500,
      187, 500, 500, 500, 442, 447, 500, 500, 500, 500, 500, 500, 343, 500, 500, 500,
      500, 462, 500, 500, 334, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 199, 500, 500, 500, 500]
    episode_reward: [498.0, 184.0, 376.0, 102.0, 500.0, 500.0, 500.0, 489.0, 500.0,
      500.0, 410.0, 194.0, 500.0, 464.0, 500.0, 283.0, 500.0, 500.0, 500.0, 389.0,
      500.0, 500.0, 483.0, 500.0, 500.0, 500.0, 500.0, 500.0, 393.0, 468.0, 244.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 335.0, 500.0, 339.0, 500.0, 500.0,
      377.0, 500.0, 500.0, 500.0, 476.0, 206.0, 500.0, 187.0, 500.0, 500.0, 500.0,
      442.0, 447.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 343.0, 500.0, 500.0,
      500.0, 500.0, 462.0, 500.0, 500.0, 334.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 199.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05513674868995624
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022553405103575282
    mean_inference_ms: 1.0105618869300104
    mean_raw_obs_processing_ms: 0.20601170500977048
time_since_restore: 165.8940944671631
time_this_iter_s: 6.236821174621582
time_total_s: 165.8940944671631
timers:
  sample_time_ms: 2606.535
  synch_weights_time_ms: 3.339
  training_iteration_time_ms: 6217.918
timestamp: 1703047745
timesteps_total: 100000
training_iteration: 25
trial_id: default
-----------------------
----------------------

Iteration 25:
agent_timesteps_total: 104000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021598339080810547
  StateBufferConnector_ms: 0.0012478828430175781
  ViewRequirementAgentConnector_ms: 0.11300277709960938
counters:
  num_agent_steps_sampled: 104000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 104000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-11
done: false
episode_len_mean: 469.75
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 469.75
episode_reward_min: 187.0
episodes_this_iter: 8
episodes_total: 647
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5752.047619047619
      num_env_steps_trained: 4000.0
      total_loss: 9.789814313252768
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.220703143189894e-05
      curr_lr: 5.0e-05
      entropy: 0.4319878135408674
      mean_kl_loss: 0.0037334897389877412
      policy_loss: -0.15738685925801596
      total_loss: 9.789814313252768
      vf_explained_var: -0.0653824011484782
      vf_loss: 9.94720104762486
      vf_loss_unclipped: 5042.326334635417
  num_agent_steps_sampled: 104000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 104000
  num_env_steps_trained: 0
iterations_since_restore: 26
node_ip: 127.0.0.1
num_agent_steps_sampled: 104000
num_agent_steps_trained: 0
num_env_steps_sampled: 104000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 651.4634596298802
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 24.9625
  ram_util_percent: 56.9375
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055131420534773226
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022547786117190834
  mean_inference_ms: 1.0106309414194177
  mean_raw_obs_processing_ms: 0.20586563963287324
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021598339080810547
    StateBufferConnector_ms: 0.0012478828430175781
    ViewRequirementAgentConnector_ms: 0.11300277709960938
  custom_metrics: {}
  episode_len_mean: 469.75
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 469.75
  episode_reward_min: 187.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 410, 194, 500, 464, 500, 283, 500, 500, 500, 389,
      500, 500, 483, 500, 500, 500, 500, 500, 393, 468, 244, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 335, 500, 339, 500,
      500, 377, 500, 500, 500, 476, 206, 500, 187, 500, 500, 500, 442, 447, 500, 500,
      500, 500, 500, 500, 343, 500, 500, 500, 500, 462, 500, 500, 334, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 199, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 410.0, 194.0, 500.0, 464.0, 500.0, 283.0, 500.0,
      500.0, 500.0, 389.0, 500.0, 500.0, 483.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      393.0, 468.0, 244.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 335.0, 500.0,
      339.0, 500.0, 500.0, 377.0, 500.0, 500.0, 500.0, 476.0, 206.0, 500.0, 187.0,
      500.0, 500.0, 500.0, 442.0, 447.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      343.0, 500.0, 500.0, 500.0, 500.0, 462.0, 500.0, 500.0, 334.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 199.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055131420534773226
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022547786117190834
    mean_inference_ms: 1.0106309414194177
    mean_raw_obs_processing_ms: 0.20586563963287324
time_since_restore: 172.03540325164795
time_this_iter_s: 6.141308784484863
time_total_s: 172.03540325164795
timers:
  sample_time_ms: 2604.91
  synch_weights_time_ms: 3.394
  training_iteration_time_ms: 6209.413
timestamp: 1703047751
timesteps_total: 104000
training_iteration: 26
trial_id: default
-----------------------
----------------------

Iteration 26:
agent_timesteps_total: 108000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021364688873291016
  StateBufferConnector_ms: 0.0012562274932861328
  ViewRequirementAgentConnector_ms: 0.11305904388427734
counters:
  num_agent_steps_sampled: 108000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 108000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-17
done: false
episode_len_mean: 474.55
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 474.55
episode_reward_min: 187.0
episodes_this_iter: 8
episodes_total: 655
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.806454113551549
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 6.10351571594947e-06
      curr_lr: 5.0e-05
      entropy: 0.3952727502300626
      mean_kl_loss: 0.004544795999609484
      policy_loss: -0.13379988571008047
      total_loss: 9.806454113551549
      vf_explained_var: -0.0753951526823498
      vf_loss: 9.940253893534342
      vf_loss_unclipped: 4954.696870349702
  num_agent_steps_sampled: 108000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 108000
  num_env_steps_trained: 0
iterations_since_restore: 27
node_ip: 127.0.0.1
num_agent_steps_sampled: 108000
num_agent_steps_trained: 0
num_env_steps_sampled: 108000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 643.1263057045837
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.77777777777778
  ram_util_percent: 57.01111111111111
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05512489095152318
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02254137633950549
  mean_inference_ms: 1.0106679443625297
  mean_raw_obs_processing_ms: 0.20572917725831188
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021364688873291016
    StateBufferConnector_ms: 0.0012562274932861328
    ViewRequirementAgentConnector_ms: 0.11305904388427734
  custom_metrics: {}
  episode_len_mean: 474.55
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 474.55
  episode_reward_min: 187.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 389, 500, 500, 483, 500, 500, 500, 500, 500,
      393, 468, 244, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 335, 500, 339, 500, 500, 377, 500, 500, 500, 476, 206, 500,
      187, 500, 500, 500, 442, 447, 500, 500, 500, 500, 500, 500, 343, 500, 500, 500,
      500, 462, 500, 500, 334, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 199, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 331, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 389.0, 500.0, 500.0, 483.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 393.0, 468.0, 244.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 335.0, 500.0, 339.0, 500.0, 500.0, 377.0, 500.0, 500.0, 500.0, 476.0,
      206.0, 500.0, 187.0, 500.0, 500.0, 500.0, 442.0, 447.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 343.0, 500.0, 500.0, 500.0, 500.0, 462.0, 500.0, 500.0,
      334.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 199.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 331.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05512489095152318
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02254137633950549
    mean_inference_ms: 1.0106679443625297
    mean_raw_obs_processing_ms: 0.20572917725831188
time_since_restore: 178.2561900615692
time_this_iter_s: 6.220786809921265
time_total_s: 178.2561900615692
timers:
  sample_time_ms: 2602.175
  synch_weights_time_ms: 3.397
  training_iteration_time_ms: 6209.407
timestamp: 1703047757
timesteps_total: 108000
training_iteration: 27
trial_id: default
-----------------------
----------------------

Iteration 27:
agent_timesteps_total: 112000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002087831497192383
  StateBufferConnector_ms: 0.0012562274932861328
  ViewRequirementAgentConnector_ms: 0.11299633979797363
counters:
  num_agent_steps_sampled: 112000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 112000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-24
done: false
episode_len_mean: 475.83
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 475.83
episode_reward_min: 187.0
episodes_this_iter: 8
episodes_total: 663
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.810921396527972
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.051757857974735e-06
      curr_lr: 5.0e-05
      entropy: 0.41627769526981173
      mean_kl_loss: 0.004466498253090582
      policy_loss: -0.13614738093955175
      total_loss: 9.810921396527972
      vf_explained_var: -0.0787383204414731
      vf_loss: 9.947068759373256
      vf_loss_unclipped: 5011.7558826264885
  num_agent_steps_sampled: 112000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 112000
  num_env_steps_trained: 0
iterations_since_restore: 28
node_ip: 127.0.0.1
num_agent_steps_sampled: 112000
num_agent_steps_trained: 0
num_env_steps_sampled: 112000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.2691828516238
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.78888888888889
  ram_util_percent: 57.044444444444444
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05511926581897162
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022537120519891044
  mean_inference_ms: 1.0106488102305875
  mean_raw_obs_processing_ms: 0.20560200182139934
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002087831497192383
    StateBufferConnector_ms: 0.0012562274932861328
    ViewRequirementAgentConnector_ms: 0.11299633979797363
  custom_metrics: {}
  episode_len_mean: 475.83
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 475.83
  episode_reward_min: 187.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 393, 468, 244, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 335, 500, 339, 500,
      500, 377, 500, 500, 500, 476, 206, 500, 187, 500, 500, 500, 442, 447, 500, 500,
      500, 500, 500, 500, 343, 500, 500, 500, 500, 462, 500, 500, 334, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 199, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 331, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 393.0, 468.0, 244.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 335.0, 500.0, 339.0, 500.0, 500.0, 377.0, 500.0,
      500.0, 500.0, 476.0, 206.0, 500.0, 187.0, 500.0, 500.0, 500.0, 442.0, 447.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 343.0, 500.0, 500.0, 500.0, 500.0,
      462.0, 500.0, 500.0, 334.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 199.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 331.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05511926581897162
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022537120519891044
    mean_inference_ms: 1.0106488102305875
    mean_raw_obs_processing_ms: 0.20560200182139934
time_since_restore: 184.46604299545288
time_this_iter_s: 6.209852933883667
time_total_s: 184.46604299545288
timers:
  sample_time_ms: 2604.259
  synch_weights_time_ms: 3.418
  training_iteration_time_ms: 6209.654
timestamp: 1703047764
timesteps_total: 112000
training_iteration: 28
trial_id: default
-----------------------
----------------------

Iteration 28:
agent_timesteps_total: 116000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002112865447998047
  StateBufferConnector_ms: 0.0012714862823486328
  ViewRequirementAgentConnector_ms: 0.11297464370727539
counters:
  num_agent_steps_sampled: 116000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 116000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-30
done: false
episode_len_mean: 479.78
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 479.78
episode_reward_min: 187.0
episodes_this_iter: 8
episodes_total: 671
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.811154274713425
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.5258789289873675e-06
      curr_lr: 5.0e-05
      entropy: 0.4144889556226276
      mean_kl_loss: 0.003651141721076913
      policy_loss: -0.13542950508140383
      total_loss: 9.811154274713425
      vf_explained_var: -0.08156979651678176
      vf_loss: 9.94658374786377
      vf_loss_unclipped: 4972.2265857514885
  num_agent_steps_sampled: 116000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 116000
  num_env_steps_trained: 0
iterations_since_restore: 29
node_ip: 127.0.0.1
num_agent_steps_sampled: 116000
num_agent_steps_trained: 0
num_env_steps_sampled: 116000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 646.8802090594914
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 25.000000000000004
  ram_util_percent: 57.022222222222226
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055111983274627324
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02253088130189258
  mean_inference_ms: 1.010661143186894
  mean_raw_obs_processing_ms: 0.2054898607202186
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002112865447998047
    StateBufferConnector_ms: 0.0012714862823486328
    ViewRequirementAgentConnector_ms: 0.11297464370727539
  custom_metrics: {}
  episode_len_mean: 479.78
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 479.78
  episode_reward_min: 187.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 335, 500, 339, 500, 500, 377, 500, 500, 500, 476, 206, 500,
      187, 500, 500, 500, 442, 447, 500, 500, 500, 500, 500, 500, 343, 500, 500, 500,
      500, 462, 500, 500, 334, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 199, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 331, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 335.0, 500.0, 339.0, 500.0,
      500.0, 377.0, 500.0, 500.0, 500.0, 476.0, 206.0, 500.0, 187.0, 500.0, 500.0,
      500.0, 442.0, 447.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 343.0, 500.0,
      500.0, 500.0, 500.0, 462.0, 500.0, 500.0, 334.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 199.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 331.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055111983274627324
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02253088130189258
    mean_inference_ms: 1.010661143186894
    mean_raw_obs_processing_ms: 0.2054898607202186
time_since_restore: 190.65080904960632
time_this_iter_s: 6.184766054153442
time_total_s: 190.65080904960632
timers:
  sample_time_ms: 2606.394
  synch_weights_time_ms: 3.381
  training_iteration_time_ms: 6208.606
timestamp: 1703047770
timesteps_total: 116000
training_iteration: 29
trial_id: default
-----------------------
----------------------

Iteration 29:
agent_timesteps_total: 120000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002120494842529297
  StateBufferConnector_ms: 0.001264810562133789
  ViewRequirementAgentConnector_ms: 0.11310505867004395
counters:
  num_agent_steps_sampled: 120000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 120000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-36
done: false
episode_len_mean: 479.78
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 479.78
episode_reward_min: 187.0
episodes_this_iter: 8
episodes_total: 679
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.811704726446242
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 7.629394644936838e-07
      curr_lr: 5.0e-05
      entropy: 0.394499937693278
      mean_kl_loss: 0.004085299737036101
      policy_loss: -0.13470735613788878
      total_loss: 9.811704726446242
      vf_explained_var: -0.08472334770929246
      vf_loss: 9.94641231355213
      vf_loss_unclipped: 4932.200241815476
  num_agent_steps_sampled: 120000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 120000
  num_env_steps_trained: 0
iterations_since_restore: 30
node_ip: 127.0.0.1
num_agent_steps_sampled: 120000
num_agent_steps_trained: 0
num_env_steps_sampled: 120000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 647.7035119656057
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.17777777777778
  ram_util_percent: 57.0
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05510953640004544
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022528522708263662
  mean_inference_ms: 1.0106733848007798
  mean_raw_obs_processing_ms: 0.20538550150632862
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002120494842529297
    StateBufferConnector_ms: 0.001264810562133789
    ViewRequirementAgentConnector_ms: 0.11310505867004395
  custom_metrics: {}
  episode_len_mean: 479.78
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 479.78
  episode_reward_min: 187.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 335, 500, 339, 500,
      500, 377, 500, 500, 500, 476, 206, 500, 187, 500, 500, 500, 442, 447, 500, 500,
      500, 500, 500, 500, 343, 500, 500, 500, 500, 462, 500, 500, 334, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 199, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 331, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 335.0,
      500.0, 339.0, 500.0, 500.0, 377.0, 500.0, 500.0, 500.0, 476.0, 206.0, 500.0,
      187.0, 500.0, 500.0, 500.0, 442.0, 447.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 343.0, 500.0, 500.0, 500.0, 500.0, 462.0, 500.0, 500.0, 334.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 199.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 331.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05510953640004544
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022528522708263662
    mean_inference_ms: 1.0106733848007798
    mean_raw_obs_processing_ms: 0.20538550150632862
time_since_restore: 196.82765698432922
time_this_iter_s: 6.1768479347229
time_total_s: 196.82765698432922
timers:
  sample_time_ms: 2607.809
  synch_weights_time_ms: 3.377
  training_iteration_time_ms: 6205.123
timestamp: 1703047776
timesteps_total: 120000
training_iteration: 30
trial_id: default
-----------------------
----------------------

Iteration 30:
agent_timesteps_total: 124000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021224021911621094
  StateBufferConnector_ms: 0.0012707710266113281
  ViewRequirementAgentConnector_ms: 0.11303281784057617
counters:
  num_agent_steps_sampled: 124000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 124000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-42
done: false
episode_len_mean: 479.78
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 479.78
episode_reward_min: 187.0
episodes_this_iter: 8
episodes_total: 687
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.809718041192918
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 7.629394644936838e-07
      curr_lr: 5.0e-05
      entropy: 0.3964735468228658
      mean_kl_loss: 0.005126274805903785
      policy_loss: -0.13637238811878932
      total_loss: 9.809718041192918
      vf_explained_var: -0.087808472769601
      vf_loss: 9.946090516589937
      vf_loss_unclipped: 4892.118373325893
  num_agent_steps_sampled: 124000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 124000
  num_env_steps_trained: 0
iterations_since_restore: 31
node_ip: 127.0.0.1
num_agent_steps_sampled: 124000
num_agent_steps_trained: 0
num_env_steps_sampled: 124000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.3308677251116
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.5
  ram_util_percent: 57.0
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05510950176806755
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02252792633704637
  mean_inference_ms: 1.0107218748552689
  mean_raw_obs_processing_ms: 0.2052916810946035
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021224021911621094
    StateBufferConnector_ms: 0.0012707710266113281
    ViewRequirementAgentConnector_ms: 0.11303281784057617
  custom_metrics: {}
  episode_len_mean: 479.78
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 479.78
  episode_reward_min: 187.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [335, 500, 339, 500, 500, 377, 500, 500, 500, 476, 206, 500,
      187, 500, 500, 500, 442, 447, 500, 500, 500, 500, 500, 500, 343, 500, 500, 500,
      500, 462, 500, 500, 334, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 199, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 331, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [335.0, 500.0, 339.0, 500.0, 500.0, 377.0, 500.0, 500.0, 500.0,
      476.0, 206.0, 500.0, 187.0, 500.0, 500.0, 500.0, 442.0, 447.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 343.0, 500.0, 500.0, 500.0, 500.0, 462.0, 500.0,
      500.0, 334.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 199.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 331.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05510950176806755
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02252792633704637
    mean_inference_ms: 1.0107218748552689
    mean_raw_obs_processing_ms: 0.2052916810946035
time_since_restore: 203.03687620162964
time_this_iter_s: 6.209219217300415
time_total_s: 203.03687620162964
timers:
  sample_time_ms: 2604.84
  synch_weights_time_ms: 3.58
  training_iteration_time_ms: 6200.065
timestamp: 1703047782
timesteps_total: 124000
training_iteration: 31
trial_id: default
-----------------------
----------------------

Iteration 31:
agent_timesteps_total: 128000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002130746841430664
  StateBufferConnector_ms: 0.0012655258178710938
  ViewRequirementAgentConnector_ms: 0.11296319961547852
counters:
  num_agent_steps_sampled: 128000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 128000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-48
done: false
episode_len_mean: 484.27
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 484.27
episode_reward_min: 187.0
episodes_this_iter: 8
episodes_total: 695
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.80945532662528
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 7.629394644936838e-07
      curr_lr: 5.0e-05
      entropy: 0.3970967871802194
      mean_kl_loss: 0.005251340780932067
      policy_loss: -0.1364360096908751
      total_loss: 9.80945532662528
      vf_explained_var: -0.09101838724953788
      vf_loss: 9.945891698201498
      vf_loss_unclipped: 4852.513137090774
  num_agent_steps_sampled: 128000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 128000
  num_env_steps_trained: 0
iterations_since_restore: 32
node_ip: 127.0.0.1
num_agent_steps_sampled: 128000
num_agent_steps_trained: 0
num_env_steps_sampled: 128000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.2767473888821
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.5
  ram_util_percent: 57.0
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05510885199896601
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02252696115833353
  mean_inference_ms: 1.0107586960834127
  mean_raw_obs_processing_ms: 0.20520319039279755
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002130746841430664
    StateBufferConnector_ms: 0.0012655258178710938
    ViewRequirementAgentConnector_ms: 0.11296319961547852
  custom_metrics: {}
  episode_len_mean: 484.27
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 484.27
  episode_reward_min: 187.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 476, 206, 500, 187, 500, 500, 500, 442, 447, 500, 500,
      500, 500, 500, 500, 343, 500, 500, 500, 500, 462, 500, 500, 334, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 199, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 331, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 476.0, 206.0, 500.0, 187.0, 500.0, 500.0, 500.0, 442.0,
      447.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 343.0, 500.0, 500.0, 500.0,
      500.0, 462.0, 500.0, 500.0, 334.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 199.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      331.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05510885199896601
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02252696115833353
    mean_inference_ms: 1.0107586960834127
    mean_raw_obs_processing_ms: 0.20520319039279755
time_since_restore: 209.26589822769165
time_this_iter_s: 6.229022026062012
time_total_s: 209.26589822769165
timers:
  sample_time_ms: 2603.605
  synch_weights_time_ms: 3.406
  training_iteration_time_ms: 6201.721
timestamp: 1703047788
timesteps_total: 128000
training_iteration: 32
trial_id: default
-----------------------
----------------------

Iteration 32:
agent_timesteps_total: 132000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002135038375854492
  StateBufferConnector_ms: 0.0012619495391845703
  ViewRequirementAgentConnector_ms: 0.11287140846252441
counters:
  num_agent_steps_sampled: 132000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 132000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-49-55
done: false
episode_len_mean: 490.58
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 490.58
episode_reward_min: 199.0
episodes_this_iter: 8
episodes_total: 703
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.808419681730724
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 7.629394644936838e-07
      curr_lr: 5.0e-05
      entropy: 0.3952078989573887
      mean_kl_loss: 0.006327284670944556
      policy_loss: -0.13718620226496742
      total_loss: 9.808419681730724
      vf_explained_var: -0.09409821601141066
      vf_loss: 9.945605822971888
      vf_loss_unclipped: 4812.906482514881
  num_agent_steps_sampled: 132000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 132000
  num_env_steps_trained: 0
iterations_since_restore: 33
node_ip: 127.0.0.1
num_agent_steps_sampled: 132000
num_agent_steps_trained: 0
num_env_steps_sampled: 132000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 646.4786709676321
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.844444444444445
  ram_util_percent: 57.0
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05510863171069747
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022526692135888496
  mean_inference_ms: 1.010718690152071
  mean_raw_obs_processing_ms: 0.20512188936850137
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002135038375854492
    StateBufferConnector_ms: 0.0012619495391845703
    ViewRequirementAgentConnector_ms: 0.11287140846252441
  custom_metrics: {}
  episode_len_mean: 490.58
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 490.58
  episode_reward_min: 199.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [442, 447, 500, 500, 500, 500, 500, 500, 343, 500, 500, 500,
      500, 462, 500, 500, 334, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 199, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 331, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [442.0, 447.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 343.0,
      500.0, 500.0, 500.0, 500.0, 462.0, 500.0, 500.0, 334.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      199.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 331.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05510863171069747
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022526692135888496
    mean_inference_ms: 1.010718690152071
    mean_raw_obs_processing_ms: 0.20512188936850137
time_since_restore: 215.45440816879272
time_this_iter_s: 6.188509941101074
time_total_s: 215.45440816879272
timers:
  sample_time_ms: 2604.627
  synch_weights_time_ms: 3.412
  training_iteration_time_ms: 6199.337
timestamp: 1703047795
timesteps_total: 132000
training_iteration: 33
trial_id: default
-----------------------
----------------------

Iteration 33:
agent_timesteps_total: 136000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021309852600097656
  StateBufferConnector_ms: 0.0012624263763427734
  ViewRequirementAgentConnector_ms: 0.11250591278076172
counters:
  num_agent_steps_sampled: 136000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 136000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-01
done: false
episode_len_mean: 491.69
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 491.69
episode_reward_min: 199.0
episodes_this_iter: 8
episodes_total: 711
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.810154324486142
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.814697322468419e-07
      curr_lr: 5.0e-05
      entropy: 0.3783896863460541
      mean_kl_loss: 0.0030249453377205696
      policy_loss: -0.13516384930837722
      total_loss: 9.810154324486142
      vf_explained_var: -0.0975906735374814
      vf_loss: 9.945318403698149
      vf_loss_unclipped: 4773.482096354167
  num_agent_steps_sampled: 136000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 136000
  num_env_steps_trained: 0
iterations_since_restore: 34
node_ip: 127.0.0.1
num_agent_steps_sampled: 136000
num_agent_steps_trained: 0
num_env_steps_sampled: 136000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.3022878254857
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 25.61111111111111
  ram_util_percent: 57.0
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055106798426955886
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022524323322755394
  mean_inference_ms: 1.0107100413825165
  mean_raw_obs_processing_ms: 0.20504814001440422
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021309852600097656
    StateBufferConnector_ms: 0.0012624263763427734
    ViewRequirementAgentConnector_ms: 0.11250591278076172
  custom_metrics: {}
  episode_len_mean: 491.69
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 491.69
  episode_reward_min: 199.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [343, 500, 500, 500, 500, 462, 500, 500, 334, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 199, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 331, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [343.0, 500.0, 500.0, 500.0, 500.0, 462.0, 500.0, 500.0, 334.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 199.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 331.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055106798426955886
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022524323322755394
    mean_inference_ms: 1.0107100413825165
    mean_raw_obs_processing_ms: 0.20504814001440422
time_since_restore: 221.66389513015747
time_this_iter_s: 6.209486961364746
time_total_s: 221.66389513015747
timers:
  sample_time_ms: 2604.312
  synch_weights_time_ms: 3.442
  training_iteration_time_ms: 6199.434
timestamp: 1703047801
timesteps_total: 136000
training_iteration: 34
trial_id: default
-----------------------
----------------------

Iteration 34:
agent_timesteps_total: 140000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021142959594726562
  StateBufferConnector_ms: 0.0012731552124023438
  ViewRequirementAgentConnector_ms: 0.11237812042236328
counters:
  num_agent_steps_sampled: 140000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 140000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-07
done: false
episode_len_mean: 493.64
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 493.64
episode_reward_min: 199.0
episodes_this_iter: 8
episodes_total: 719
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.808502469744
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.814697322468419e-07
      curr_lr: 5.0e-05
      entropy: 0.3945041128567287
      mean_kl_loss: 0.006212770275449689
      policy_loss: -0.1366480521502949
      total_loss: 9.808502469744
      vf_explained_var: -0.1009057987303961
      vf_loss: 9.945150829496837
      vf_loss_unclipped: 4734.459333147322
  num_agent_steps_sampled: 140000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 140000
  num_env_steps_trained: 0
iterations_since_restore: 35
node_ip: 127.0.0.1
num_agent_steps_sampled: 140000
num_agent_steps_trained: 0
num_env_steps_sampled: 140000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.3033773765723
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.022222222222222
  ram_util_percent: 57.0
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05510566375036765
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02252221818995369
  mean_inference_ms: 1.010706394404088
  mean_raw_obs_processing_ms: 0.2049824105788283
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021142959594726562
    StateBufferConnector_ms: 0.0012731552124023438
    ViewRequirementAgentConnector_ms: 0.11237812042236328
  custom_metrics: {}
  episode_len_mean: 493.64
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 493.64
  episode_reward_min: 199.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [334, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 199, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 331, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [334.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 199.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 331.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05510566375036765
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02252221818995369
    mean_inference_ms: 1.010706394404088
    mean_raw_obs_processing_ms: 0.2049824105788283
time_since_restore: 227.89261412620544
time_this_iter_s: 6.228718996047974
time_total_s: 227.89261412620544
timers:
  sample_time_ms: 2603.467
  synch_weights_time_ms: 3.373
  training_iteration_time_ms: 6198.637
timestamp: 1703047807
timesteps_total: 140000
training_iteration: 35
trial_id: default
-----------------------
----------------------

Iteration 35:
agent_timesteps_total: 144000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0020995140075683594
  StateBufferConnector_ms: 0.0012865066528320312
  ViewRequirementAgentConnector_ms: 0.11261105537414551
counters:
  num_agent_steps_sampled: 144000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 144000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-13
done: false
episode_len_mean: 495.3
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 495.3
episode_reward_min: 199.0
episodes_this_iter: 8
episodes_total: 727
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.808857872372581
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.9073486612342094e-07
      curr_lr: 5.0e-05
      entropy: 0.39616282497133526
      mean_kl_loss: 0.004112952316569633
      policy_loss: -0.1359490170365288
      total_loss: 9.808857872372581
      vf_explained_var: -0.10412780443827312
      vf_loss: 9.944807052612305
      vf_loss_unclipped: 4695.579659598215
  num_agent_steps_sampled: 144000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 144000
  num_env_steps_trained: 0
iterations_since_restore: 36
node_ip: 127.0.0.1
num_agent_steps_sampled: 144000
num_agent_steps_trained: 0
num_env_steps_sampled: 144000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 631.8921666944499
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 29.922222222222224
  ram_util_percent: 57.01111111111111
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05510561170124859
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022520570647570706
  mean_inference_ms: 1.0107148121205025
  mean_raw_obs_processing_ms: 0.20492107804618695
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0020995140075683594
    StateBufferConnector_ms: 0.0012865066528320312
    ViewRequirementAgentConnector_ms: 0.11261105537414551
  custom_metrics: {}
  episode_len_mean: 495.3
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 495.3
  episode_reward_min: 199.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 199, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 331, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 199.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 331.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05510561170124859
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022520570647570706
    mean_inference_ms: 1.0107148121205025
    mean_raw_obs_processing_ms: 0.20492107804618695
time_since_restore: 234.22400522232056
time_this_iter_s: 6.331391096115112
time_total_s: 234.22400522232056
timers:
  sample_time_ms: 2605.119
  synch_weights_time_ms: 3.518
  training_iteration_time_ms: 6217.654
timestamp: 1703047813
timesteps_total: 144000
training_iteration: 36
trial_id: default
-----------------------
----------------------

Iteration 36:
agent_timesteps_total: 148000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.00209808349609375
  StateBufferConnector_ms: 0.001291036605834961
  ViewRequirementAgentConnector_ms: 0.11255097389221191
counters:
  num_agent_steps_sampled: 148000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 148000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-20
done: false
episode_len_mean: 498.31
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 498.31
episode_reward_min: 331.0
episodes_this_iter: 8
episodes_total: 735
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.80932281130836
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 9.536743306171047e-08
      curr_lr: 5.0e-05
      entropy: 0.413586057367779
      mean_kl_loss: 0.002323416012164411
      policy_loss: -0.135195863034044
      total_loss: 9.80932281130836
      vf_explained_var: -0.10748939287094843
      vf_loss: 9.94451854342506
      vf_loss_unclipped: 4656.780459449405
  num_agent_steps_sampled: 148000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 148000
  num_env_steps_trained: 0
iterations_since_restore: 37
node_ip: 127.0.0.1
num_agent_steps_sampled: 148000
num_agent_steps_trained: 0
num_env_steps_sampled: 148000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 639.0264344124769
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 31.233333333333334
  ram_util_percent: 57.03333333333333
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.0551079301604593
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022520888007229744
  mean_inference_ms: 1.0106824475837894
  mean_raw_obs_processing_ms: 0.20485838015047478
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.00209808349609375
    StateBufferConnector_ms: 0.001291036605834961
    ViewRequirementAgentConnector_ms: 0.11255097389221191
  custom_metrics: {}
  episode_len_mean: 498.31
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 498.31
  episode_reward_min: 331.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 331, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 331.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0551079301604593
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022520888007229744
    mean_inference_ms: 1.0106824475837894
    mean_raw_obs_processing_ms: 0.20485838015047478
time_since_restore: 240.48471236228943
time_this_iter_s: 6.260707139968872
time_total_s: 240.48471236228943
timers:
  sample_time_ms: 2607.949
  synch_weights_time_ms: 3.42
  training_iteration_time_ms: 6221.645
timestamp: 1703047820
timesteps_total: 148000
training_iteration: 37
trial_id: default
-----------------------
----------------------

Iteration 37:
agent_timesteps_total: 152000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002103090286254883
  StateBufferConnector_ms: 0.0012936592102050781
  ViewRequirementAgentConnector_ms: 0.11254215240478516
counters:
  num_agent_steps_sampled: 152000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 152000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-26
done: false
episode_len_mean: 498.31
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 498.31
episode_reward_min: 331.0
episodes_this_iter: 8
episodes_total: 743
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.808559962681361
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 4.7683716530855236e-08
      curr_lr: 5.0e-05
      entropy: 0.4194884385381426
      mean_kl_loss: 0.0040587418934967855
      policy_loss: -0.13568789902187528
      total_loss: 9.808559962681361
      vf_explained_var: -0.11121819700513567
      vf_loss: 9.944247654506139
      vf_loss_unclipped: 4618.206124441965
  num_agent_steps_sampled: 152000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 152000
  num_env_steps_trained: 0
iterations_since_restore: 38
node_ip: 127.0.0.1
num_agent_steps_sampled: 152000
num_agent_steps_trained: 0
num_env_steps_sampled: 152000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 645.2949366057908
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.766666666666666
  ram_util_percent: 56.94444444444444
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055108078662502995
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022519633800848073
  mean_inference_ms: 1.0106751818172588
  mean_raw_obs_processing_ms: 0.2047937375479357
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002103090286254883
    StateBufferConnector_ms: 0.0012936592102050781
    ViewRequirementAgentConnector_ms: 0.11254215240478516
  custom_metrics: {}
  episode_len_mean: 498.31
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 498.31
  episode_reward_min: 331.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 331, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 331.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055108078662502995
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022519633800848073
    mean_inference_ms: 1.0106751818172588
    mean_raw_obs_processing_ms: 0.2047937375479357
time_since_restore: 246.68455958366394
time_this_iter_s: 6.199847221374512
time_total_s: 246.68455958366394
timers:
  sample_time_ms: 2606.265
  synch_weights_time_ms: 3.389
  training_iteration_time_ms: 6220.658
timestamp: 1703047826
timesteps_total: 152000
training_iteration: 38
trial_id: default
-----------------------
----------------------

Iteration 38:
agent_timesteps_total: 156000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021944046020507812
  StateBufferConnector_ms: 0.001294851303100586
  ViewRequirementAgentConnector_ms: 0.11280298233032227
counters:
  num_agent_steps_sampled: 156000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 156000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-32
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 751
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.807815506344749
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.3841858265427618e-08
      curr_lr: 5.0e-05
      entropy: 0.41875809289160226
      mean_kl_loss: 0.004773950172835704
      policy_loss: -0.1360675252619244
      total_loss: 9.807815506344749
      vf_explained_var: -0.1145591622307187
      vf_loss: 9.943883123851958
      vf_loss_unclipped: 4579.6435779389885
  num_agent_steps_sampled: 156000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 156000
  num_env_steps_trained: 0
iterations_since_restore: 39
node_ip: 127.0.0.1
num_agent_steps_sampled: 156000
num_agent_steps_trained: 0
num_env_steps_sampled: 156000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.1377565058908
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.625
  ram_util_percent: 56.9
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05511010675819408
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022519442745145866
  mean_inference_ms: 1.010696138832853
  mean_raw_obs_processing_ms: 0.20473531603163983
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021944046020507812
    StateBufferConnector_ms: 0.001294851303100586
    ViewRequirementAgentConnector_ms: 0.11280298233032227
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05511010675819408
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022519442745145866
    mean_inference_ms: 1.010696138832853
    mean_raw_obs_processing_ms: 0.20473531603163983
time_since_restore: 252.91493773460388
time_this_iter_s: 6.230378150939941
time_total_s: 252.91493773460388
timers:
  sample_time_ms: 2607.947
  synch_weights_time_ms: 3.437
  training_iteration_time_ms: 6225.225
timestamp: 1703047832
timesteps_total: 156000
training_iteration: 39
trial_id: default
-----------------------
----------------------

Iteration 39:
agent_timesteps_total: 160000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022635459899902344
  StateBufferConnector_ms: 0.0012903213500976562
  ViewRequirementAgentConnector_ms: 0.11277437210083008
counters:
  num_agent_steps_sampled: 160000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 160000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-38
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 759
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.80773689633324
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.1920929132713809e-08
      curr_lr: 5.0e-05
      entropy: 0.3977196372690655
      mean_kl_loss: 0.004047748522885914
      policy_loss: -0.13585326820611954
      total_loss: 9.80773689633324
      vf_explained_var: -0.11835956005823045
      vf_loss: 9.943589937119256
      vf_loss_unclipped: 4541.237072172619
  num_agent_steps_sampled: 160000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 160000
  num_env_steps_trained: 0
iterations_since_restore: 40
node_ip: 127.0.0.1
num_agent_steps_sampled: 160000
num_agent_steps_trained: 0
num_env_steps_sampled: 160000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.9976342958554
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.54444444444444
  ram_util_percent: 56.911111111111104
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05511330930447066
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022519988653057217
  mean_inference_ms: 1.010736735862812
  mean_raw_obs_processing_ms: 0.20468259543662015
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022635459899902344
    StateBufferConnector_ms: 0.0012903213500976562
    ViewRequirementAgentConnector_ms: 0.11277437210083008
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05511330930447066
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022519988653057217
    mean_inference_ms: 1.010736735862812
    mean_raw_obs_processing_ms: 0.20468259543662015
time_since_restore: 259.11772656440735
time_this_iter_s: 6.202788829803467
time_total_s: 259.11772656440735
timers:
  sample_time_ms: 2608.155
  synch_weights_time_ms: 3.475
  training_iteration_time_ms: 6227.815
timestamp: 1703047838
timesteps_total: 160000
training_iteration: 40
trial_id: default
-----------------------
----------------------

Iteration 40:
agent_timesteps_total: 164000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0023462772369384766
  StateBufferConnector_ms: 0.0012886524200439453
  ViewRequirementAgentConnector_ms: 0.11280441284179688
counters:
  num_agent_steps_sampled: 164000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 164000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-45
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 767
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.80667173294794
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.9604645663569045e-09
      curr_lr: 5.0e-05
      entropy: 0.4084048242796035
      mean_kl_loss: 0.0038169732808774996
      policy_loss: -0.1365892326547986
      total_loss: 9.80667173294794
      vf_explained_var: -0.12212991714477539
      vf_loss: 9.94326096489316
      vf_loss_unclipped: 4503.140066964285
  num_agent_steps_sampled: 164000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 164000
  num_env_steps_trained: 0
iterations_since_restore: 41
node_ip: 127.0.0.1
num_agent_steps_sampled: 164000
num_agent_steps_trained: 0
num_env_steps_sampled: 164000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 647.9768355029419
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.833333333333336
  ram_util_percent: 56.900000000000006
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055117538516950136
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022521087524385724
  mean_inference_ms: 1.010787173156612
  mean_raw_obs_processing_ms: 0.20463380330924458
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0023462772369384766
    StateBufferConnector_ms: 0.0012886524200439453
    ViewRequirementAgentConnector_ms: 0.11280441284179688
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055117538516950136
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022521087524385724
    mean_inference_ms: 1.010787173156612
    mean_raw_obs_processing_ms: 0.20463380330924458
time_since_restore: 265.29215574264526
time_this_iter_s: 6.174429178237915
time_total_s: 265.29215574264526
timers:
  sample_time_ms: 2610.325
  synch_weights_time_ms: 3.461
  training_iteration_time_ms: 6224.322
timestamp: 1703047845
timesteps_total: 164000
training_iteration: 41
trial_id: default
-----------------------
----------------------

Iteration 41:
agent_timesteps_total: 168000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024445056915283203
  StateBufferConnector_ms: 0.001295328140258789
  ViewRequirementAgentConnector_ms: 0.11275529861450195
counters:
  num_agent_steps_sampled: 168000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 168000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-51
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 775
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.807043166387649
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.9802322831784522e-09
      curr_lr: 5.0e-05
      entropy: 0.38817988407044185
      mean_kl_loss: 0.00317179592518425
      policy_loss: -0.1358252675050781
      total_loss: 9.807043166387649
      vf_explained_var: -0.12573692344483875
      vf_loss: 9.942868414379301
      vf_loss_unclipped: 4464.7333984375
  num_agent_steps_sampled: 168000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 168000
  num_env_steps_trained: 0
iterations_since_restore: 42
node_ip: 127.0.0.1
num_agent_steps_sampled: 168000
num_agent_steps_trained: 0
num_env_steps_sampled: 168000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.9790121429374
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.344444444444445
  ram_util_percent: 56.900000000000006
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.0551209417172314
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022522500216527675
  mean_inference_ms: 1.0108311156771908
  mean_raw_obs_processing_ms: 0.2045843086957893
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024445056915283203
    StateBufferConnector_ms: 0.001295328140258789
    ViewRequirementAgentConnector_ms: 0.11275529861450195
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0551209417172314
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022522500216527675
    mean_inference_ms: 1.0108311156771908
    mean_raw_obs_processing_ms: 0.2045843086957893
time_since_restore: 271.51438570022583
time_this_iter_s: 6.222229957580566
time_total_s: 271.51438570022583
timers:
  sample_time_ms: 2611.121
  synch_weights_time_ms: 3.499
  training_iteration_time_ms: 6223.642
timestamp: 1703047851
timesteps_total: 168000
training_iteration: 42
trial_id: default
-----------------------
----------------------

Iteration 42:
agent_timesteps_total: 172000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002503633499145508
  StateBufferConnector_ms: 0.0012884140014648438
  ViewRequirementAgentConnector_ms: 0.11275124549865723
counters:
  num_agent_steps_sampled: 172000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 172000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-50-57
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 783
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.806837354387556
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.4901161415892261e-09
      curr_lr: 5.0e-05
      entropy: 0.3818746904532115
      mean_kl_loss: 0.0031594008316698733
      policy_loss: -0.13569191843271255
      total_loss: 9.806837354387556
      vf_explained_var: -0.1296935592378889
      vf_loss: 9.942529178801037
      vf_loss_unclipped: 4426.9438244047615
  num_agent_steps_sampled: 172000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 172000
  num_env_steps_trained: 0
iterations_since_restore: 43
node_ip: 127.0.0.1
num_agent_steps_sampled: 172000
num_agent_steps_trained: 0
num_env_steps_sampled: 172000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 646.0229993086638
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.95555555555556
  ram_util_percent: 56.94444444444444
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05512343097230787
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022522996902003812
  mean_inference_ms: 1.010864708264296
  mean_raw_obs_processing_ms: 0.20453415738923092
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002503633499145508
    StateBufferConnector_ms: 0.0012884140014648438
    ViewRequirementAgentConnector_ms: 0.11275124549865723
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05512343097230787
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022522996902003812
    mean_inference_ms: 1.010864708264296
    mean_raw_obs_processing_ms: 0.20453415738923092
time_since_restore: 277.7072696685791
time_this_iter_s: 6.1928839683532715
time_total_s: 277.7072696685791
timers:
  sample_time_ms: 2609.767
  synch_weights_time_ms: 3.502
  training_iteration_time_ms: 6224.078
timestamp: 1703047857
timesteps_total: 172000
training_iteration: 43
trial_id: default
-----------------------
----------------------

Iteration 43:
agent_timesteps_total: 176000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002504587173461914
  StateBufferConnector_ms: 0.0012950897216796875
  ViewRequirementAgentConnector_ms: 0.11292552947998047
counters:
  num_agent_steps_sampled: 176000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 176000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-03
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 791
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.806851931980678
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 7.450580707946131e-10
      curr_lr: 5.0e-05
      entropy: 0.3760666520822616
      mean_kl_loss: 0.00158246714871287
      policy_loss: -0.135306977445171
      total_loss: 9.806851931980678
      vf_explained_var: -0.13334320840381442
      vf_loss: 9.94215910775321
      vf_loss_unclipped: 4388.966122581845
  num_agent_steps_sampled: 176000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 176000
  num_env_steps_trained: 0
iterations_since_restore: 44
node_ip: 127.0.0.1
num_agent_steps_sampled: 176000
num_agent_steps_trained: 0
num_env_steps_sampled: 176000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 648.0242390467565
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.988888888888887
  ram_util_percent: 56.977777777777774
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05512669206402588
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02252423278702258
  mean_inference_ms: 1.0109132680204598
  mean_raw_obs_processing_ms: 0.20449110758991193
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002504587173461914
    StateBufferConnector_ms: 0.0012950897216796875
    ViewRequirementAgentConnector_ms: 0.11292552947998047
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05512669206402588
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02252423278702258
    mean_inference_ms: 1.0109132680204598
    mean_raw_obs_processing_ms: 0.20449110758991193
time_since_restore: 283.8809998035431
time_this_iter_s: 6.173730134963989
time_total_s: 283.8809998035431
timers:
  sample_time_ms: 2610.731
  synch_weights_time_ms: 3.462
  training_iteration_time_ms: 6220.512
timestamp: 1703047863
timesteps_total: 176000
training_iteration: 44
trial_id: default
-----------------------
----------------------

Iteration 44:
agent_timesteps_total: 180000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0025026798248291016
  StateBufferConnector_ms: 0.0012946128845214844
  ViewRequirementAgentConnector_ms: 0.11313533782958984
counters:
  num_agent_steps_sampled: 180000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 180000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-09
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 799
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.805780365353538
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.7252903539730653e-10
      curr_lr: 5.0e-05
      entropy: 0.3591977244331723
      mean_kl_loss: 0.003678088224068872
      policy_loss: -0.13592358962410972
      total_loss: 9.805780365353538
      vf_explained_var: -0.13739025025140672
      vf_loss: 9.941703932625908
      vf_loss_unclipped: 4351.068289620535
  num_agent_steps_sampled: 180000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 180000
  num_env_steps_trained: 0
iterations_since_restore: 45
node_ip: 127.0.0.1
num_agent_steps_sampled: 180000
num_agent_steps_trained: 0
num_env_steps_sampled: 180000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 636.6362300509682
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 36.2125
  ram_util_percent: 57.025
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05513186475850409
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022527071701424414
  mean_inference_ms: 1.0110028676630696
  mean_raw_obs_processing_ms: 0.20445804102174128
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0025026798248291016
    StateBufferConnector_ms: 0.0012946128845214844
    ViewRequirementAgentConnector_ms: 0.11313533782958984
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05513186475850409
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022527071701424414
    mean_inference_ms: 1.0110028676630696
    mean_raw_obs_processing_ms: 0.20445804102174128
time_since_restore: 290.1652548313141
time_this_iter_s: 6.284255027770996
time_total_s: 290.1652548313141
timers:
  sample_time_ms: 2617.549
  synch_weights_time_ms: 3.458
  training_iteration_time_ms: 6226.056
timestamp: 1703047869
timesteps_total: 180000
training_iteration: 45
trial_id: default
-----------------------
----------------------

Iteration 45:
agent_timesteps_total: 184000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024971961975097656
  StateBufferConnector_ms: 0.0012934207916259766
  ViewRequirementAgentConnector_ms: 0.11307811737060547
counters:
  num_agent_steps_sampled: 184000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 184000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-16
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 807
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.805222375052315
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.8626451769865326e-10
      curr_lr: 5.0e-05
      entropy: 0.35155104313577923
      mean_kl_loss: 0.004149530062209449
      policy_loss: -0.136069108687696
      total_loss: 9.805222375052315
      vf_explained_var: -0.1416992403212048
      vf_loss: 9.941291218712216
      vf_loss_unclipped: 4313.296293712798
  num_agent_steps_sampled: 184000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 184000
  num_env_steps_trained: 0
iterations_since_restore: 46
node_ip: 127.0.0.1
num_agent_steps_sampled: 184000
num_agent_steps_trained: 0
num_env_steps_sampled: 184000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 637.4476706540227
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 32.70000000000001
  ram_util_percent: 56.977777777777774
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05513657293772832
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022529910472914628
  mean_inference_ms: 1.0111309525121854
  mean_raw_obs_processing_ms: 0.20442457005178546
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024971961975097656
    StateBufferConnector_ms: 0.0012934207916259766
    ViewRequirementAgentConnector_ms: 0.11307811737060547
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05513657293772832
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022529910472914628
    mean_inference_ms: 1.0111309525121854
    mean_raw_obs_processing_ms: 0.20442457005178546
time_since_restore: 296.44146394729614
time_this_iter_s: 6.276209115982056
time_total_s: 296.44146394729614
timers:
  sample_time_ms: 2625.096
  synch_weights_time_ms: 3.423
  training_iteration_time_ms: 6220.539
timestamp: 1703047876
timesteps_total: 184000
training_iteration: 46
trial_id: default
-----------------------
----------------------

Iteration 46:
agent_timesteps_total: 188000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002513885498046875
  StateBufferConnector_ms: 0.001280069351196289
  ViewRequirementAgentConnector_ms: 0.1133413314819336
counters:
  num_agent_steps_sampled: 188000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 188000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-22
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 815
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.804454758053733
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 9.313225884932663e-11
      curr_lr: 5.0e-05
      entropy: 0.3431426967893328
      mean_kl_loss: 0.003636124370835941
      policy_loss: -0.13645185955933162
      total_loss: 9.804454758053733
      vf_explained_var: -0.1457690057300386
      vf_loss: 9.94090647924514
      vf_loss_unclipped: 4275.565104166667
  num_agent_steps_sampled: 188000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 188000
  num_env_steps_trained: 0
iterations_since_restore: 47
node_ip: 127.0.0.1
num_agent_steps_sampled: 188000
num_agent_steps_trained: 0
num_env_steps_sampled: 188000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 632.8159801460456
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.477777777777774
  ram_util_percent: 56.977777777777774
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05514149624410852
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022532709828110776
  mean_inference_ms: 1.011296617680804
  mean_raw_obs_processing_ms: 0.20439164674081947
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002513885498046875
    StateBufferConnector_ms: 0.001280069351196289
    ViewRequirementAgentConnector_ms: 0.1133413314819336
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05514149624410852
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022532709828110776
    mean_inference_ms: 1.011296617680804
    mean_raw_obs_processing_ms: 0.20439164674081947
time_since_restore: 302.76361894607544
time_this_iter_s: 6.322154998779297
time_total_s: 302.76361894607544
timers:
  sample_time_ms: 2633.772
  synch_weights_time_ms: 3.433
  training_iteration_time_ms: 6226.682
timestamp: 1703047882
timesteps_total: 188000
training_iteration: 47
trial_id: default
-----------------------
----------------------

Iteration 47:
agent_timesteps_total: 192000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.00250244140625
  StateBufferConnector_ms: 0.0012769699096679688
  ViewRequirementAgentConnector_ms: 0.11299419403076172
counters:
  num_agent_steps_sampled: 192000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 192000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-28
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 823
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.804222515651158
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 4.6566129424663316e-11
      curr_lr: 5.0e-05
      entropy: 0.35651097127369474
      mean_kl_loss: 0.004254491010042573
      policy_loss: -0.13632770060073762
      total_loss: 9.804222515651158
      vf_explained_var: -0.1501708882195609
      vf_loss: 9.940550077529181
      vf_loss_unclipped: 4238.455031622024
  num_agent_steps_sampled: 192000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 192000
  num_env_steps_trained: 0
iterations_since_restore: 48
node_ip: 127.0.0.1
num_agent_steps_sampled: 192000
num_agent_steps_trained: 0
num_env_steps_sampled: 192000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.0855091352586
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 29.28888888888889
  ram_util_percent: 57.01111111111111
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05514668263050533
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022535734636304813
  mean_inference_ms: 1.0114719800074172
  mean_raw_obs_processing_ms: 0.20436012965371492
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.00250244140625
    StateBufferConnector_ms: 0.0012769699096679688
    ViewRequirementAgentConnector_ms: 0.11299419403076172
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05514668263050533
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022535734636304813
    mean_inference_ms: 1.0114719800074172
    mean_raw_obs_processing_ms: 0.20436012965371492
time_since_restore: 308.99446272850037
time_this_iter_s: 6.230843782424927
time_total_s: 308.99446272850037
timers:
  sample_time_ms: 2636.958
  synch_weights_time_ms: 3.409
  training_iteration_time_ms: 6229.781
timestamp: 1703047888
timesteps_total: 192000
training_iteration: 48
trial_id: default
-----------------------
----------------------

Iteration 48:
agent_timesteps_total: 196000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0025093555450439453
  StateBufferConnector_ms: 0.0012924671173095703
  ViewRequirementAgentConnector_ms: 0.11315584182739258
counters:
  num_agent_steps_sampled: 196000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 196000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-34
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 831
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.802841413588752
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 4.6566129424663316e-11
      curr_lr: 5.0e-05
      entropy: 0.32155332395008634
      mean_kl_loss: 0.006427948152306618
      policy_loss: -0.13745212767805373
      total_loss: 9.802841413588752
      vf_explained_var: -0.15421791303725468
      vf_loss: 9.940293357485817
      vf_loss_unclipped: 4201.800595238095
  num_agent_steps_sampled: 196000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 196000
  num_env_steps_trained: 0
iterations_since_restore: 49
node_ip: 127.0.0.1
num_agent_steps_sampled: 196000
num_agent_steps_trained: 0
num_env_steps_sampled: 196000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 646.6870675429869
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.08888888888889
  ram_util_percent: 57.022222222222226
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055150556344372784
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022538524193623108
  mean_inference_ms: 1.011632149250999
  mean_raw_obs_processing_ms: 0.20432675191659555
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0025093555450439453
    StateBufferConnector_ms: 0.0012924671173095703
    ViewRequirementAgentConnector_ms: 0.11315584182739258
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055150556344372784
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022538524193623108
    mean_inference_ms: 1.011632149250999
    mean_raw_obs_processing_ms: 0.20432675191659555
time_since_restore: 315.1809628009796
time_this_iter_s: 6.186500072479248
time_total_s: 315.1809628009796
timers:
  sample_time_ms: 2634.784
  synch_weights_time_ms: 3.391
  training_iteration_time_ms: 6225.399
timestamp: 1703047894
timesteps_total: 196000
training_iteration: 49
trial_id: default
-----------------------
----------------------

Iteration 49:
agent_timesteps_total: 200000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0025010108947753906
  StateBufferConnector_ms: 0.0013022422790527344
  ViewRequirementAgentConnector_ms: 0.11330032348632812
counters:
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-41
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 839
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.804339318048386
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.3283064712331658e-11
      curr_lr: 5.0e-05
      entropy: 0.32708077345575604
      mean_kl_loss: 0.003453875197952324
      policy_loss: -0.13565665447995776
      total_loss: 9.804339318048386
      vf_explained_var: -0.15851221765790666
      vf_loss: 9.93999599275135
      vf_loss_unclipped: 4165.926374162947
  num_agent_steps_sampled: 200000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 200000
  num_env_steps_trained: 0
iterations_since_restore: 50
node_ip: 127.0.0.1
num_agent_steps_sampled: 200000
num_agent_steps_trained: 0
num_env_steps_sampled: 200000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 641.3375551917054
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 30.244444444444444
  ram_util_percent: 57.022222222222226
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05515678440710106
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022542504920308525
  mean_inference_ms: 1.0118205115990566
  mean_raw_obs_processing_ms: 0.20430464377196358
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0025010108947753906
    StateBufferConnector_ms: 0.0013022422790527344
    ViewRequirementAgentConnector_ms: 0.11330032348632812
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05515678440710106
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022542504920308525
    mean_inference_ms: 1.0118205115990566
    mean_raw_obs_processing_ms: 0.20430464377196358
time_since_restore: 321.4190979003906
time_this_iter_s: 6.238135099411011
time_total_s: 321.4190979003906
timers:
  sample_time_ms: 2639.462
  synch_weights_time_ms: 3.367
  training_iteration_time_ms: 6228.938
timestamp: 1703047901
timesteps_total: 200000
training_iteration: 50
trial_id: default
-----------------------
----------------------

Iteration 50:
agent_timesteps_total: 204000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002414703369140625
  StateBufferConnector_ms: 0.0012958049774169922
  ViewRequirementAgentConnector_ms: 0.11315512657165527
counters:
  num_agent_steps_sampled: 204000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 204000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-47
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 847
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.804277420043945
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.1641532356165829e-11
      curr_lr: 5.0e-05
      entropy: 0.31626653387433007
      mean_kl_loss: 0.002345042263671292
      policy_loss: -0.1354249974801427
      total_loss: 9.804277420043945
      vf_explained_var: -0.16270348003932408
      vf_loss: 9.939702306474958
      vf_loss_unclipped: 4130.838169642857
  num_agent_steps_sampled: 204000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 204000
  num_env_steps_trained: 0
iterations_since_restore: 51
node_ip: 127.0.0.1
num_agent_steps_sampled: 204000
num_agent_steps_trained: 0
num_env_steps_sampled: 204000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 641.7173400219689
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.922222222222224
  ram_util_percent: 57.03333333333333
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05516218191244213
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022546283929908838
  mean_inference_ms: 1.011990758286763
  mean_raw_obs_processing_ms: 0.2042828615839748
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002414703369140625
    StateBufferConnector_ms: 0.0012958049774169922
    ViewRequirementAgentConnector_ms: 0.11315512657165527
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05516218191244213
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022546283929908838
    mean_inference_ms: 1.011990758286763
    mean_raw_obs_processing_ms: 0.2042828615839748
time_since_restore: 327.65398383140564
time_this_iter_s: 6.234885931015015
time_total_s: 327.65398383140564
timers:
  sample_time_ms: 2635.798
  synch_weights_time_ms: 3.305
  training_iteration_time_ms: 6234.959
timestamp: 1703047907
timesteps_total: 204000
training_iteration: 51
trial_id: default
-----------------------
----------------------

Iteration 51:
agent_timesteps_total: 208000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0023345947265625
  StateBufferConnector_ms: 0.001293182373046875
  ViewRequirementAgentConnector_ms: 0.11304283142089844
counters:
  num_agent_steps_sampled: 208000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 208000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-53
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 855
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.80385244460333
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.8207661780829145e-12
      curr_lr: 5.0e-05
      entropy: 0.3237809978780292
      mean_kl_loss: 0.002695690481768257
      policy_loss: -0.1355630499976022
      total_loss: 9.80385244460333
      vf_explained_var: -0.1670856135232108
      vf_loss: 9.939415341331845
      vf_loss_unclipped: 4096.151204427083
  num_agent_steps_sampled: 208000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 208000
  num_env_steps_trained: 0
iterations_since_restore: 52
node_ip: 127.0.0.1
num_agent_steps_sampled: 208000
num_agent_steps_trained: 0
num_env_steps_sampled: 208000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 640.5070909181562
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.266666666666666
  ram_util_percent: 57.08888888888888
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05516714368763743
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022549676173234744
  mean_inference_ms: 1.0121510600923882
  mean_raw_obs_processing_ms: 0.20426242407197484
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0023345947265625
    StateBufferConnector_ms: 0.001293182373046875
    ViewRequirementAgentConnector_ms: 0.11304283142089844
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05516714368763743
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022549676173234744
    mean_inference_ms: 1.0121510600923882
    mean_raw_obs_processing_ms: 0.20426242407197484
time_since_restore: 333.9001648426056
time_this_iter_s: 6.246181011199951
time_total_s: 333.9001648426056
timers:
  sample_time_ms: 2636.992
  synch_weights_time_ms: 3.358
  training_iteration_time_ms: 6237.36
timestamp: 1703047913
timesteps_total: 208000
training_iteration: 52
trial_id: default
-----------------------
----------------------

Iteration 52:
agent_timesteps_total: 212000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0023033618927001953
  StateBufferConnector_ms: 0.001302957534790039
  ViewRequirementAgentConnector_ms: 0.11312389373779297
counters:
  num_agent_steps_sampled: 212000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 212000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-51-59
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 863
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.802955082484655
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.9103830890414573e-12
      curr_lr: 5.0e-05
      entropy: 0.34097374478975934
      mean_kl_loss: 0.003929072403524613
      policy_loss: -0.1361759060195514
      total_loss: 9.802955082484655
      vf_explained_var: -0.1715801443372454
      vf_loss: 9.939130964733305
      vf_loss_unclipped: 4062.043503534226
  num_agent_steps_sampled: 212000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 212000
  num_env_steps_trained: 0
iterations_since_restore: 53
node_ip: 127.0.0.1
num_agent_steps_sampled: 212000
num_agent_steps_trained: 0
num_env_steps_sampled: 212000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 645.765811926795
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.488888888888894
  ram_util_percent: 57.044444444444444
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05517170218911815
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022552958862120463
  mean_inference_ms: 1.0123068025357387
  mean_raw_obs_processing_ms: 0.20424460896273616
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0023033618927001953
    StateBufferConnector_ms: 0.001302957534790039
    ViewRequirementAgentConnector_ms: 0.11312389373779297
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05517170218911815
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022552958862120463
    mean_inference_ms: 1.0123068025357387
    mean_raw_obs_processing_ms: 0.20424460896273616
time_since_restore: 340.0955317020416
time_this_iter_s: 6.195366859436035
time_total_s: 340.0955317020416
timers:
  sample_time_ms: 2640.21
  synch_weights_time_ms: 3.357
  training_iteration_time_ms: 6237.607
timestamp: 1703047919
timesteps_total: 212000
training_iteration: 53
trial_id: default
-----------------------
----------------------

Iteration 53:
agent_timesteps_total: 216000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002237081527709961
  StateBufferConnector_ms: 0.0012965202331542969
  ViewRequirementAgentConnector_ms: 0.11338329315185547
counters:
  num_agent_steps_sampled: 216000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 216000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-06
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 871
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.802761350359235
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.4551915445207286e-12
      curr_lr: 5.0e-05
      entropy: 0.33347976491564796
      mean_kl_loss: 0.003065215571014267
      policy_loss: -0.1360835645880018
      total_loss: 9.802761350359235
      vf_explained_var: -0.17602912017277308
      vf_loss: 9.93884513491676
      vf_loss_unclipped: 4028.701927548363
  num_agent_steps_sampled: 216000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 216000
  num_env_steps_trained: 0
iterations_since_restore: 54
node_ip: 127.0.0.1
num_agent_steps_sampled: 216000
num_agent_steps_trained: 0
num_env_steps_sampled: 216000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 637.9109186380707
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 29.987499999999997
  ram_util_percent: 57.05
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05517656042179727
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022556389754180922
  mean_inference_ms: 1.0124639763161696
  mean_raw_obs_processing_ms: 0.20423072508948578
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002237081527709961
    StateBufferConnector_ms: 0.0012965202331542969
    ViewRequirementAgentConnector_ms: 0.11338329315185547
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05517656042179727
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022556389754180922
    mean_inference_ms: 1.0124639763161696
    mean_raw_obs_processing_ms: 0.20423072508948578
time_since_restore: 346.3672137260437
time_this_iter_s: 6.271682024002075
time_total_s: 346.3672137260437
timers:
  sample_time_ms: 2641.21
  synch_weights_time_ms: 3.361
  training_iteration_time_ms: 6247.392
timestamp: 1703047926
timesteps_total: 216000
training_iteration: 54
trial_id: default
-----------------------
----------------------

Iteration 54:
agent_timesteps_total: 220000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002160310745239258
  StateBufferConnector_ms: 0.0013031959533691406
  ViewRequirementAgentConnector_ms: 0.11333489418029785
counters:
  num_agent_steps_sampled: 220000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 220000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-12
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 879
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.802907762073335
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 7.275957722603643e-13
      curr_lr: 5.0e-05
      entropy: 0.3396118595486596
      mean_kl_loss: 0.003704766143748711
      policy_loss: -0.1356609428212756
      total_loss: 9.802907762073335
      vf_explained_var: -0.1803192070552281
      vf_loss: 9.938568569365001
      vf_loss_unclipped: 3995.5313778831846
  num_agent_steps_sampled: 220000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 220000
  num_env_steps_trained: 0
iterations_since_restore: 55
node_ip: 127.0.0.1
num_agent_steps_sampled: 220000
num_agent_steps_trained: 0
num_env_steps_sampled: 220000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 646.2287125615777
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.288888888888888
  ram_util_percent: 57.099999999999994
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05518109121797037
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.0225594916801685
  mean_inference_ms: 1.0126090775701002
  mean_raw_obs_processing_ms: 0.2042186952903051
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002160310745239258
    StateBufferConnector_ms: 0.0013031959533691406
    ViewRequirementAgentConnector_ms: 0.11333489418029785
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05518109121797037
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.0225594916801685
    mean_inference_ms: 1.0126090775701002
    mean_raw_obs_processing_ms: 0.2042186952903051
time_since_restore: 352.5580835342407
time_this_iter_s: 6.1908698081970215
time_total_s: 352.5580835342407
timers:
  sample_time_ms: 2631.951
  synch_weights_time_ms: 3.527
  training_iteration_time_ms: 6238.066
timestamp: 1703047932
timesteps_total: 220000
training_iteration: 55
trial_id: default
-----------------------
----------------------

Iteration 55:
agent_timesteps_total: 224000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002111196517944336
  StateBufferConnector_ms: 0.0013158321380615234
  ViewRequirementAgentConnector_ms: 0.11367297172546387
counters:
  num_agent_steps_sampled: 224000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 224000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-18
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 887
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.802923974536714
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.6379788613018216e-13
      curr_lr: 5.0e-05
      entropy: 0.3254605233669281
      mean_kl_loss: 0.0033687540849774407
      policy_loss: -0.1353300146403767
      total_loss: 9.802923974536714
      vf_explained_var: -0.18467458656855992
      vf_loss: 9.938253993079776
      vf_loss_unclipped: 3963.053908575149
  num_agent_steps_sampled: 224000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 224000
  num_env_steps_trained: 0
iterations_since_restore: 56
node_ip: 127.0.0.1
num_agent_steps_sampled: 224000
num_agent_steps_trained: 0
num_env_steps_sampled: 224000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 617.6736325527228
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 40.788888888888884
  ram_util_percent: 57.37777777777777
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055185655129958835
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02256276911959893
  mean_inference_ms: 1.0127609313942871
  mean_raw_obs_processing_ms: 0.20421125167341522
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002111196517944336
    StateBufferConnector_ms: 0.0013158321380615234
    ViewRequirementAgentConnector_ms: 0.11367297172546387
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055185655129958835
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02256276911959893
    mean_inference_ms: 1.0127609313942871
    mean_raw_obs_processing_ms: 0.20421125167341522
time_since_restore: 359.0352144241333
time_this_iter_s: 6.477130889892578
time_total_s: 359.0352144241333
timers:
  sample_time_ms: 2625.97
  synch_weights_time_ms: 3.346
  training_iteration_time_ms: 6258.155
timestamp: 1703047938
timesteps_total: 224000
training_iteration: 56
trial_id: default
-----------------------
----------------------

Iteration 56:
agent_timesteps_total: 228000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002123594284057617
  StateBufferConnector_ms: 0.001316070556640625
  ViewRequirementAgentConnector_ms: 0.11380600929260254
counters:
  num_agent_steps_sampled: 228000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 228000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-25
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 895
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800370670500255
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.6379788613018216e-13
      curr_lr: 5.0e-05
      entropy: 0.33646414677302044
      mean_kl_loss: 0.006044281021610502
      policy_loss: -0.13760221678586232
      total_loss: 9.800370670500255
      vf_explained_var: -0.18933264982132686
      vf_loss: 9.937972613743373
      vf_loss_unclipped: 3931.044945126488
  num_agent_steps_sampled: 228000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 228000
  num_env_steps_trained: 0
iterations_since_restore: 57
node_ip: 127.0.0.1
num_agent_steps_sampled: 228000
num_agent_steps_trained: 0
num_env_steps_sampled: 228000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 635.5200568259817
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 42.111111111111114
  ram_util_percent: 57.82222222222222
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055189966855364425
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022565693053958544
  mean_inference_ms: 1.012907111259712
  mean_raw_obs_processing_ms: 0.20421027836987432
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002123594284057617
    StateBufferConnector_ms: 0.001316070556640625
    ViewRequirementAgentConnector_ms: 0.11380600929260254
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055189966855364425
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022565693053958544
    mean_inference_ms: 1.012907111259712
    mean_raw_obs_processing_ms: 0.20421027836987432
time_since_restore: 365.33052039146423
time_this_iter_s: 6.295305967330933
time_total_s: 365.33052039146423
timers:
  sample_time_ms: 2626.331
  synch_weights_time_ms: 3.332
  training_iteration_time_ms: 6255.465
timestamp: 1703047945
timesteps_total: 228000
training_iteration: 57
trial_id: default
-----------------------
----------------------

Iteration 57:
agent_timesteps_total: 232000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002122163772583008
  StateBufferConnector_ms: 0.001325845718383789
  ViewRequirementAgentConnector_ms: 0.11370563507080078
counters:
  num_agent_steps_sampled: 232000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 232000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-31
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 903
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800977479843866
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.8189894306509108e-13
      curr_lr: 5.0e-05
      entropy: 0.335126104808989
      mean_kl_loss: 0.003868318524324433
      policy_loss: -0.13673440508899234
      total_loss: 9.800977479843866
      vf_explained_var: -0.19395555201030912
      vf_loss: 9.937711851937431
      vf_loss_unclipped: 3899.5291457403273
  num_agent_steps_sampled: 232000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 232000
  num_env_steps_trained: 0
iterations_since_restore: 58
node_ip: 127.0.0.1
num_agent_steps_sampled: 232000
num_agent_steps_trained: 0
num_env_steps_sampled: 232000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 643.5875271220489
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 35.3
  ram_util_percent: 57.833333333333336
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05519431902100003
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.0225686999818668
  mean_inference_ms: 1.0129993716342396
  mean_raw_obs_processing_ms: 0.2042124604986879
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002122163772583008
    StateBufferConnector_ms: 0.001325845718383789
    ViewRequirementAgentConnector_ms: 0.11370563507080078
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05519431902100003
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.0225686999818668
    mean_inference_ms: 1.0129993716342396
    mean_raw_obs_processing_ms: 0.2042124604986879
time_since_restore: 371.54680347442627
time_this_iter_s: 6.216283082962036
time_total_s: 371.54680347442627
timers:
  sample_time_ms: 2625.126
  synch_weights_time_ms: 3.322
  training_iteration_time_ms: 6254.011
timestamp: 1703047951
timesteps_total: 232000
training_iteration: 58
trial_id: default
-----------------------
----------------------

Iteration 58:
agent_timesteps_total: 236000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002111673355102539
  StateBufferConnector_ms: 0.001333475112915039
  ViewRequirementAgentConnector_ms: 0.1137399673461914
counters:
  num_agent_steps_sampled: 236000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 236000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-37
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 911
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.801899955386208
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 9.094947153254554e-14
      curr_lr: 5.0e-05
      entropy: 0.30485241895630244
      mean_kl_loss: 0.0025430893128508443
      policy_loss: -0.13552683201574145
      total_loss: 9.801899955386208
      vf_explained_var: -0.1982056981041318
      vf_loss: 9.937426930382138
      vf_loss_unclipped: 3868.1941964285716
  num_agent_steps_sampled: 236000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 236000
  num_env_steps_trained: 0
iterations_since_restore: 59
node_ip: 127.0.0.1
num_agent_steps_sampled: 236000
num_agent_steps_trained: 0
num_env_steps_sampled: 236000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 649.1588341633537
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 31.400000000000002
  ram_util_percent: 57.85555555555555
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055198217055459846
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022571604400930863
  mean_inference_ms: 1.0130869847232833
  mean_raw_obs_processing_ms: 0.2042161890187856
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002111673355102539
    StateBufferConnector_ms: 0.001333475112915039
    ViewRequirementAgentConnector_ms: 0.1137399673461914
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055198217055459846
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022571604400930863
    mean_inference_ms: 1.0130869847232833
    mean_raw_obs_processing_ms: 0.2042161890187856
time_since_restore: 377.7099165916443
time_this_iter_s: 6.163113117218018
time_total_s: 377.7099165916443
timers:
  sample_time_ms: 2626.394
  synch_weights_time_ms: 3.391
  training_iteration_time_ms: 6251.656
timestamp: 1703047957
timesteps_total: 236000
training_iteration: 59
trial_id: default
-----------------------
----------------------

Iteration 59:
agent_timesteps_total: 240000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002105712890625
  StateBufferConnector_ms: 0.0013306140899658203
  ViewRequirementAgentConnector_ms: 0.11376810073852539
counters:
  num_agent_steps_sampled: 240000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 240000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-43
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 919
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800643920898438
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 9.094947153254554e-14
      curr_lr: 5.0e-05
      entropy: 0.30367339508874075
      mean_kl_loss: 0.005239777022145816
      policy_loss: -0.13650615619761602
      total_loss: 9.800643920898438
      vf_explained_var: -0.20266712279546828
      vf_loss: 9.93715018317813
      vf_loss_unclipped: 3837.221505301339
  num_agent_steps_sampled: 240000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 240000
  num_env_steps_trained: 0
iterations_since_restore: 60
node_ip: 127.0.0.1
num_agent_steps_sampled: 240000
num_agent_steps_trained: 0
num_env_steps_sampled: 240000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 648.3949966672454
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 32.05555555555556
  ram_util_percent: 57.78888888888889
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055201134641573966
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02257353142325083
  mean_inference_ms: 1.013111764075142
  mean_raw_obs_processing_ms: 0.20422228776164053
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002105712890625
    StateBufferConnector_ms: 0.0013306140899658203
    ViewRequirementAgentConnector_ms: 0.11376810073852539
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055201134641573966
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02257353142325083
    mean_inference_ms: 1.013111764075142
    mean_raw_obs_processing_ms: 0.20422228776164053
time_since_restore: 383.88018345832825
time_this_iter_s: 6.17026686668396
time_total_s: 383.88018345832825
timers:
  sample_time_ms: 2618.991
  synch_weights_time_ms: 3.559
  training_iteration_time_ms: 6244.867
timestamp: 1703047963
timesteps_total: 240000
training_iteration: 60
trial_id: default
-----------------------
----------------------

Iteration 60:
agent_timesteps_total: 244000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021102428436279297
  StateBufferConnector_ms: 0.0013391971588134766
  ViewRequirementAgentConnector_ms: 0.11399221420288086
counters:
  num_agent_steps_sampled: 244000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 244000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-49
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 927
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.801327796209426
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 4.547473576627277e-14
      curr_lr: 5.0e-05
      entropy: 0.31927378404708134
      mean_kl_loss: 0.0030258772953478306
      policy_loss: -0.13554831878060386
      total_loss: 9.801327796209426
      vf_explained_var: -0.20761068094344365
      vf_loss: 9.936875979105631
      vf_loss_unclipped: 3806.7928641183034
  num_agent_steps_sampled: 244000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 244000
  num_env_steps_trained: 0
iterations_since_restore: 61
node_ip: 127.0.0.1
num_agent_steps_sampled: 244000
num_agent_steps_trained: 0
num_env_steps_sampled: 244000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 649.693832286506
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.125
  ram_util_percent: 57.8125
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055203841058881584
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02257496873824478
  mean_inference_ms: 1.0131169793991086
  mean_raw_obs_processing_ms: 0.20422989767873262
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021102428436279297
    StateBufferConnector_ms: 0.0013391971588134766
    ViewRequirementAgentConnector_ms: 0.11399221420288086
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055203841058881584
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02257496873824478
    mean_inference_ms: 1.0131169793991086
    mean_raw_obs_processing_ms: 0.20422989767873262
time_since_restore: 390.03812646865845
time_this_iter_s: 6.1579430103302
time_total_s: 390.03812646865845
timers:
  sample_time_ms: 2618.508
  synch_weights_time_ms: 3.554
  training_iteration_time_ms: 6237.214
timestamp: 1703047969
timesteps_total: 244000
training_iteration: 61
trial_id: default
-----------------------
----------------------

Iteration 61:
agent_timesteps_total: 248000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021219253540039062
  StateBufferConnector_ms: 0.0013284683227539062
  ViewRequirementAgentConnector_ms: 0.113922119140625
counters:
  num_agent_steps_sampled: 248000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 248000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-52-56
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 935
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800651323227655
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.2737367883136385e-14
      curr_lr: 5.0e-05
      entropy: 0.3278573723066421
      mean_kl_loss: 0.003077429150312669
      policy_loss: -0.1359445122735841
      total_loss: 9.800651323227655
      vf_explained_var: -0.21184595425923666
      vf_loss: 9.936595780508858
      vf_loss_unclipped: 3776.618117559524
  num_agent_steps_sampled: 248000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 248000
  num_env_steps_trained: 0
iterations_since_restore: 62
node_ip: 127.0.0.1
num_agent_steps_sampled: 248000
num_agent_steps_trained: 0
num_env_steps_sampled: 248000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 636.4453899557715
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 32.93333333333333
  ram_util_percent: 57.78888888888888
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05520490349935933
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022575320985335497
  mean_inference_ms: 1.0131082368093984
  mean_raw_obs_processing_ms: 0.20423380888153567
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021219253540039062
    StateBufferConnector_ms: 0.0013284683227539062
    ViewRequirementAgentConnector_ms: 0.113922119140625
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05520490349935933
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022575320985335497
    mean_inference_ms: 1.0131082368093984
    mean_raw_obs_processing_ms: 0.20423380888153567
time_since_restore: 396.3241333961487
time_this_iter_s: 6.286006927490234
time_total_s: 396.3241333961487
timers:
  sample_time_ms: 2617.68
  synch_weights_time_ms: 3.46
  training_iteration_time_ms: 6241.2
timestamp: 1703047976
timesteps_total: 248000
training_iteration: 62
trial_id: default
-----------------------
----------------------

Iteration 62:
agent_timesteps_total: 252000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021104812622070312
  StateBufferConnector_ms: 0.0013248920440673828
  ViewRequirementAgentConnector_ms: 0.11380672454833984
counters:
  num_agent_steps_sampled: 252000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 252000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-02
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 943
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800271987915039
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.1368683941568192e-14
      curr_lr: 5.0e-05
      entropy: 0.2955300368013836
      mean_kl_loss: 0.0035069504457017806
      policy_loss: -0.13604034802743367
      total_loss: 9.800271987915039
      vf_explained_var: -0.21680724620819092
      vf_loss: 9.936312175932384
      vf_loss_unclipped: 3746.8297293526784
  num_agent_steps_sampled: 252000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 252000
  num_env_steps_trained: 0
iterations_since_restore: 63
node_ip: 127.0.0.1
num_agent_steps_sampled: 252000
num_agent_steps_trained: 0
num_env_steps_sampled: 252000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 643.9933661514812
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 30.233333333333334
  ram_util_percent: 57.844444444444434
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05520492785655584
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022574747239380202
  mean_inference_ms: 1.0130666681151563
  mean_raw_obs_processing_ms: 0.20423641263169245
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021104812622070312
    StateBufferConnector_ms: 0.0013248920440673828
    ViewRequirementAgentConnector_ms: 0.11380672454833984
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05520492785655584
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022574747239380202
    mean_inference_ms: 1.0130666681151563
    mean_raw_obs_processing_ms: 0.20423641263169245
time_since_restore: 402.5365333557129
time_this_iter_s: 6.212399959564209
time_total_s: 402.5365333557129
timers:
  sample_time_ms: 2612.953
  synch_weights_time_ms: 3.554
  training_iteration_time_ms: 6242.905
timestamp: 1703047982
timesteps_total: 252000
training_iteration: 63
trial_id: default
-----------------------
----------------------

Iteration 63:
agent_timesteps_total: 256000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.00218963623046875
  StateBufferConnector_ms: 0.0013232231140136719
  ViewRequirementAgentConnector_ms: 0.1135704517364502
counters:
  num_agent_steps_sampled: 256000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 256000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-08
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 951
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.801054273332868
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.684341970784096e-15
      curr_lr: 5.0e-05
      entropy: 0.30221218296459745
      mean_kl_loss: 0.0027306567749346435
      policy_loss: -0.13500766562564032
      total_loss: 9.801054273332868
      vf_explained_var: -0.22134728658766972
      vf_loss: 9.936062177022299
      vf_loss_unclipped: 3717.353492373512
  num_agent_steps_sampled: 256000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 256000
  num_env_steps_trained: 0
iterations_since_restore: 64
node_ip: 127.0.0.1
num_agent_steps_sampled: 256000
num_agent_steps_trained: 0
num_env_steps_sampled: 256000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 649.2597732809572
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 29.688888888888894
  ram_util_percent: 57.666666666666664
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05520331704153406
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022572694341702285
  mean_inference_ms: 1.0129871541386526
  mean_raw_obs_processing_ms: 0.20423656183165495
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.00218963623046875
    StateBufferConnector_ms: 0.0013232231140136719
    ViewRequirementAgentConnector_ms: 0.1135704517364502
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05520331704153406
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022572694341702285
    mean_inference_ms: 1.0129871541386526
    mean_raw_obs_processing_ms: 0.20423656183165495
time_since_restore: 408.69853615760803
time_this_iter_s: 6.162002801895142
time_total_s: 408.69853615760803
timers:
  sample_time_ms: 2605.636
  synch_weights_time_ms: 3.559
  training_iteration_time_ms: 6231.945
timestamp: 1703047988
timesteps_total: 256000
training_iteration: 64
trial_id: default
-----------------------
----------------------

Iteration 64:
agent_timesteps_total: 260000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022623538970947266
  StateBufferConnector_ms: 0.0013303756713867188
  ViewRequirementAgentConnector_ms: 0.11344742774963379
counters:
  num_agent_steps_sampled: 260000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 260000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-14
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 959
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800109727042061
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.842170985392048e-15
      curr_lr: 5.0e-05
      entropy: 0.3199536431403387
      mean_kl_loss: 0.003232495190455135
      policy_loss: -0.13565798245725177
      total_loss: 9.800109727042061
      vf_explained_var: -0.22582619530814035
      vf_loss: 9.935767718723842
      vf_loss_unclipped: 3688.1999627976193
  num_agent_steps_sampled: 260000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 260000
  num_env_steps_trained: 0
iterations_since_restore: 65
node_ip: 127.0.0.1
num_agent_steps_sampled: 260000
num_agent_steps_trained: 0
num_env_steps_sampled: 260000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 646.6064639618563
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 31.188888888888886
  ram_util_percent: 57.70000000000001
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05519999515563072
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022569594955984002
  mean_inference_ms: 1.0128781931819242
  mean_raw_obs_processing_ms: 0.20423284693613145
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022623538970947266
    StateBufferConnector_ms: 0.0013303756713867188
    ViewRequirementAgentConnector_ms: 0.11344742774963379
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05519999515563072
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022569594955984002
    mean_inference_ms: 1.0128781931819242
    mean_raw_obs_processing_ms: 0.20423284693613145
time_since_restore: 414.88584423065186
time_this_iter_s: 6.187308073043823
time_total_s: 414.88584423065186
timers:
  sample_time_ms: 2604.277
  synch_weights_time_ms: 3.542
  training_iteration_time_ms: 6231.583
timestamp: 1703047994
timesteps_total: 260000
training_iteration: 65
trial_id: default
-----------------------
----------------------

Iteration 65:
agent_timesteps_total: 264000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0023431777954101562
  StateBufferConnector_ms: 0.0013289451599121094
  ViewRequirementAgentConnector_ms: 0.1133584976196289
counters:
  num_agent_steps_sampled: 264000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 264000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-21
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 967
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800236656552268
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.421085492696024e-15
      curr_lr: 5.0e-05
      entropy: 0.32030291074798223
      mean_kl_loss: 0.003766235520180046
      policy_loss: -0.13525831060750143
      total_loss: 9.800236656552268
      vf_explained_var: -0.23086260613941012
      vf_loss: 9.935494786217099
      vf_loss_unclipped: 3659.3897065662204
  num_agent_steps_sampled: 264000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 264000
  num_env_steps_trained: 0
iterations_since_restore: 66
node_ip: 127.0.0.1
num_agent_steps_sampled: 264000
num_agent_steps_trained: 0
num_env_steps_sampled: 264000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.7730720743656
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.9375
  ram_util_percent: 57.6
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055197058739346286
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022566940679128667
  mean_inference_ms: 1.0127704356212226
  mean_raw_obs_processing_ms: 0.20422947611141196
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0023431777954101562
    StateBufferConnector_ms: 0.0013289451599121094
    ViewRequirementAgentConnector_ms: 0.1133584976196289
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055197058739346286
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022566940679128667
    mean_inference_ms: 1.0127704356212226
    mean_raw_obs_processing_ms: 0.20422947611141196
time_since_restore: 421.1100540161133
time_this_iter_s: 6.224209785461426
time_total_s: 421.1100540161133
timers:
  sample_time_ms: 2604.074
  synch_weights_time_ms: 3.565
  training_iteration_time_ms: 6206.296
timestamp: 1703048001
timesteps_total: 264000
training_iteration: 66
trial_id: default
-----------------------
----------------------

Iteration 66:
agent_timesteps_total: 268000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024023056030273438
  StateBufferConnector_ms: 0.0013287067413330078
  ViewRequirementAgentConnector_ms: 0.11300206184387207
counters:
  num_agent_steps_sampled: 268000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 268000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-27
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 975
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.800166402544294
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 7.10542746348012e-16
      curr_lr: 5.0e-05
      entropy: 0.31540033363160636
      mean_kl_loss: 0.0019474484548980052
      policy_loss: -0.1350195947147551
      total_loss: 9.800166402544294
      vf_explained_var: -0.23608882086617605
      vf_loss: 9.935186022803897
      vf_loss_unclipped: 3630.7979445684523
  num_agent_steps_sampled: 268000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 268000
  num_env_steps_trained: 0
iterations_since_restore: 67
node_ip: 127.0.0.1
num_agent_steps_sampled: 268000
num_agent_steps_trained: 0
num_env_steps_sampled: 268000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 653.7774499601901
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 26.755555555555556
  ram_util_percent: 57.58888888888888
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055191128054686234
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022562316646151777
  mean_inference_ms: 1.012606725035597
  mean_raw_obs_processing_ms: 0.20421963355963044
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024023056030273438
    StateBufferConnector_ms: 0.0013287067413330078
    ViewRequirementAgentConnector_ms: 0.11300206184387207
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055191128054686234
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022562316646151777
    mean_inference_ms: 1.012606725035597
    mean_raw_obs_processing_ms: 0.20421963355963044
time_since_restore: 427.22951912879944
time_this_iter_s: 6.119465112686157
time_total_s: 427.22951912879944
timers:
  sample_time_ms: 2584.241
  synch_weights_time_ms: 3.603
  training_iteration_time_ms: 6188.719
timestamp: 1703048007
timesteps_total: 268000
training_iteration: 67
trial_id: default
-----------------------
----------------------

Iteration 67:
agent_timesteps_total: 272000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024428367614746094
  StateBufferConnector_ms: 0.0013127326965332031
  ViewRequirementAgentConnector_ms: 0.1128242015838623
counters:
  num_agent_steps_sampled: 272000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 272000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-33
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 983
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.798449198404947
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.55271373174006e-16
      curr_lr: 5.0e-05
      entropy: 0.3212947590010507
      mean_kl_loss: 0.004946733612126222
      policy_loss: -0.13642369033325286
      total_loss: 9.798449198404947
      vf_explained_var: -0.24072212264651344
      vf_loss: 9.934872672671364
      vf_loss_unclipped: 3601.880545479911
  num_agent_steps_sampled: 272000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 272000
  num_env_steps_trained: 0
iterations_since_restore: 68
node_ip: 127.0.0.1
num_agent_steps_sampled: 272000
num_agent_steps_trained: 0
num_env_steps_sampled: 272000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 647.9300644639053
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.77777777777778
  ram_util_percent: 57.62222222222223
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055184012961388994
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022556743073052873
  mean_inference_ms: 1.0124160672318385
  mean_raw_obs_processing_ms: 0.20420669552011797
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024428367614746094
    StateBufferConnector_ms: 0.0013127326965332031
    ViewRequirementAgentConnector_ms: 0.1128242015838623
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055184012961388994
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022556743073052873
    mean_inference_ms: 1.0124160672318385
    mean_raw_obs_processing_ms: 0.20420669552011797
time_since_restore: 433.40425419807434
time_this_iter_s: 6.174735069274902
time_total_s: 433.40425419807434
timers:
  sample_time_ms: 2578.785
  synch_weights_time_ms: 3.62
  training_iteration_time_ms: 6184.554
timestamp: 1703048013
timesteps_total: 272000
training_iteration: 68
trial_id: default
-----------------------
----------------------

Iteration 68:
agent_timesteps_total: 276000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024271011352539062
  StateBufferConnector_ms: 0.0013213157653808594
  ViewRequirementAgentConnector_ms: 0.11254549026489258
counters:
  num_agent_steps_sampled: 276000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 276000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-39
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 991
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.797651835850306
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.55271373174006e-16
      curr_lr: 5.0e-05
      entropy: 0.32139105314300176
      mean_kl_loss: 0.0051195257056709425
      policy_loss: -0.13699030982596533
      total_loss: 9.797651835850306
      vf_explained_var: -0.24600806690397717
      vf_loss: 9.934642065139045
      vf_loss_unclipped: 3573.645193917411
  num_agent_steps_sampled: 276000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 276000
  num_env_steps_trained: 0
iterations_since_restore: 69
node_ip: 127.0.0.1
num_agent_steps_sampled: 276000
num_agent_steps_trained: 0
num_env_steps_sampled: 276000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 645.5323499425947
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.266666666666666
  ram_util_percent: 57.64444444444445
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05517540851065917
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022549922535965702
  mean_inference_ms: 1.0121892853168506
  mean_raw_obs_processing_ms: 0.20418787071027425
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024271011352539062
    StateBufferConnector_ms: 0.0013213157653808594
    ViewRequirementAgentConnector_ms: 0.11254549026489258
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05517540851065917
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022549922535965702
    mean_inference_ms: 1.0121892853168506
    mean_raw_obs_processing_ms: 0.20418787071027425
time_since_restore: 439.601948261261
time_this_iter_s: 6.1976940631866455
time_total_s: 439.601948261261
timers:
  sample_time_ms: 2576.649
  synch_weights_time_ms: 3.557
  training_iteration_time_ms: 6188.015
timestamp: 1703048019
timesteps_total: 276000
training_iteration: 69
trial_id: default
-----------------------
----------------------

Iteration 69:
agent_timesteps_total: 280000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024161338806152344
  StateBufferConnector_ms: 0.0013260841369628906
  ViewRequirementAgentConnector_ms: 0.11225652694702148
counters:
  num_agent_steps_sampled: 280000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 280000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-45
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 999
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.798983937218075
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.77635686587003e-16
      curr_lr: 5.0e-05
      entropy: 0.31897242012478055
      mean_kl_loss: 0.002377836011061975
      policy_loss: -0.13542745794568742
      total_loss: 9.798983937218075
      vf_explained_var: -0.25083239873250324
      vf_loss: 9.934411321367536
      vf_loss_unclipped: 3545.5563034784227
  num_agent_steps_sampled: 280000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 280000
  num_env_steps_trained: 0
iterations_since_restore: 70
node_ip: 127.0.0.1
num_agent_steps_sampled: 280000
num_agent_steps_trained: 0
num_env_steps_sampled: 280000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 647.4197508139432
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 30.2125
  ram_util_percent: 57.6375
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05516438927935153
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02254116669805769
  mean_inference_ms: 1.0119155843034184
  mean_raw_obs_processing_ms: 0.2041594144279477
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024161338806152344
    StateBufferConnector_ms: 0.0013260841369628906
    ViewRequirementAgentConnector_ms: 0.11225652694702148
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05516438927935153
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02254116669805769
    mean_inference_ms: 1.0119155843034184
    mean_raw_obs_processing_ms: 0.2041594144279477
time_since_restore: 445.78149008750916
time_this_iter_s: 6.179541826248169
time_total_s: 445.78149008750916
timers:
  sample_time_ms: 2571.327
  synch_weights_time_ms: 3.521
  training_iteration_time_ms: 6188.944
timestamp: 1703048025
timesteps_total: 280000
training_iteration: 70
trial_id: default
-----------------------
----------------------

Iteration 70:
agent_timesteps_total: 284000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002415895462036133
  StateBufferConnector_ms: 0.001323699951171875
  ViewRequirementAgentConnector_ms: 0.1120462417602539
counters:
  num_agent_steps_sampled: 284000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 284000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-51
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1007
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.797886939275832
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 8.88178432935015e-17
      curr_lr: 5.0e-05
      entropy: 0.33736193889663335
      mean_kl_loss: 0.003285011640812064
      policy_loss: -0.13625766443354742
      total_loss: 9.797886939275832
      vf_explained_var: -0.2562499784287952
      vf_loss: 9.934144519624256
      vf_loss_unclipped: 3517.9010067894346
  num_agent_steps_sampled: 284000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 284000
  num_env_steps_trained: 0
iterations_since_restore: 71
node_ip: 127.0.0.1
num_agent_steps_sampled: 284000
num_agent_steps_trained: 0
num_env_steps_sampled: 284000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 646.3254059552781
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.555555555555557
  ram_util_percent: 57.55555555555556
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05515163853452574
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02253116212139921
  mean_inference_ms: 1.0116161026086965
  mean_raw_obs_processing_ms: 0.20412461448900213
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002415895462036133
    StateBufferConnector_ms: 0.001323699951171875
    ViewRequirementAgentConnector_ms: 0.1120462417602539
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05515163853452574
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02253116212139921
    mean_inference_ms: 1.0116161026086965
    mean_raw_obs_processing_ms: 0.20412461448900213
time_since_restore: 451.9715418815613
time_this_iter_s: 6.190051794052124
time_total_s: 451.9715418815613
timers:
  sample_time_ms: 2568.072
  synch_weights_time_ms: 3.498
  training_iteration_time_ms: 6192.153
timestamp: 1703048031
timesteps_total: 284000
training_iteration: 71
trial_id: default
-----------------------
----------------------

Iteration 71:
agent_timesteps_total: 288000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024225711822509766
  StateBufferConnector_ms: 0.0013175010681152344
  ViewRequirementAgentConnector_ms: 0.11167669296264648
counters:
  num_agent_steps_sampled: 288000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 288000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-53-58
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1015
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.797521091642833
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 8.88178432935015e-17
      curr_lr: 5.0e-05
      entropy: 0.31985256927353994
      mean_kl_loss: 0.005178967596380278
      policy_loss: -0.13635759694235666
      total_loss: 9.797521091642833
      vf_explained_var: -0.26168592203231084
      vf_loss: 9.933878762381417
      vf_loss_unclipped: 3490.52197265625
  num_agent_steps_sampled: 288000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 288000
  num_env_steps_trained: 0
iterations_since_restore: 72
node_ip: 127.0.0.1
num_agent_steps_sampled: 288000
num_agent_steps_trained: 0
num_env_steps_sampled: 288000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 643.3902027931987
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.63333333333334
  ram_util_percent: 57.56666666666667
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.0551381337335847
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02252084093895129
  mean_inference_ms: 1.0113050346452894
  mean_raw_obs_processing_ms: 0.20408605855120066
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024225711822509766
    StateBufferConnector_ms: 0.0013175010681152344
    ViewRequirementAgentConnector_ms: 0.11167669296264648
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0551381337335847
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02252084093895129
    mean_inference_ms: 1.0113050346452894
    mean_raw_obs_processing_ms: 0.20408605855120066
time_since_restore: 458.18988490104675
time_this_iter_s: 6.218343019485474
time_total_s: 458.18988490104675
timers:
  sample_time_ms: 2564.073
  synch_weights_time_ms: 3.582
  training_iteration_time_ms: 6185.369
timestamp: 1703048038
timesteps_total: 288000
training_iteration: 72
trial_id: default
-----------------------
----------------------

Iteration 72:
agent_timesteps_total: 292000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002410411834716797
  StateBufferConnector_ms: 0.0013127326965332031
  ViewRequirementAgentConnector_ms: 0.1113581657409668
counters:
  num_agent_steps_sampled: 292000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 292000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-04
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1023
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.797396205720448
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 4.440892164675075e-17
      curr_lr: 5.0e-05
      entropy: 0.33066231580007643
      mean_kl_loss: 0.0025584590994889196
      policy_loss: -0.13623052267801194
      total_loss: 9.797396205720448
      vf_explained_var: -0.26632986182258245
      vf_loss: 9.933626674470448
      vf_loss_unclipped: 3463.4146902901784
  num_agent_steps_sampled: 292000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 292000
  num_env_steps_trained: 0
iterations_since_restore: 73
node_ip: 127.0.0.1
num_agent_steps_sampled: 292000
num_agent_steps_trained: 0
num_env_steps_sampled: 292000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 659.5160589522796
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.65
  ram_util_percent: 57.5625
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.0551207173282192
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022507966906936887
  mean_inference_ms: 1.010930950321106
  mean_raw_obs_processing_ms: 0.20403734854068403
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002410411834716797
    StateBufferConnector_ms: 0.0013127326965332031
    ViewRequirementAgentConnector_ms: 0.1113581657409668
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0551207173282192
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022507966906936887
    mean_inference_ms: 1.010930950321106
    mean_raw_obs_processing_ms: 0.20403734854068403
time_since_restore: 464.25610303878784
time_this_iter_s: 6.066218137741089
time_total_s: 464.25610303878784
timers:
  sample_time_ms: 2550.422
  synch_weights_time_ms: 3.495
  training_iteration_time_ms: 6170.75
timestamp: 1703048044
timesteps_total: 292000
training_iteration: 73
trial_id: default
-----------------------
----------------------

Iteration 73:
agent_timesteps_total: 296000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002405405044555664
  StateBufferConnector_ms: 0.0013132095336914062
  ViewRequirementAgentConnector_ms: 0.11120128631591797
counters:
  num_agent_steps_sampled: 296000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 296000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-10
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1031
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.796800477164131
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.2204460823375376e-17
      curr_lr: 5.0e-05
      entropy: 0.33617637270972844
      mean_kl_loss: 0.004377459097457047
      policy_loss: -0.13656252267814817
      total_loss: 9.796800477164131
      vf_explained_var: -0.2719227700006394
      vf_loss: 9.933363006228493
      vf_loss_unclipped: 3436.5916806175596
  num_agent_steps_sampled: 296000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 296000
  num_env_steps_trained: 0
iterations_since_restore: 74
node_ip: 127.0.0.1
num_agent_steps_sampled: 296000
num_agent_steps_trained: 0
num_env_steps_sampled: 296000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 639.9316967434887
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.622222222222224
  ram_util_percent: 57.62222222222223
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05510369069691109
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022495396254320132
  mean_inference_ms: 1.010571331503369
  mean_raw_obs_processing_ms: 0.2039896611575115
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002405405044555664
    StateBufferConnector_ms: 0.0013132095336914062
    ViewRequirementAgentConnector_ms: 0.11120128631591797
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05510369069691109
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022495396254320132
    mean_inference_ms: 1.010571331503369
    mean_raw_obs_processing_ms: 0.2039896611575115
time_since_restore: 470.5079288482666
time_this_iter_s: 6.25182580947876
time_total_s: 470.5079288482666
timers:
  sample_time_ms: 2556.991
  synch_weights_time_ms: 3.521
  training_iteration_time_ms: 6179.73
timestamp: 1703048050
timesteps_total: 296000
training_iteration: 74
trial_id: default
-----------------------
----------------------

Iteration 74:
agent_timesteps_total: 300000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0024051666259765625
  StateBufferConnector_ms: 0.0013022422790527344
  ViewRequirementAgentConnector_ms: 0.11110854148864746
counters:
  num_agent_steps_sampled: 300000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 300000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-16
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1039
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.79736378079369
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.1102230411687688e-17
      curr_lr: 5.0e-05
      entropy: 0.31657829738798593
      mean_kl_loss: 0.0041488022697021305
      policy_loss: -0.13575141309272676
      total_loss: 9.79736378079369
      vf_explained_var: -0.27762495336078463
      vf_loss: 9.933115141732353
      vf_loss_unclipped: 3410.0983072916665
  num_agent_steps_sampled: 300000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 300000
  num_env_steps_trained: 0
iterations_since_restore: 75
node_ip: 127.0.0.1
num_agent_steps_sampled: 300000
num_agent_steps_trained: 0
num_env_steps_sampled: 300000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 642.8114910701235
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.766666666666666
  ram_util_percent: 57.655555555555566
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055087407385964654
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02248355261338656
  mean_inference_ms: 1.0102325105886598
  mean_raw_obs_processing_ms: 0.2039439516820584
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0024051666259765625
    StateBufferConnector_ms: 0.0013022422790527344
    ViewRequirementAgentConnector_ms: 0.11110854148864746
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055087407385964654
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02248355261338656
    mean_inference_ms: 1.0102325105886598
    mean_raw_obs_processing_ms: 0.2039439516820584
time_since_restore: 476.7318437099457
time_this_iter_s: 6.223914861679077
time_total_s: 476.7318437099457
timers:
  sample_time_ms: 2559.695
  synch_weights_time_ms: 3.513
  training_iteration_time_ms: 6183.382
timestamp: 1703048056
timesteps_total: 300000
training_iteration: 75
trial_id: default
-----------------------
----------------------

Iteration 75:
agent_timesteps_total: 304000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0023391246795654297
  StateBufferConnector_ms: 0.0013222694396972656
  ViewRequirementAgentConnector_ms: 0.11109280586242676
counters:
  num_agent_steps_sampled: 304000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 304000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-22
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1047
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.797651745024181
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.551115205843844e-18
      curr_lr: 5.0e-05
      entropy: 0.29460920038677396
      mean_kl_loss: 0.0028980041179996383
      policy_loss: -0.13521415953125274
      total_loss: 9.797651745024181
      vf_explained_var: -0.282735858644758
      vf_loss: 9.932865778605143
      vf_loss_unclipped: 3383.7403622581846
  num_agent_steps_sampled: 304000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 304000
  num_env_steps_trained: 0
iterations_since_restore: 76
node_ip: 127.0.0.1
num_agent_steps_sampled: 304000
num_agent_steps_trained: 0
num_env_steps_sampled: 304000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.3914506483239
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.666666666666668
  ram_util_percent: 57.6888888888889
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05507235859882367
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022472810759840325
  mean_inference_ms: 1.0099272817431904
  mean_raw_obs_processing_ms: 0.2038998990169459
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0023391246795654297
    StateBufferConnector_ms: 0.0013222694396972656
    ViewRequirementAgentConnector_ms: 0.11109280586242676
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05507235859882367
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022472810759840325
    mean_inference_ms: 1.0099272817431904
    mean_raw_obs_processing_ms: 0.2038998990169459
time_since_restore: 482.9404857158661
time_this_iter_s: 6.20864200592041
time_total_s: 482.9404857158661
timers:
  sample_time_ms: 2556.379
  synch_weights_time_ms: 3.533
  training_iteration_time_ms: 6181.819
timestamp: 1703048062
timesteps_total: 304000
training_iteration: 76
trial_id: default
-----------------------
----------------------

Iteration 76:
agent_timesteps_total: 308000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.00226593017578125
  StateBufferConnector_ms: 0.0013163089752197266
  ViewRequirementAgentConnector_ms: 0.1111903190612793
counters:
  num_agent_steps_sampled: 308000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 308000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-29
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1055
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.796023187183199
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.551115205843844e-18
      curr_lr: 5.0e-05
      entropy: 0.30441440854753765
      mean_kl_loss: 0.0062306384453382265
      policy_loss: -0.1365968875941776
      total_loss: 9.796023187183199
      vf_explained_var: -0.288480281829834
      vf_loss: 9.932619821457635
      vf_loss_unclipped: 3357.5610002790177
  num_agent_steps_sampled: 308000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 308000
  num_env_steps_trained: 0
iterations_since_restore: 77
node_ip: 127.0.0.1
num_agent_steps_sampled: 308000
num_agent_steps_trained: 0
num_env_steps_sampled: 308000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 637.230856799544
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 29.144444444444446
  ram_util_percent: 57.77777777777778
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05505826975635036
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022462695210868622
  mean_inference_ms: 1.0096394536137223
  mean_raw_obs_processing_ms: 0.20385843080799723
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.00226593017578125
    StateBufferConnector_ms: 0.0013163089752197266
    ViewRequirementAgentConnector_ms: 0.1111903190612793
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05505826975635036
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022462695210868622
    mean_inference_ms: 1.0096394536137223
    mean_raw_obs_processing_ms: 0.20385843080799723
time_since_restore: 489.2188687324524
time_this_iter_s: 6.278383016586304
time_total_s: 489.2188687324524
timers:
  sample_time_ms: 2564.825
  synch_weights_time_ms: 3.522
  training_iteration_time_ms: 6197.706
timestamp: 1703048069
timesteps_total: 308000
training_iteration: 77
trial_id: default
-----------------------
----------------------

Iteration 77:
agent_timesteps_total: 312000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002230405807495117
  StateBufferConnector_ms: 0.0013058185577392578
  ViewRequirementAgentConnector_ms: 0.11117291450500488
counters:
  num_agent_steps_sampled: 312000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 312000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-35
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1063
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.79680365607852
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.775557602921922e-18
      curr_lr: 5.0e-05
      entropy: 0.2895405093828837
      mean_kl_loss: 0.002724838448043123
      policy_loss: -0.13556787300677525
      total_loss: 9.79680365607852
      vf_explained_var: -0.29418768769218806
      vf_loss: 9.932371775309244
      vf_loss_unclipped: 3331.7540341331846
  num_agent_steps_sampled: 312000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 312000
  num_env_steps_trained: 0
iterations_since_restore: 78
node_ip: 127.0.0.1
num_agent_steps_sampled: 312000
num_agent_steps_trained: 0
num_env_steps_sampled: 312000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.017542908743
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.98888888888889
  ram_util_percent: 57.733333333333334
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.055044092337703424
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022452318413919033
  mean_inference_ms: 1.0093494148349669
  mean_raw_obs_processing_ms: 0.20381713071767119
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002230405807495117
    StateBufferConnector_ms: 0.0013058185577392578
    ViewRequirementAgentConnector_ms: 0.11117291450500488
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.055044092337703424
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022452318413919033
    mean_inference_ms: 1.0093494148349669
    mean_raw_obs_processing_ms: 0.20381713071767119
time_since_restore: 495.43108677864075
time_this_iter_s: 6.2122180461883545
time_total_s: 495.43108677864075
timers:
  sample_time_ms: 2567.715
  synch_weights_time_ms: 3.592
  training_iteration_time_ms: 6201.456
timestamp: 1703048075
timesteps_total: 312000
training_iteration: 78
trial_id: default
-----------------------
----------------------

Iteration 78:
agent_timesteps_total: 316000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0022268295288085938
  StateBufferConnector_ms: 0.0013074874877929688
  ViewRequirementAgentConnector_ms: 0.11127710342407227
counters:
  num_agent_steps_sampled: 316000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 316000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-41
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1071
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.796513239542643
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.387778801460961e-18
      curr_lr: 5.0e-05
      entropy: 0.2840930393763951
      mean_kl_loss: 0.002546059012347866
      policy_loss: -0.13561109488918668
      total_loss: 9.796513239542643
      vf_explained_var: -0.29962477797553655
      vf_loss: 9.932124364943732
      vf_loss_unclipped: 3306.0626278831846
  num_agent_steps_sampled: 316000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 316000
  num_env_steps_trained: 0
iterations_since_restore: 79
node_ip: 127.0.0.1
num_agent_steps_sampled: 316000
num_agent_steps_trained: 0
num_env_steps_sampled: 316000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 653.6393200228258
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 27.9125
  ram_util_percent: 57.725
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05502933225721396
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022441535108692633
  mean_inference_ms: 1.0090533276406768
  mean_raw_obs_processing_ms: 0.20377466247358503
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0022268295288085938
    StateBufferConnector_ms: 0.0013074874877929688
    ViewRequirementAgentConnector_ms: 0.11127710342407227
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05502933225721396
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022441535108692633
    mean_inference_ms: 1.0090533276406768
    mean_raw_obs_processing_ms: 0.20377466247358503
time_since_restore: 501.55201172828674
time_this_iter_s: 6.120924949645996
time_total_s: 501.55201172828674
timers:
  sample_time_ms: 2561.476
  synch_weights_time_ms: 3.623
  training_iteration_time_ms: 6193.771
timestamp: 1703048081
timesteps_total: 316000
training_iteration: 79
trial_id: default
-----------------------
----------------------

Iteration 79:
agent_timesteps_total: 320000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002146005630493164
  StateBufferConnector_ms: 0.0013089179992675781
  ViewRequirementAgentConnector_ms: 0.11113691329956055
counters:
  num_agent_steps_sampled: 320000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 320000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-47
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1079
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.794920512608119
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.387778801460961e-18
      curr_lr: 5.0e-05
      entropy: 0.2999142436754136
      mean_kl_loss: 0.005077435307687968
      policy_loss: -0.13695404501188368
      total_loss: 9.794920512608119
      vf_explained_var: -0.3049882650375366
      vf_loss: 9.931874729338146
      vf_loss_unclipped: 3280.5978190104165
  num_agent_steps_sampled: 320000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 320000
  num_env_steps_trained: 0
iterations_since_restore: 80
node_ip: 127.0.0.1
num_agent_steps_sampled: 320000
num_agent_steps_trained: 0
num_env_steps_sampled: 320000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 641.1704226774744
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.444444444444443
  ram_util_percent: 57.6888888888889
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05501675520781992
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02243239522667479
  mean_inference_ms: 1.0088028421067614
  mean_raw_obs_processing_ms: 0.20373816781895862
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002146005630493164
    StateBufferConnector_ms: 0.0013089179992675781
    ViewRequirementAgentConnector_ms: 0.11113691329956055
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05501675520781992
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02243239522667479
    mean_inference_ms: 1.0088028421067614
    mean_raw_obs_processing_ms: 0.20373816781895862
time_since_restore: 507.7918047904968
time_this_iter_s: 6.239793062210083
time_total_s: 507.7918047904968
timers:
  sample_time_ms: 2568.744
  synch_weights_time_ms: 3.582
  training_iteration_time_ms: 6199.793
timestamp: 1703048087
timesteps_total: 320000
training_iteration: 80
trial_id: default
-----------------------
----------------------

Iteration 80:
agent_timesteps_total: 324000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021028518676757812
  StateBufferConnector_ms: 0.0013158321380615234
  ViewRequirementAgentConnector_ms: 0.11153244972229004
counters:
  num_agent_steps_sampled: 324000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 324000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-54-54
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1087
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.795325506301154
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 6.938894007304805e-19
      curr_lr: 5.0e-05
      entropy: 0.3115810851256053
      mean_kl_loss: 0.004490124199551003
      policy_loss: -0.13631613659007208
      total_loss: 9.795325506301154
      vf_explained_var: -0.3103536253883725
      vf_loss: 9.931641442435128
      vf_loss_unclipped: 3255.3453427269346
  num_agent_steps_sampled: 324000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 324000
  num_env_steps_trained: 0
iterations_since_restore: 81
node_ip: 127.0.0.1
num_agent_steps_sampled: 324000
num_agent_steps_trained: 0
num_env_steps_sampled: 324000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 639.3878419472811
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.144444444444446
  ram_util_percent: 57.63333333333334
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05500548203038966
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02242411357713502
  mean_inference_ms: 1.0085803279896182
  mean_raw_obs_processing_ms: 0.20370382032948148
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021028518676757812
    StateBufferConnector_ms: 0.0013158321380615234
    ViewRequirementAgentConnector_ms: 0.11153244972229004
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05500548203038966
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02242411357713502
    mean_inference_ms: 1.0085803279896182
    mean_raw_obs_processing_ms: 0.20370382032948148
time_since_restore: 514.0490267276764
time_this_iter_s: 6.257221937179565
time_total_s: 514.0490267276764
timers:
  sample_time_ms: 2576.288
  synch_weights_time_ms: 3.521
  training_iteration_time_ms: 6206.508
timestamp: 1703048094
timesteps_total: 324000
training_iteration: 81
trial_id: default
-----------------------
----------------------

Iteration 81:
agent_timesteps_total: 328000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0021255016326904297
  StateBufferConnector_ms: 0.0013167858123779297
  ViewRequirementAgentConnector_ms: 0.11151790618896484
counters:
  num_agent_steps_sampled: 328000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 328000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-00
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1095
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.795481091453915
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 3.4694470036524025e-19
      curr_lr: 5.0e-05
      entropy: 0.3131102664130075
      mean_kl_loss: 0.0039903615804253905
      policy_loss: -0.13591445095482327
      total_loss: 9.795481091453915
      vf_explained_var: -0.31589278720674063
      vf_loss: 9.931395757765998
      vf_loss_unclipped: 3230.3372744605654
  num_agent_steps_sampled: 328000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 328000
  num_env_steps_trained: 0
iterations_since_restore: 82
node_ip: 127.0.0.1
num_agent_steps_sampled: 328000
num_agent_steps_trained: 0
num_env_steps_sampled: 328000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 639.7202883169293
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 28.38888888888889
  ram_util_percent: 57.62222222222223
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.0549963204485914
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022417387030958677
  mean_inference_ms: 1.0083987838246395
  mean_raw_obs_processing_ms: 0.20367390233407867
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0021255016326904297
    StateBufferConnector_ms: 0.0013167858123779297
    ViewRequirementAgentConnector_ms: 0.11151790618896484
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.0549963204485914
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022417387030958677
    mean_inference_ms: 1.0083987838246395
    mean_raw_obs_processing_ms: 0.20367390233407867
time_since_restore: 520.3029420375824
time_this_iter_s: 6.253915309906006
time_total_s: 520.3029420375824
timers:
  sample_time_ms: 2581.224
  synch_weights_time_ms: 3.446
  training_iteration_time_ms: 6210.075
timestamp: 1703048100
timesteps_total: 328000
training_iteration: 82
trial_id: default
-----------------------
----------------------

Iteration 82:
agent_timesteps_total: 332000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002124786376953125
  StateBufferConnector_ms: 0.001310110092163086
  ViewRequirementAgentConnector_ms: 0.11153364181518555
counters:
  num_agent_steps_sampled: 332000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 332000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-06
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1103
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.794793673924037
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.7347235018262012e-19
      curr_lr: 5.0e-05
      entropy: 0.3161701815468924
      mean_kl_loss: 0.004076305310431529
      policy_loss: -0.1362440437078476
      total_loss: 9.794793673924037
      vf_explained_var: -0.32229576792035786
      vf_loss: 9.931037766592842
      vf_loss_unclipped: 3205.1747116815477
  num_agent_steps_sampled: 332000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 332000
  num_env_steps_trained: 0
iterations_since_restore: 83
node_ip: 127.0.0.1
num_agent_steps_sampled: 332000
num_agent_steps_trained: 0
num_env_steps_sampled: 332000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 625.5866920477757
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 33.5
  ram_util_percent: 57.71111111111112
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05498935040371009
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022412284805618538
  mean_inference_ms: 1.008259164487213
  mean_raw_obs_processing_ms: 0.2036491404457941
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002124786376953125
    StateBufferConnector_ms: 0.001310110092163086
    ViewRequirementAgentConnector_ms: 0.11153364181518555
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05498935040371009
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022412284805618538
    mean_inference_ms: 1.008259164487213
    mean_raw_obs_processing_ms: 0.2036491404457941
time_since_restore: 526.6982123851776
time_this_iter_s: 6.395270347595215
time_total_s: 526.6982123851776
timers:
  sample_time_ms: 2599.144
  synch_weights_time_ms: 3.42
  training_iteration_time_ms: 6242.969
timestamp: 1703048106
timesteps_total: 332000
training_iteration: 83
trial_id: default
-----------------------
----------------------

Iteration 83:
agent_timesteps_total: 336000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002134084701538086
  StateBufferConnector_ms: 0.0013206005096435547
  ViewRequirementAgentConnector_ms: 0.11207890510559082
counters:
  num_agent_steps_sampled: 336000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 336000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-13
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1111
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.795131229218983
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 8.673617509131006e-20
      curr_lr: 5.0e-05
      entropy: 0.30262292992501033
      mean_kl_loss: 0.0026384669773327524
      policy_loss: -0.13549548600401198
      total_loss: 9.795131229218983
      vf_explained_var: -0.3286660058157785
      vf_loss: 9.930626823788597
      vf_loss_unclipped: 3179.6173153831846
  num_agent_steps_sampled: 336000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 336000
  num_env_steps_trained: 0
iterations_since_restore: 84
node_ip: 127.0.0.1
num_agent_steps_sampled: 336000
num_agent_steps_trained: 0
num_env_steps_sampled: 336000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 648.5493201162975
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 35.34444444444445
  ram_util_percent: 57.8111111111111
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.054983878617921436
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02240825726286573
  mean_inference_ms: 1.0081433737566088
  mean_raw_obs_processing_ms: 0.20362983617101102
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002134084701538086
    StateBufferConnector_ms: 0.0013206005096435547
    ViewRequirementAgentConnector_ms: 0.11207890510559082
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.054983878617921436
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02240825726286573
    mean_inference_ms: 1.0081433737566088
    mean_raw_obs_processing_ms: 0.20362983617101102
time_since_restore: 532.8670561313629
time_this_iter_s: 6.168843746185303
time_total_s: 532.8670561313629
timers:
  sample_time_ms: 2597.421
  synch_weights_time_ms: 3.575
  training_iteration_time_ms: 6234.663
timestamp: 1703048113
timesteps_total: 336000
training_iteration: 84
trial_id: default
-----------------------
----------------------

Iteration 84:
agent_timesteps_total: 340000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002131223678588867
  StateBufferConnector_ms: 0.0013310909271240234
  ViewRequirementAgentConnector_ms: 0.11238694190979004
counters:
  num_agent_steps_sampled: 340000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 340000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-19
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1119
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.794055439177013
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 4.336808754565503e-20
      curr_lr: 5.0e-05
      entropy: 0.27946111417952035
      mean_kl_loss: 0.00433014737399196
      policy_loss: -0.13589677001748765
      total_loss: 9.794055439177013
      vf_explained_var: -0.33481620039258686
      vf_loss: 9.929952121916271
      vf_loss_unclipped: 3152.6611909412204
  num_agent_steps_sampled: 340000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 340000
  num_env_steps_trained: 0
iterations_since_restore: 85
node_ip: 127.0.0.1
num_agent_steps_sampled: 340000
num_agent_steps_trained: 0
num_env_steps_sampled: 340000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 644.5439978117212
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 32.599999999999994
  ram_util_percent: 57.775
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.054981515300016576
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022406204351520537
  mean_inference_ms: 1.008086159637545
  mean_raw_obs_processing_ms: 0.20361923378666802
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002131223678588867
    StateBufferConnector_ms: 0.0013310909271240234
    ViewRequirementAgentConnector_ms: 0.11238694190979004
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.054981515300016576
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022406204351520537
    mean_inference_ms: 1.008086159637545
    mean_raw_obs_processing_ms: 0.20361923378666802
time_since_restore: 539.0741789340973
time_this_iter_s: 6.207122802734375
time_total_s: 539.0741789340973
timers:
  sample_time_ms: 2599.207
  synch_weights_time_ms: 3.403
  training_iteration_time_ms: 6232.991
timestamp: 1703048119
timesteps_total: 340000
training_iteration: 85
trial_id: default
-----------------------
----------------------

Iteration 85:
agent_timesteps_total: 344000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002126932144165039
  StateBufferConnector_ms: 0.0013377666473388672
  ViewRequirementAgentConnector_ms: 0.11245155334472656
counters:
  num_agent_steps_sampled: 344000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 344000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-25
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1127
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.79341307140532
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 2.1684043772827515e-20
      curr_lr: 5.0e-05
      entropy: 0.30639260439645677
      mean_kl_loss: 0.004268011980194669
      policy_loss: -0.13607684274514517
      total_loss: 9.79341307140532
      vf_explained_var: -0.34201779819670175
      vf_loss: 9.92948986235119
      vf_loss_unclipped: 3124.4057035900296
  num_agent_steps_sampled: 344000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 344000
  num_env_steps_trained: 0
iterations_since_restore: 86
node_ip: 127.0.0.1
num_agent_steps_sampled: 344000
num_agent_steps_trained: 0
num_env_steps_sampled: 344000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 645.5088540766704
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 31.899999999999995
  ram_util_percent: 57.72222222222222
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05498132602248793
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.02240541857236877
  mean_inference_ms: 1.0080665904075818
  mean_raw_obs_processing_ms: 0.2036149787134134
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002126932144165039
    StateBufferConnector_ms: 0.0013377666473388672
    ViewRequirementAgentConnector_ms: 0.11245155334472656
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05498132602248793
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.02240541857236877
    mean_inference_ms: 1.0080665904075818
    mean_raw_obs_processing_ms: 0.2036149787134134
time_since_restore: 545.272027015686
time_this_iter_s: 6.197848081588745
time_total_s: 545.272027015686
timers:
  sample_time_ms: 2601.027
  synch_weights_time_ms: 3.399
  training_iteration_time_ms: 6231.916
timestamp: 1703048125
timesteps_total: 344000
training_iteration: 86
trial_id: default
-----------------------
----------------------

Iteration 86:
agent_timesteps_total: 348000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.00212860107421875
  StateBufferConnector_ms: 0.0013344287872314453
  ViewRequirementAgentConnector_ms: 0.11242151260375977
counters:
  num_agent_steps_sampled: 348000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 348000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-31
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1135
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.792359715416318
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.0842021886413758e-20
      curr_lr: 5.0e-05
      entropy: 0.32016174282346455
      mean_kl_loss: 0.0041839869366418455
      policy_loss: -0.1367815878419649
      total_loss: 9.792359715416318
      vf_explained_var: -0.3489862283070882
      vf_loss: 9.929141498747326
      vf_loss_unclipped: 3096.449148995536
  num_agent_steps_sampled: 348000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 348000
  num_env_steps_trained: 0
iterations_since_restore: 87
node_ip: 127.0.0.1
num_agent_steps_sampled: 348000
num_agent_steps_trained: 0
num_env_steps_sampled: 348000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 640.9549873312066
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 33.17777777777778
  ram_util_percent: 57.78888888888889
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05497936960665953
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022403445539750343
  mean_inference_ms: 1.0080124538741808
  mean_raw_obs_processing_ms: 0.20360624538199892
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.00212860107421875
    StateBufferConnector_ms: 0.0013344287872314453
    ViewRequirementAgentConnector_ms: 0.11242151260375977
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05497936960665953
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022403445539750343
    mean_inference_ms: 1.0080124538741808
    mean_raw_obs_processing_ms: 0.20360624538199892
time_since_restore: 551.5139801502228
time_this_iter_s: 6.241953134536743
time_total_s: 551.5139801502228
timers:
  sample_time_ms: 2594.224
  synch_weights_time_ms: 3.386
  training_iteration_time_ms: 6228.269
timestamp: 1703048131
timesteps_total: 348000
training_iteration: 87
trial_id: default
-----------------------
----------------------

Iteration 87:
agent_timesteps_total: 352000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002144336700439453
  StateBufferConnector_ms: 0.0013370513916015625
  ViewRequirementAgentConnector_ms: 0.11291933059692383
counters:
  num_agent_steps_sampled: 352000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 352000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-37
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1143
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.791228294372559
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 1.0842021886413758e-20
      curr_lr: 5.0e-05
      entropy: 0.3171537873290834
      mean_kl_loss: 0.005383422928032035
      policy_loss: -0.137506793652262
      total_loss: 9.791228294372559
      vf_explained_var: -0.3565714529582432
      vf_loss: 9.928735142662411
      vf_loss_unclipped: 3068.5186360677085
  num_agent_steps_sampled: 352000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 352000
  num_env_steps_trained: 0
iterations_since_restore: 88
node_ip: 127.0.0.1
num_agent_steps_sampled: 352000
num_agent_steps_trained: 0
num_env_steps_sampled: 352000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 638.7876281409114
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 34.9
  ram_util_percent: 58.03333333333333
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05497853347411856
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022402103305240458
  mean_inference_ms: 1.007975614894623
  mean_raw_obs_processing_ms: 0.20360090016419707
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002144336700439453
    StateBufferConnector_ms: 0.0013370513916015625
    ViewRequirementAgentConnector_ms: 0.11291933059692383
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05497853347411856
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022402103305240458
    mean_inference_ms: 1.007975614894623
    mean_raw_obs_processing_ms: 0.20360090016419707
time_since_restore: 557.7770800590515
time_this_iter_s: 6.263099908828735
time_total_s: 557.7770800590515
timers:
  sample_time_ms: 2599.522
  synch_weights_time_ms: 3.399
  training_iteration_time_ms: 6233.354
timestamp: 1703048137
timesteps_total: 352000
training_iteration: 88
trial_id: default
-----------------------
----------------------

Iteration 88:
agent_timesteps_total: 356000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.002231597900390625
  StateBufferConnector_ms: 0.001336812973022461
  ViewRequirementAgentConnector_ms: 0.11341118812561035
counters:
  num_agent_steps_sampled: 356000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 356000
  num_env_steps_trained: 0
custom_metrics: {}
date: 2023-12-19_23-55-44
done: false
episode_len_mean: 500.0
episode_media: {}
episode_reward_max: 500.0
episode_reward_mean: 500.0
episode_reward_min: 500.0
episodes_this_iter: 8
episodes_total: 1151
hostname: Alexs-MacBook-Pro.local
info:
  learner:
    __all__:
      num_agent_steps_trained: 5818.666666666667
      num_env_steps_trained: 4000.0
      total_loss: 9.792199906848726
    default_policy:
      curr_entropy_coeff: 0.0
      curr_kl_coeff: 5.421010943206879e-21
      curr_lr: 5.0e-05
      entropy: 0.32731278311638606
      mean_kl_loss: 0.0036458185031950138
      policy_loss: -0.13610802910157613
      total_loss: 9.792199906848726
      vf_explained_var: -0.3636490333647955
      vf_loss: 9.9283078511556
      vf_loss_unclipped: 3040.4258626302085
  num_agent_steps_sampled: 356000
  num_agent_steps_trained: 0
  num_env_steps_sampled: 356000
  num_env_steps_trained: 0
iterations_since_restore: 89
node_ip: 127.0.0.1
num_agent_steps_sampled: 356000
num_agent_steps_trained: 0
num_env_steps_sampled: 356000
num_env_steps_sampled_this_iter: 4000
num_env_steps_sampled_throughput_per_sec: 636.8815292612869
num_env_steps_trained: 0
num_env_steps_trained_this_iter: 0
num_env_steps_trained_throughput_per_sec: 0.0
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 0
perf:
  cpu_util_percent: 33.833333333333336
  ram_util_percent: 58.03333333333333
pid: 36114
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.05497851512262855
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.022401226915316763
  mean_inference_ms: 1.0079499894174098
  mean_raw_obs_processing_ms: 0.2036001274436903
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.002231597900390625
    StateBufferConnector_ms: 0.001336812973022461
    ViewRequirementAgentConnector_ms: 0.11341118812561035
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 500.0
  episode_reward_mean: 500.0
  episode_reward_min: 500.0
  episodes_this_iter: 8
  hist_stats:
    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,
      500, 500, 500, 500, 500, 500, 500, 500]
    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,
      500.0, 500.0, 500.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.05497851512262855
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.022401226915316763
    mean_inference_ms: 1.0079499894174098
    mean_raw_obs_processing_ms: 0.2036001274436903
time_since_restore: 564.0588531494141
time_this_iter_s: 6.281773090362549
time_total_s: 564.0588531494141
timers:
  sample_time_ms: 2609.387
  synch_weights_time_ms: 3.525
  training_iteration_time_ms: 6249.456
timestamp: 1703048144
timesteps_total: 356000
training_iteration: 89
trial_id: default
-----------------------
----------------------

